{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shughes-21/TF2_Specialization/blob/customising_your_models_with_tf2/Copy_of_Coding_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "T1-3Ka58Pwt-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c78cc03-be49-42a0-d932-08eb3290af59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.9.2\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00ncP9rEPwuB"
      },
      "source": [
        "# Model subclassing and custom training loops"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1DN3Y-lPwuB"
      },
      "source": [
        " ## Coding tutorials\n",
        " #### [1. Model subclassing](#coding_tutorial_1)\n",
        " #### [2. Custom layers](#coding_tutorial_2)\n",
        " #### [3. Automatic differentiation](#coding_tutorial_3)\n",
        " #### [4. Custom training loops](#coding_tutorial_4)\n",
        " #### [5. tf.function decorator](#coding_tutorial_5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdMNfOE5PwuC"
      },
      "source": [
        "***\n",
        "<a id=\"coding_tutorial_1\"></a>\n",
        "## Model subclassing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bFPp5pbiPwuC"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, Softmax, concatenate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Adp6SI-DPwuC"
      },
      "source": [
        "#### Create a simple model using the model subclassing API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "xLCCdkfFPwuD"
      },
      "outputs": [],
      "source": [
        "# Build the model\n",
        "\n",
        "class MyModel(Model):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(MyModel, self).__init__()\n",
        "    self.dense_1 = Dense(64, activation='relu')\n",
        "    self.dense_2 = Dense(10)\n",
        "    self.dense_3 = Dense(5)\n",
        "    self.softmax = Softmax()\n",
        "\n",
        "  def call(self, inputs, training=True):\n",
        "    x = self.dense_1(inputs)\n",
        "    y1 = self.dense_2(inputs)\n",
        "    y2 = self.dense_3(y1)\n",
        "    concat = concatenate([x, y2])\n",
        "    return self.softmax(concat)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "hrxRXfcUX45h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dhkEJq7FPwuD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2ec525b-77bb-4678-fc34-1a56ef270c09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               multiple                  704       \n",
            "                                                                 \n",
            " dense_1 (Dense)             multiple                  110       \n",
            "                                                                 \n",
            " dense_2 (Dense)             multiple                  55        \n",
            "                                                                 \n",
            " softmax (Softmax)           multiple                  0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 869\n",
            "Trainable params: 869\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Print the model summary\n",
        "\n",
        "model = MyModel()\n",
        "model(tf.random.uniform([1,10]))\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hit3hvZhPwuD"
      },
      "source": [
        "***\n",
        "<a id=\"coding_tutorial_2\"></a>\n",
        "## Custom layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "aw-9uevAPwuE"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Layer, Softmax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LE1Iwnj6PwuE"
      },
      "source": [
        "#### Create custom layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Xmaf477LPwuE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "019a26fb-d238-4c9d-f24b-219be5b6b738"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([[ 0.19869033  0.06520779 -0.0231192 ]], shape=(1, 3), dtype=float32)\n",
            "[<tf.Variable 'Variable:0' shape=(5, 3) dtype=float32, numpy=\n",
            "array([[ 0.00246435,  0.03733185, -0.05712557],\n",
            "       [ 0.05111831,  0.0473155 ,  0.00717612],\n",
            "       [-0.03711481, -0.01631223, -0.02810367],\n",
            "       [ 0.07626984, -0.06978043,  0.00765672],\n",
            "       [ 0.10595264,  0.06665309,  0.0472772 ]], dtype=float32)>, <tf.Variable 'Variable:0' shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>]\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.python.ops.init_ops import Initializer\n",
        "# Create a custom layer\n",
        "\n",
        "class MyLayer(Layer):\n",
        "\n",
        "  def __init__(self, units, input_dim):\n",
        "    super(MyLayer, self).__init__()\n",
        "    self.w = self.add_weight(shape=(input_dim, units), \n",
        "                             initializer='random_normal')\n",
        "    self.b = self.add_weight(shape=(units), \n",
        "                             initializer='zeros')\n",
        "  def call(self, inputs):\n",
        "    return tf.matmul(inputs, self.w) + self.b\n",
        "dense_layer = MyLayer(3,5)\n",
        "x = tf.ones((1, 5))\n",
        "print(dense_layer(x))\n",
        "print(dense_layer.weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "2ddCFgrbPwuE"
      },
      "outputs": [],
      "source": [
        "# Specify trainable weights\n",
        "\n",
        "\n",
        "class MyLayer(Layer):\n",
        "\n",
        "  def __init__(self, units, input_dim):\n",
        "    super(MyLayer, self).__init__()\n",
        "    self.w = self.add_weight(shape=(input_dim, units), \n",
        "                             initializer='random_normal', trainable=False)\n",
        "    self.b = self.add_weight(shape=(units), \n",
        "                             initializer='zeros',trainable=False)\n",
        "  def call(self, inputs):\n",
        "    return tf.matmul(inputs, self.w) + self.b\n",
        "dense_layer = MyLayer(3,5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "KElrstIKPwuF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54a6a08f-887d-4cc6-9d5d-ec2901126dec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable weights: 0\n",
            "non-trainable weights: 2\n"
          ]
        }
      ],
      "source": [
        "print('trainable weights:', len(dense_layer.trainable_weights))\n",
        "print('non-trainable weights:', len(dense_layer.non_trainable_weights))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "u6OdhsEEPwuF"
      },
      "outputs": [],
      "source": [
        "# Create a custom layer to accumulate means of output values\n",
        "\n",
        "class MyLayerMean(Layer):\n",
        "\n",
        "  def __init__(self, units, input_dim):\n",
        "    super(MyLayerMean, self).__init__()\n",
        "    self.w = self.add_weight(shape=(input_dim, units), \n",
        "                             initializer='random_normal', trainable=False)\n",
        "    self.b = self.add_weight(shape=(units), \n",
        "                             initializer='zeros',trainable=False)\n",
        "    self.sum_activation = tf.Variable(initial_value=tf.zeros((units)),\n",
        "                                      trainable =False)\n",
        "    self.number_call = tf.Variable(initial_value=0,\n",
        "                                      trainable =False)\n",
        "  def call(self, inputs):\n",
        "    activations = tf.matmul(inputs, self.w)+self.b\n",
        "    self.sum_activation.assign_add(tf.reduce_sum(activations, axis=0))\n",
        "    self.number_call.assign_add(inputs.shape[0])\n",
        "\n",
        "    return activations, self.sum_activation / tf.cast(self.number_call, tf.float32)\n",
        "\n",
        "dense_layer = MyLayerMean(3,5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "64XpDaYfPwuF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ea1cdc1-0744-47bb-dbc9-f6f977117c8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.21102676 -0.10523226  0.12704943]\n",
            "[-0.21102676 -0.10523226  0.12704943]\n"
          ]
        }
      ],
      "source": [
        "# Test the layer\n",
        "\n",
        "y, activation_means = dense_layer(tf.ones((1, 5)))\n",
        "print(activation_means.numpy())\n",
        "\n",
        "y, activation_means = dense_layer(tf.ones((1, 5)))\n",
        "print(activation_means.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "pwu4kbsaPwuF"
      },
      "outputs": [],
      "source": [
        "# Create a Dropout layer as a custom layer\n",
        "\n",
        "class MyDropout(Layer):\n",
        "\n",
        "    def __init__(self, rate):\n",
        "        super(MyDropout, self).__init__()\n",
        "        self.rate = rate\n",
        "        \n",
        "    def call(self, inputs):\n",
        "        # Define forward pass for dropout layer\n",
        "        return tf.nn.dropout(inputs, rate=self.rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsGobNUpPwuG"
      },
      "source": [
        "#### Implement the custom layers into a model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "_BAHzCpnPwuG"
      },
      "outputs": [],
      "source": [
        "# Build the model using custom layers with the model subclassing API\n",
        "\n",
        "class MyModel(Model):\n",
        "\n",
        "    def __init__(self, units_1, input_dim_1, units_2, units_3):\n",
        "        super(MyModel, self).__init__()\n",
        "        # Define layers\n",
        "        self.layer_1 = MyLayer(units_1, input_dim_1)  \n",
        "        self.dropout_1 = MyDropout(0.5)\n",
        "        self.layer_2 = MyLayer(units_2, units_1)\n",
        "        self.dropout_2 = MyDropout(0.5)\n",
        "        self.layer_3 = MyLayer(units_3, units_2)\n",
        "        self.softmax = Softmax()\n",
        "    def call(self, inputs):\n",
        "        # Define forward pass\n",
        "        x = self.layer_1(inputs)\n",
        "        x = tf.nn.relu(x)\n",
        "        x = self.dropout_1(x)\n",
        "        x = self.layer_2(x)\n",
        "        x = tf.nn.relu(x)\n",
        "        x = self.dropout_2(x)\n",
        "        return self.softmax(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "BSurB_aQPwuG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df87c4f6-e55b-469c-bf65-3570ad672427"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[4.70893610e-05 4.70893610e-05 4.70893610e-05 4.70893610e-05\n",
            "  4.70893610e-05 4.70893610e-05 1.82888249e-03 4.70893610e-05\n",
            "  4.70893610e-05 4.70893610e-05 1.35578052e-03 1.03561186e-04\n",
            "  4.70893610e-05 4.70893610e-05 4.70893610e-05 4.70893610e-05\n",
            "  4.70893610e-05 4.70893610e-05 4.70893610e-05 2.20169750e-04\n",
            "  4.70893610e-05 4.70893610e-05 4.79717739e-04 4.70893610e-05\n",
            "  4.70893610e-05 5.65739865e-05 4.70893610e-05 4.70893610e-05\n",
            "  4.70893610e-05 2.35156869e-04 4.70893610e-05 4.70893610e-05\n",
            "  4.70893610e-05 7.30640313e-05 4.70893610e-05 4.70893610e-05\n",
            "  1.50927436e-02 4.70893610e-05 4.70893610e-05 8.68977726e-01\n",
            "  5.99781610e-02 4.70893610e-05 2.41999718e-04 4.70893610e-05\n",
            "  4.70893610e-05 4.70893610e-05 4.70893610e-05 4.70893610e-05\n",
            "  4.70893610e-05 4.70893610e-05 3.89800668e-02 4.70893610e-05\n",
            "  4.70893610e-05 4.64778161e-03 4.70893610e-05 4.70893610e-05\n",
            "  4.70893610e-05 4.70893610e-05 2.01213057e-04 1.56815292e-03\n",
            "  4.70893610e-05 4.70893610e-05 4.70893610e-05 3.74605134e-03]], shape=(1, 64), dtype=float32)\n",
            "Model: \"my_model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " my_layer_2 (MyLayer)        multiple                  640064    \n",
            "                                                                 \n",
            " my_dropout (MyDropout)      multiple                  0         \n",
            "                                                                 \n",
            " my_layer_3 (MyLayer)        multiple                  4160      \n",
            "                                                                 \n",
            " my_dropout_1 (MyDropout)    multiple                  0         \n",
            "                                                                 \n",
            " my_layer_4 (MyLayer)        multiple                  0 (unused)\n",
            "                                                                 \n",
            " softmax_1 (Softmax)         multiple                  0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 647,214\n",
            "Trainable params: 0\n",
            "Non-trainable params: 647,214\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Instantiate a model object\n",
        "\n",
        "model = MyModel(64,10000,64,46)\n",
        "print(model(tf.ones((1, 10000))))\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsHYfGyxPwuG"
      },
      "source": [
        "***\n",
        "<a id=\"coding_tutorial_3\"></a>\n",
        "## Automatic differentiation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ei_cNgDKPwuG"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FaFYdPEPwuH"
      },
      "source": [
        "#### Create synthetic data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "W02XxpNWPwuH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "e78fd230-78d0-4d15-e35e-faffd6f75686"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f6f4a139050>]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "\n",
        "# Create data from a noise contaminated linear model\n",
        "\n",
        "def MakeNoisyData(m, b, n=20):\n",
        "    x = tf.random.uniform(shape=(n,))\n",
        "    noise = tf.random.normal(shape=(len(x),), stddev=0.1)\n",
        "    y = m * x + b + noise\n",
        "    return x, y\n",
        "\n",
        "m=1\n",
        "b=2\n",
        "x_train, y_train = MakeNoisyData(m,b)\n",
        "plt.plot(x_train, y_train, 'b.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tumPItLKPwuH"
      },
      "source": [
        "#### Define a linear regression model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "DNkrD1zGPwuH"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Cnx5NYRpPwuH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a915505-a73b-453c-a47d-875cc9680fe0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[0.00828395 0.00533165 0.00222717 0.00057298 0.00692147 0.00949637\n",
            " 0.00501528 0.00135434 0.00226494 0.0081677  0.00202794 0.00627938\n",
            " 0.00590462 0.0047628  0.00695088 0.00115253 0.00079682 0.00454772\n",
            " 0.00168195 0.00546363], shape=(20,), dtype=float32)\n",
            "[<tf.Variable 'Variable:0' shape=(1,) dtype=float32, numpy=array([0.00976268], dtype=float32)>, <tf.Variable 'Variable:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>]\n"
          ]
        }
      ],
      "source": [
        "# Build a custom layer for the linear regression model\n",
        "\n",
        "class LinearLayer(Layer):\n",
        "  def __init__(self):\n",
        "    super(LinearLayer, self).__init__()\n",
        "    self.m = self.add_weight(shape=(1,), \n",
        "                             initializer='random_normal')\n",
        "    self.b = self.add_weight(shape=(1, ), \n",
        "                             initializer='zeros')\n",
        "  def call(self, inputs):\n",
        "    return self.m * inputs + self.b\n",
        "\n",
        "linear_regression = LinearLayer()\n",
        "\n",
        "print(linear_regression(x_train))\n",
        "print(linear_regression.weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0JIVjSSPwuH"
      },
      "source": [
        "#### Define the loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "hjwhebCFPwuH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57dea4fd-ec72-4b74-ccaa-6f8eb7bbbb16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting loss 6.0482697\n"
          ]
        }
      ],
      "source": [
        "# Define the mean squared error loss function\n",
        "\n",
        "def SquaredError(y_pred, y_true):\n",
        "    return tf.reduce_mean(tf.square(y_pred - y_true)) \n",
        "\n",
        "starting_loss = SquaredError(linear_regression(x_train), y_train)\n",
        "print(\"Starting loss\", starting_loss.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrK9r7bZPwuI"
      },
      "source": [
        "#### Train and plot the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "LeOme0b7PwuI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64e2a1dd-731c-48cd-c3e6-8ec180d860e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0, loss 6.048270 \n",
            "step 1, loss 4.661901 \n",
            "step 2, loss 3.593824 \n",
            "step 3, loss 2.770965 \n",
            "step 4, loss 2.137023 \n",
            "step 5, loss 1.648627 \n",
            "step 6, loss 1.272360 \n",
            "step 7, loss 0.982478 \n",
            "step 8, loss 0.759150 \n",
            "step 9, loss 0.587095 \n",
            "step 10, loss 0.454542 \n",
            "step 11, loss 0.352421 \n",
            "step 12, loss 0.273746 \n",
            "step 13, loss 0.213133 \n",
            "step 14, loss 0.166436 \n",
            "step 15, loss 0.130460 \n",
            "step 16, loss 0.102744 \n",
            "step 17, loss 0.081391 \n",
            "step 18, loss 0.064940 \n",
            "step 19, loss 0.052266 \n",
            "step 20, loss 0.042501 \n",
            "step 21, loss 0.034979 \n",
            "step 22, loss 0.029183 \n",
            "step 23, loss 0.024718 \n",
            "step 24, loss 0.021278 \n"
          ]
        }
      ],
      "source": [
        "# Implement a gradient descent training loop for the linear regression model\n",
        "learning_rate = 0.05\n",
        "steps = 25\n",
        "\n",
        "for i in range(steps):\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions = linear_regression(x_train)\n",
        "    loss = SquaredError(predictions, y_train)\n",
        "\n",
        "  gradients = tape.gradient(loss, linear_regression.trainable_variables)\n",
        "\n",
        "  linear_regression.m.assign_sub(learning_rate * gradients[0])\n",
        "  linear_regression.b.assign_sub(learning_rate * gradients[1])\n",
        "  print(\"step %d, loss %f \" % (i, loss.numpy()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "sJ3CdwtfPwuI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "outputId": "351e9956-5e93-4ff8-8f06-74fb540441c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "m:1,  trained m:[0.9494293]\n",
            "b:2,  trained b:[1.919378]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f6f49ffd350>]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Plot the learned regression model\n",
        "\n",
        "print(\"m:{},  trained m:{}\".format(m,linear_regression.m.numpy()))\n",
        "print(\"b:{},  trained b:{}\".format(b,linear_regression.b.numpy()))\n",
        "\n",
        "plt.plot(x_train, y_train, 'b.')\n",
        "\n",
        "x_linear_regression=np.linspace(min(x_train), max(x_train),50)\n",
        "plt.plot(x_linear_regression, linear_regression.m*x_linear_regression+linear_regression.b, 'r.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oY-V01cPPwuI"
      },
      "source": [
        "***\n",
        "<a id=\"coding_tutorial_4\"></a>\n",
        "## Custom training loops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "2L-c61OIPwuI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwX5FlpyPwuI"
      },
      "source": [
        "#### Build the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "xbxGgvRAPwuI"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Layer, Softmax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "MsKhicVOPwuJ"
      },
      "outputs": [],
      "source": [
        "# Define the custom layers and model\n",
        "class MyLayer(Layer):\n",
        "\n",
        "  def __init__(self, units):\n",
        "    super(MyLayer, self).__init__()\n",
        "    self.units = units\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.w = self.add_weight(shape=(input_shape[-1], self.units),\n",
        "                             initializer ='random_normal',\n",
        "                             name = 'kernel')\n",
        "    self.b = self.add_weight(shape=(self.units),\n",
        "                              initializer='zeros',\n",
        "                              name = 'bias')\n",
        "  def call(self, inputs):\n",
        "      return tf.matmul(inputs, self.w) + self.b\n",
        "\n",
        "class MyDropout(Layer):\n",
        "\n",
        "    def __init__(self, rate):\n",
        "        super(MyDropout, self).__init__()\n",
        "        self.rate = rate\n",
        "        \n",
        "    def call(self, inputs):\n",
        "        # Define forward pass for dropout layer\n",
        "        return tf.nn.dropout(inputs, rate=self.rate)\n",
        " \n",
        "    \n",
        "class MyModel(Model):\n",
        "\n",
        "    def __init__(self, units_1, units_2, units_3):\n",
        "        super(MyModel, self).__init__()\n",
        "        # Define layers\n",
        "        self.layer_1 = MyLayer(units_1)  \n",
        "        self.dropout_1 = MyDropout(0.5)\n",
        "        self.layer_2 = MyLayer(units_2)\n",
        "        self.dropout_2 = MyDropout(0.5)\n",
        "        self.layer_3 = MyLayer(units_3 )\n",
        "        self.softmax = Softmax()\n",
        "\n",
        "    def call(self, inputs):\n",
        "      x = self.layer_1(inputs)\n",
        "      x = tf.nn.relu(x)\n",
        "      x = self.dropout_1(x)\n",
        "      x = self.layer_2(x)\n",
        "      x = tf.nn.relu(x)\n",
        "      x = self.dropout_2(x)\n",
        "      x = self.layer_3(x)\n",
        "      return self.softmax(x)\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the model object\n",
        "\n",
        "model = MyModel(64, 64, 46)\n",
        "print(model(tf.ones((1, 10000))))\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C99XGPA9PqIb",
        "outputId": "45200277-84a8-4705-b872-d588eec414b9"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[0.13304032 0.00962026 0.00406004 0.00241754 0.01180195 0.01615354\n",
            "  0.01110915 0.02009499 0.00478623 0.00266359 0.00150164 0.02032366\n",
            "  0.00617647 0.08405445 0.02258542 0.1207489  0.02305907 0.09634505\n",
            "  0.01243536 0.00627639 0.01066421 0.03010367 0.00674756 0.00620644\n",
            "  0.01000352 0.0036553  0.04581668 0.00481903 0.01728416 0.00202547\n",
            "  0.0554806  0.00776013 0.0101869  0.01180182 0.00801578 0.02299351\n",
            "  0.00343927 0.04755261 0.00374032 0.01627345 0.0026289  0.03292407\n",
            "  0.00769633 0.00441138 0.01171388 0.00680098]], shape=(1, 46), dtype=float32)\n",
            "Model: \"my_model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " my_layer_5 (MyLayer)        multiple                  640064    \n",
            "                                                                 \n",
            " my_dropout_2 (MyDropout)    multiple                  0         \n",
            "                                                                 \n",
            " my_layer_6 (MyLayer)        multiple                  4160      \n",
            "                                                                 \n",
            " my_dropout_3 (MyDropout)    multiple                  0         \n",
            "                                                                 \n",
            " my_layer_7 (MyLayer)        multiple                  2990      \n",
            "                                                                 \n",
            " softmax_2 (Softmax)         multiple                  0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 647,214\n",
            "Trainable params: 647,214\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJIEbQIcPwuJ"
      },
      "source": [
        "#### Load the reuters dataset and define the class_names "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "TTC70Jg2PwuJ"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "\n",
        "from tensorflow.keras.datasets import reuters\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)\n",
        "\n",
        "class_names = ['cocoa','grain','veg-oil','earn','acq','wheat','copper','housing','money-supply',\n",
        "   'coffee','sugar','trade','reserves','ship','cotton','carcass','crude','nat-gas',\n",
        "   'cpi','money-fx','interest','gnp','meal-feed','alum','oilseed','gold','tin',\n",
        "   'strategic-metal','livestock','retail','ipi','iron-steel','rubber','heat','jobs',\n",
        "   'lei','bop','zinc','orange','pet-chem','dlr','gas','silver','wpi','hog','lead']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "nLBYyREkPwuJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58d717bc-97ea-4954-ddf2-4ca619725589"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label: earn\n"
          ]
        }
      ],
      "source": [
        "# Print the class of the first sample\n",
        "\n",
        "print(\"Label: {}\".format(class_names[train_labels[0]]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AvaR-UTPwuJ"
      },
      "source": [
        "#### Get the dataset word index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "jSGSRL9CPwuJ"
      },
      "outputs": [],
      "source": [
        "# Load the Reuters word index\n",
        "\n",
        "word_to_index = reuters.get_word_index()\n",
        "\n",
        "invert_word_index = dict([(value, key) for (key, value) in word_to_index.items()])\n",
        "text_news = ' '.join([invert_word_index.get(i - 3, '?') for i in train_data[0]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "9V14XAXXPwuK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ed02f5d-b419-487c-83ae-fbc206905482"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "? ? ? said as a result of its december acquisition of space co it expects earnings per share in 1987 of 1 15 to 1 30 dlrs per share up from 70 cts in 1986 the company said pretax net should rise to nine to 10 mln dlrs from six mln dlrs in 1986 and rental operation revenues to 19 to 22 mln dlrs from 12 5 mln dlrs it said cash flow per share this year should be 2 50 to three dlrs reuter 3\n"
          ]
        }
      ],
      "source": [
        "# Print the first data example sentence\n",
        "\n",
        "print(text_news)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEpF1c96PwuK"
      },
      "source": [
        "#### Preprocess the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "nhfGcu-wPwuK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ab7144a-e907-44c2-eea0-1326bb3a7c82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of x_train: (8982, 10000)\n",
            "Shape of x_test: (2246, 10000)\n"
          ]
        }
      ],
      "source": [
        "# Define a function that encodes the data into a 'bag of words' representation\n",
        "\n",
        "def bag_of_words(text_samples, elements=10000):\n",
        "    output = np.zeros((len(text_samples), elements))\n",
        "    for i, word in enumerate(text_samples):\n",
        "        output[i, word] = 1.\n",
        "    return output\n",
        "\n",
        "x_train = bag_of_words(train_data)\n",
        "x_test = bag_of_words(test_data)\n",
        "\n",
        "print(\"Shape of x_train:\", x_train.shape)\n",
        "print(\"Shape of x_test:\", x_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4A2f4l49PwuK"
      },
      "source": [
        "#### Define the loss function and optimizer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "7ObrMs0qPwuK"
      },
      "outputs": [],
      "source": [
        "# Define the categorical cross entropy loss and Adam optimizer\n",
        "\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "\n",
        "def loss(model, x, y, wd):\n",
        "    kernel_variables = []\n",
        "    for l in model.layers:\n",
        "        for w in l.weights:\n",
        "            if 'kernel' in w.name:\n",
        "                kernel_variables.append(w)\n",
        "    wd_penalty = wd * tf.reduce_sum([tf.reduce_sum(tf.square(k)) for k in kernel_variables])\n",
        "    y_ = model(x)\n",
        "    return loss_object(y_true=y, y_pred=y_) + wd_penalty\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNVLyRqDPwuK"
      },
      "source": [
        "#### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "s8UGKhC7PwuK"
      },
      "outputs": [],
      "source": [
        "# Define a function to compute the forward and backward pass\n",
        "\n",
        "def grad(model, inputs, targets, wd):\n",
        "    with tf.GradientTape() as tape:\n",
        "        loss_value = loss(model, inputs, targets, wd)\n",
        "    return loss_value, tape.gradient(loss_value, model.trainable_variables)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "PpqRr9N3PwuL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7561fb7-d61e-4f39-dd71-3cc4e9989384"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 000: Loss: 11.929, Accuracy: 6.250%\n",
            "Epoch 000: Loss: 11.786, Accuracy: 7.812%\n",
            "Epoch 000: Loss: 11.678, Accuracy: 8.333%\n",
            "Epoch 000: Loss: 11.555, Accuracy: 10.938%\n",
            "Epoch 000: Loss: 11.426, Accuracy: 11.875%\n",
            "Epoch 000: Loss: 11.304, Accuracy: 11.979%\n",
            "Epoch 000: Loss: 11.189, Accuracy: 13.393%\n",
            "Epoch 000: Loss: 11.071, Accuracy: 14.453%\n",
            "Epoch 000: Loss: 10.961, Accuracy: 15.972%\n",
            "Epoch 000: Loss: 10.852, Accuracy: 17.500%\n",
            "Epoch 000: Loss: 10.741, Accuracy: 19.318%\n",
            "Epoch 000: Loss: 10.633, Accuracy: 19.531%\n",
            "Epoch 000: Loss: 10.521, Accuracy: 19.952%\n",
            "Epoch 000: Loss: 10.418, Accuracy: 21.205%\n",
            "Epoch 000: Loss: 10.314, Accuracy: 21.875%\n",
            "Epoch 000: Loss: 10.213, Accuracy: 23.047%\n",
            "Epoch 000: Loss: 10.110, Accuracy: 23.162%\n",
            "Epoch 000: Loss: 10.012, Accuracy: 23.264%\n",
            "Epoch 000: Loss: 9.912, Accuracy: 24.013%\n",
            "Epoch 000: Loss: 9.814, Accuracy: 24.688%\n",
            "Epoch 000: Loss: 9.713, Accuracy: 24.851%\n",
            "Epoch 000: Loss: 9.612, Accuracy: 25.426%\n",
            "Epoch 000: Loss: 9.526, Accuracy: 25.408%\n",
            "Epoch 000: Loss: 9.430, Accuracy: 26.042%\n",
            "Epoch 000: Loss: 9.334, Accuracy: 26.500%\n",
            "Epoch 000: Loss: 9.247, Accuracy: 27.163%\n",
            "Epoch 000: Loss: 9.165, Accuracy: 27.546%\n",
            "Epoch 000: Loss: 9.075, Accuracy: 27.009%\n",
            "Epoch 000: Loss: 8.983, Accuracy: 27.047%\n",
            "Epoch 000: Loss: 8.888, Accuracy: 27.187%\n",
            "Epoch 000: Loss: 8.797, Accuracy: 27.722%\n",
            "Epoch 000: Loss: 8.710, Accuracy: 28.027%\n",
            "Epoch 000: Loss: 8.632, Accuracy: 28.409%\n",
            "Epoch 000: Loss: 8.550, Accuracy: 28.952%\n",
            "Epoch 000: Loss: 8.477, Accuracy: 28.929%\n",
            "Epoch 000: Loss: 8.396, Accuracy: 29.080%\n",
            "Epoch 000: Loss: 8.312, Accuracy: 29.223%\n",
            "Epoch 000: Loss: 8.234, Accuracy: 29.276%\n",
            "Epoch 000: Loss: 8.166, Accuracy: 29.167%\n",
            "Epoch 000: Loss: 8.083, Accuracy: 29.219%\n",
            "Epoch 000: Loss: 8.010, Accuracy: 29.421%\n",
            "Epoch 000: Loss: 7.941, Accuracy: 29.762%\n",
            "Epoch 000: Loss: 7.864, Accuracy: 29.942%\n",
            "Epoch 000: Loss: 7.813, Accuracy: 29.830%\n",
            "Epoch 000: Loss: 7.747, Accuracy: 29.792%\n",
            "Epoch 000: Loss: 7.686, Accuracy: 29.823%\n",
            "Epoch 000: Loss: 7.613, Accuracy: 30.053%\n",
            "Epoch 000: Loss: 7.553, Accuracy: 30.404%\n",
            "Epoch 000: Loss: 7.484, Accuracy: 30.548%\n",
            "Epoch 000: Loss: 7.419, Accuracy: 30.625%\n",
            "Epoch 000: Loss: 7.362, Accuracy: 30.760%\n",
            "Epoch 000: Loss: 7.300, Accuracy: 31.070%\n",
            "Epoch 000: Loss: 7.248, Accuracy: 31.309%\n",
            "Epoch 000: Loss: 7.198, Accuracy: 31.134%\n",
            "Epoch 000: Loss: 7.141, Accuracy: 31.364%\n",
            "Epoch 000: Loss: 7.086, Accuracy: 31.250%\n",
            "Epoch 000: Loss: 7.037, Accuracy: 31.250%\n",
            "Epoch 000: Loss: 6.986, Accuracy: 31.196%\n",
            "Epoch 000: Loss: 6.947, Accuracy: 31.091%\n",
            "Epoch 000: Loss: 6.891, Accuracy: 31.406%\n",
            "Epoch 000: Loss: 6.838, Accuracy: 31.609%\n",
            "Epoch 000: Loss: 6.787, Accuracy: 31.804%\n",
            "Epoch 000: Loss: 6.739, Accuracy: 32.093%\n",
            "Epoch 000: Loss: 6.699, Accuracy: 32.129%\n",
            "Epoch 000: Loss: 6.657, Accuracy: 32.163%\n",
            "Epoch 000: Loss: 6.611, Accuracy: 32.434%\n",
            "Epoch 000: Loss: 6.559, Accuracy: 32.882%\n",
            "Epoch 000: Loss: 6.510, Accuracy: 33.088%\n",
            "Epoch 000: Loss: 6.466, Accuracy: 33.288%\n",
            "Epoch 000: Loss: 6.425, Accuracy: 33.348%\n",
            "Epoch 000: Loss: 6.386, Accuracy: 33.451%\n",
            "Epoch 000: Loss: 6.344, Accuracy: 33.507%\n",
            "Epoch 000: Loss: 6.296, Accuracy: 33.818%\n",
            "Epoch 000: Loss: 6.258, Accuracy: 33.953%\n",
            "Epoch 000: Loss: 6.211, Accuracy: 34.208%\n",
            "Epoch 000: Loss: 6.171, Accuracy: 34.375%\n",
            "Epoch 000: Loss: 6.134, Accuracy: 34.456%\n",
            "Epoch 000: Loss: 6.096, Accuracy: 34.535%\n",
            "Epoch 000: Loss: 6.061, Accuracy: 34.612%\n",
            "Epoch 000: Loss: 6.030, Accuracy: 34.570%\n",
            "Epoch 000: Loss: 5.991, Accuracy: 34.799%\n",
            "Epoch 000: Loss: 5.951, Accuracy: 34.985%\n",
            "Epoch 000: Loss: 5.912, Accuracy: 35.203%\n",
            "Epoch 000: Loss: 5.881, Accuracy: 35.231%\n",
            "Epoch 000: Loss: 5.852, Accuracy: 35.147%\n",
            "Epoch 000: Loss: 5.816, Accuracy: 35.392%\n",
            "Epoch 000: Loss: 5.777, Accuracy: 35.668%\n",
            "Epoch 000: Loss: 5.741, Accuracy: 35.938%\n",
            "Epoch 000: Loss: 5.712, Accuracy: 36.060%\n",
            "Epoch 000: Loss: 5.682, Accuracy: 36.146%\n",
            "Epoch 000: Loss: 5.651, Accuracy: 36.298%\n",
            "Epoch 000: Loss: 5.623, Accuracy: 36.379%\n",
            "Epoch 000: Loss: 5.587, Accuracy: 36.626%\n",
            "Epoch 000: Loss: 5.565, Accuracy: 36.669%\n",
            "Epoch 000: Loss: 5.535, Accuracy: 36.875%\n",
            "Epoch 000: Loss: 5.512, Accuracy: 36.947%\n",
            "Epoch 000: Loss: 5.485, Accuracy: 37.017%\n",
            "Epoch 000: Loss: 5.458, Accuracy: 37.181%\n",
            "Epoch 000: Loss: 5.428, Accuracy: 37.342%\n",
            "Epoch 000: Loss: 5.405, Accuracy: 37.312%\n",
            "Epoch 000: Loss: 5.375, Accuracy: 37.469%\n",
            "Epoch 000: Loss: 5.353, Accuracy: 37.469%\n",
            "Epoch 000: Loss: 5.326, Accuracy: 37.682%\n",
            "Epoch 000: Loss: 5.299, Accuracy: 37.891%\n",
            "Epoch 000: Loss: 5.274, Accuracy: 38.006%\n",
            "Epoch 000: Loss: 5.251, Accuracy: 38.001%\n",
            "Epoch 000: Loss: 5.227, Accuracy: 38.084%\n",
            "Epoch 000: Loss: 5.203, Accuracy: 38.166%\n",
            "Epoch 000: Loss: 5.179, Accuracy: 38.245%\n",
            "Epoch 000: Loss: 5.155, Accuracy: 38.324%\n",
            "Epoch 000: Loss: 5.127, Accuracy: 38.570%\n",
            "Epoch 000: Loss: 5.102, Accuracy: 38.728%\n",
            "Epoch 000: Loss: 5.080, Accuracy: 38.772%\n",
            "Epoch 000: Loss: 5.064, Accuracy: 38.706%\n",
            "Epoch 000: Loss: 5.044, Accuracy: 38.750%\n",
            "Epoch 000: Loss: 5.020, Accuracy: 38.820%\n",
            "Epoch 000: Loss: 5.001, Accuracy: 38.916%\n",
            "Epoch 000: Loss: 4.978, Accuracy: 39.036%\n",
            "Epoch 000: Loss: 4.962, Accuracy: 39.076%\n",
            "Epoch 000: Loss: 4.939, Accuracy: 39.271%\n",
            "Epoch 000: Loss: 4.915, Accuracy: 39.411%\n",
            "Epoch 000: Loss: 4.897, Accuracy: 39.447%\n",
            "Epoch 000: Loss: 4.879, Accuracy: 39.431%\n",
            "Epoch 000: Loss: 4.858, Accuracy: 39.592%\n",
            "Epoch 000: Loss: 4.845, Accuracy: 39.650%\n",
            "Epoch 000: Loss: 4.825, Accuracy: 39.757%\n",
            "Epoch 000: Loss: 4.808, Accuracy: 39.788%\n",
            "Epoch 000: Loss: 4.790, Accuracy: 39.868%\n",
            "Epoch 000: Loss: 4.769, Accuracy: 40.044%\n",
            "Epoch 000: Loss: 4.748, Accuracy: 40.144%\n",
            "Epoch 000: Loss: 4.733, Accuracy: 40.172%\n",
            "Epoch 000: Loss: 4.713, Accuracy: 40.365%\n",
            "Epoch 000: Loss: 4.693, Accuracy: 40.555%\n",
            "Epoch 000: Loss: 4.678, Accuracy: 40.532%\n",
            "Epoch 000: Loss: 4.665, Accuracy: 40.486%\n",
            "Epoch 000: Loss: 4.646, Accuracy: 40.648%\n",
            "Epoch 000: Loss: 4.626, Accuracy: 40.807%\n",
            "Epoch 000: Loss: 4.605, Accuracy: 40.942%\n",
            "Epoch 000: Loss: 4.588, Accuracy: 40.985%\n",
            "Epoch 000: Loss: 4.572, Accuracy: 41.071%\n",
            "Epoch 000: Loss: 4.556, Accuracy: 41.090%\n",
            "Epoch 000: Loss: 4.537, Accuracy: 41.197%\n",
            "Epoch 000: Loss: 4.517, Accuracy: 41.368%\n",
            "Epoch 000: Loss: 4.502, Accuracy: 41.385%\n",
            "Epoch 000: Loss: 4.488, Accuracy: 41.422%\n",
            "Epoch 000: Loss: 4.472, Accuracy: 41.524%\n",
            "Epoch 000: Loss: 4.458, Accuracy: 41.560%\n",
            "Epoch 000: Loss: 4.444, Accuracy: 41.596%\n",
            "Epoch 000: Loss: 4.431, Accuracy: 41.653%\n",
            "Epoch 000: Loss: 4.413, Accuracy: 41.813%\n",
            "Epoch 000: Loss: 4.398, Accuracy: 41.929%\n",
            "Epoch 000: Loss: 4.385, Accuracy: 42.002%\n",
            "Epoch 000: Loss: 4.371, Accuracy: 42.075%\n",
            "Epoch 000: Loss: 4.355, Accuracy: 42.208%\n",
            "Epoch 000: Loss: 4.341, Accuracy: 42.278%\n",
            "Epoch 000: Loss: 4.327, Accuracy: 42.428%\n",
            "Epoch 000: Loss: 4.312, Accuracy: 42.516%\n",
            "Epoch 000: Loss: 4.302, Accuracy: 42.504%\n",
            "Epoch 000: Loss: 4.290, Accuracy: 42.610%\n",
            "Epoch 000: Loss: 4.278, Accuracy: 42.598%\n",
            "Epoch 000: Loss: 4.265, Accuracy: 42.663%\n",
            "Epoch 000: Loss: 4.250, Accuracy: 42.766%\n",
            "Epoch 000: Loss: 4.235, Accuracy: 42.887%\n",
            "Epoch 000: Loss: 4.221, Accuracy: 42.988%\n",
            "Epoch 000: Loss: 4.210, Accuracy: 43.049%\n",
            "Epoch 000: Loss: 4.199, Accuracy: 43.091%\n",
            "Epoch 000: Loss: 4.189, Accuracy: 43.114%\n",
            "Epoch 000: Loss: 4.180, Accuracy: 43.099%\n",
            "Epoch 000: Loss: 4.167, Accuracy: 43.177%\n",
            "Epoch 000: Loss: 4.155, Accuracy: 43.217%\n",
            "Epoch 000: Loss: 4.144, Accuracy: 43.238%\n",
            "Epoch 000: Loss: 4.128, Accuracy: 43.405%\n",
            "Epoch 000: Loss: 4.113, Accuracy: 43.551%\n",
            "Epoch 000: Loss: 4.100, Accuracy: 43.570%\n",
            "Epoch 000: Loss: 4.085, Accuracy: 43.750%\n",
            "Epoch 000: Loss: 4.074, Accuracy: 43.803%\n",
            "Epoch 000: Loss: 4.062, Accuracy: 43.891%\n",
            "Epoch 000: Loss: 4.054, Accuracy: 43.978%\n",
            "Epoch 000: Loss: 4.045, Accuracy: 44.012%\n",
            "Epoch 000: Loss: 4.036, Accuracy: 44.028%\n",
            "Epoch 000: Loss: 4.027, Accuracy: 44.044%\n",
            "Epoch 000: Loss: 4.013, Accuracy: 44.179%\n",
            "Epoch 000: Loss: 4.001, Accuracy: 44.262%\n",
            "Epoch 000: Loss: 3.992, Accuracy: 44.293%\n",
            "Epoch 000: Loss: 3.982, Accuracy: 44.324%\n",
            "Epoch 000: Loss: 3.975, Accuracy: 44.304%\n",
            "Epoch 000: Loss: 3.966, Accuracy: 44.368%\n",
            "Epoch 000: Loss: 3.953, Accuracy: 44.481%\n",
            "Epoch 000: Loss: 3.944, Accuracy: 44.461%\n",
            "Epoch 000: Loss: 3.934, Accuracy: 44.490%\n",
            "Epoch 000: Loss: 3.925, Accuracy: 44.486%\n",
            "Epoch 000: Loss: 3.918, Accuracy: 44.548%\n",
            "Epoch 000: Loss: 3.910, Accuracy: 44.641%\n",
            "Epoch 000: Loss: 3.903, Accuracy: 44.604%\n",
            "Epoch 000: Loss: 3.895, Accuracy: 44.599%\n",
            "Epoch 000: Loss: 3.885, Accuracy: 44.659%\n",
            "Epoch 000: Loss: 3.874, Accuracy: 44.765%\n",
            "Epoch 000: Loss: 3.864, Accuracy: 44.839%\n",
            "Epoch 000: Loss: 3.853, Accuracy: 44.912%\n",
            "Epoch 000: Loss: 3.844, Accuracy: 45.000%\n",
            "Epoch 000: Loss: 3.833, Accuracy: 45.072%\n",
            "Epoch 000: Loss: 3.828, Accuracy: 45.065%\n",
            "Epoch 000: Loss: 3.818, Accuracy: 45.105%\n",
            "Epoch 000: Loss: 3.810, Accuracy: 45.129%\n",
            "Epoch 000: Loss: 3.803, Accuracy: 45.137%\n",
            "Epoch 000: Loss: 3.794, Accuracy: 45.161%\n",
            "Epoch 000: Loss: 3.783, Accuracy: 45.290%\n",
            "Epoch 000: Loss: 3.774, Accuracy: 45.358%\n",
            "Epoch 000: Loss: 3.766, Accuracy: 45.410%\n",
            "Epoch 000: Loss: 3.759, Accuracy: 45.461%\n",
            "Epoch 000: Loss: 3.752, Accuracy: 45.483%\n",
            "Epoch 000: Loss: 3.743, Accuracy: 45.563%\n",
            "Epoch 000: Loss: 3.736, Accuracy: 45.599%\n",
            "Epoch 000: Loss: 3.731, Accuracy: 45.619%\n",
            "Epoch 000: Loss: 3.723, Accuracy: 45.683%\n",
            "Epoch 000: Loss: 3.716, Accuracy: 45.747%\n",
            "Epoch 000: Loss: 3.708, Accuracy: 45.795%\n",
            "Epoch 000: Loss: 3.700, Accuracy: 45.900%\n",
            "Epoch 000: Loss: 3.692, Accuracy: 45.962%\n",
            "Epoch 000: Loss: 3.683, Accuracy: 46.037%\n",
            "Epoch 000: Loss: 3.674, Accuracy: 46.083%\n",
            "Epoch 000: Loss: 3.667, Accuracy: 46.143%\n",
            "Epoch 000: Loss: 3.659, Accuracy: 46.258%\n",
            "Epoch 000: Loss: 3.652, Accuracy: 46.317%\n",
            "Epoch 000: Loss: 3.642, Accuracy: 46.431%\n",
            "Epoch 000: Loss: 3.635, Accuracy: 46.460%\n",
            "Epoch 000: Loss: 3.628, Accuracy: 46.503%\n",
            "Epoch 000: Loss: 3.620, Accuracy: 46.546%\n",
            "Epoch 000: Loss: 3.614, Accuracy: 46.588%\n",
            "Epoch 000: Loss: 3.607, Accuracy: 46.617%\n",
            "Epoch 000: Loss: 3.600, Accuracy: 46.645%\n",
            "Epoch 000: Loss: 3.591, Accuracy: 46.713%\n",
            "Epoch 000: Loss: 3.584, Accuracy: 46.727%\n",
            "Epoch 000: Loss: 3.576, Accuracy: 46.782%\n",
            "Epoch 000: Loss: 3.570, Accuracy: 46.782%\n",
            "Epoch 000: Loss: 3.562, Accuracy: 46.875%\n",
            "Epoch 000: Loss: 3.554, Accuracy: 46.980%\n",
            "Epoch 000: Loss: 3.550, Accuracy: 46.941%\n",
            "Epoch 000: Loss: 3.544, Accuracy: 46.953%\n",
            "Epoch 000: Loss: 3.538, Accuracy: 46.966%\n",
            "Epoch 000: Loss: 3.533, Accuracy: 47.005%\n",
            "Epoch 000: Loss: 3.527, Accuracy: 47.082%\n",
            "Epoch 000: Loss: 3.521, Accuracy: 47.119%\n",
            "Epoch 000: Loss: 3.515, Accuracy: 47.182%\n",
            "Epoch 000: Loss: 3.509, Accuracy: 47.219%\n",
            "Epoch 000: Loss: 3.503, Accuracy: 47.269%\n",
            "Epoch 000: Loss: 3.495, Accuracy: 47.356%\n",
            "Epoch 000: Loss: 3.487, Accuracy: 47.392%\n",
            "Epoch 000: Loss: 3.482, Accuracy: 47.390%\n",
            "Epoch 000: Loss: 3.475, Accuracy: 47.450%\n",
            "Epoch 000: Loss: 3.468, Accuracy: 47.522%\n",
            "Epoch 000: Loss: 3.461, Accuracy: 47.607%\n",
            "Epoch 000: Loss: 3.457, Accuracy: 47.641%\n",
            "Epoch 000: Loss: 3.452, Accuracy: 47.662%\n",
            "Epoch 000: Loss: 3.447, Accuracy: 47.708%\n",
            "Epoch 000: Loss: 3.441, Accuracy: 47.754%\n",
            "Epoch 000: Loss: 3.436, Accuracy: 47.811%\n",
            "Epoch 000: Loss: 3.429, Accuracy: 47.880%\n",
            "Epoch 000: Loss: 3.424, Accuracy: 47.925%\n",
            "Epoch 000: Loss: 3.418, Accuracy: 47.969%\n",
            "Epoch 000: Loss: 3.412, Accuracy: 48.000%\n",
            "Epoch 000: Loss: 3.407, Accuracy: 48.032%\n",
            "Epoch 000: Loss: 3.402, Accuracy: 48.051%\n",
            "Epoch 000: Loss: 3.397, Accuracy: 48.023%\n",
            "Epoch 000: Loss: 3.392, Accuracy: 48.042%\n",
            "Epoch 000: Loss: 3.387, Accuracy: 48.085%\n",
            "Epoch 000: Loss: 3.383, Accuracy: 48.116%\n",
            "Epoch 000: Loss: 3.377, Accuracy: 48.193%\n",
            "Epoch 000: Loss: 3.373, Accuracy: 48.176%\n",
            "Epoch 000: Loss: 3.370, Accuracy: 48.125%\n",
            "Epoch 000: Loss: 3.363, Accuracy: 48.178%\n",
            "Epoch 000: Loss: 3.358, Accuracy: 48.231%\n",
            "Epoch 000: Loss: 3.353, Accuracy: 48.283%\n",
            "Epoch 000: Loss: 3.349, Accuracy: 48.312%\n",
            "Epoch 000: Loss: 3.344, Accuracy: 48.341%\n",
            "Epoch 000: Loss: 3.338, Accuracy: 48.404%\n",
            "Epoch 000: Loss: 3.334, Accuracy: 48.409%\n",
            "Epoch 000: Loss: 3.331, Accuracy: 48.404%\n",
            "Epoch 000: Loss: 3.326, Accuracy: 48.477%\n",
            "Epoch 000: Loss: 3.320, Accuracy: 48.538%\n",
            "Epoch 000: Loss: 3.314, Accuracy: 48.619%\n",
            "Epoch 001: Loss: 1.463, Accuracy: 65.625%\n",
            "Epoch 001: Loss: 1.437, Accuracy: 68.750%\n",
            "Epoch 001: Loss: 1.616, Accuracy: 64.583%\n",
            "Epoch 001: Loss: 1.654, Accuracy: 66.406%\n",
            "Epoch 001: Loss: 1.616, Accuracy: 67.500%\n",
            "Epoch 001: Loss: 1.648, Accuracy: 65.625%\n",
            "Epoch 001: Loss: 1.776, Accuracy: 62.500%\n",
            "Epoch 001: Loss: 1.736, Accuracy: 63.672%\n",
            "Epoch 001: Loss: 1.864, Accuracy: 61.111%\n",
            "Epoch 001: Loss: 1.856, Accuracy: 60.938%\n",
            "Epoch 001: Loss: 1.813, Accuracy: 62.500%\n",
            "Epoch 001: Loss: 1.833, Accuracy: 61.979%\n",
            "Epoch 001: Loss: 1.847, Accuracy: 61.538%\n",
            "Epoch 001: Loss: 1.846, Accuracy: 61.607%\n",
            "Epoch 001: Loss: 1.863, Accuracy: 61.458%\n",
            "Epoch 001: Loss: 1.892, Accuracy: 61.133%\n",
            "Epoch 001: Loss: 1.914, Accuracy: 60.478%\n",
            "Epoch 001: Loss: 1.913, Accuracy: 60.938%\n",
            "Epoch 001: Loss: 1.914, Accuracy: 60.855%\n",
            "Epoch 001: Loss: 1.912, Accuracy: 60.781%\n",
            "Epoch 001: Loss: 1.919, Accuracy: 59.821%\n",
            "Epoch 001: Loss: 1.913, Accuracy: 60.227%\n",
            "Epoch 001: Loss: 1.943, Accuracy: 59.511%\n",
            "Epoch 001: Loss: 1.946, Accuracy: 59.505%\n",
            "Epoch 001: Loss: 1.931, Accuracy: 59.750%\n",
            "Epoch 001: Loss: 1.944, Accuracy: 59.615%\n",
            "Epoch 001: Loss: 1.970, Accuracy: 59.259%\n",
            "Epoch 001: Loss: 1.983, Accuracy: 58.482%\n",
            "Epoch 001: Loss: 1.980, Accuracy: 58.621%\n",
            "Epoch 001: Loss: 1.980, Accuracy: 58.542%\n",
            "Epoch 001: Loss: 1.960, Accuracy: 59.173%\n",
            "Epoch 001: Loss: 1.959, Accuracy: 59.180%\n",
            "Epoch 001: Loss: 1.973, Accuracy: 58.996%\n",
            "Epoch 001: Loss: 1.988, Accuracy: 58.824%\n",
            "Epoch 001: Loss: 2.007, Accuracy: 58.482%\n",
            "Epoch 001: Loss: 2.005, Accuracy: 58.507%\n",
            "Epoch 001: Loss: 2.006, Accuracy: 58.530%\n",
            "Epoch 001: Loss: 2.004, Accuracy: 58.635%\n",
            "Epoch 001: Loss: 2.013, Accuracy: 58.333%\n",
            "Epoch 001: Loss: 2.001, Accuracy: 58.516%\n",
            "Epoch 001: Loss: 2.000, Accuracy: 58.613%\n",
            "Epoch 001: Loss: 2.002, Accuracy: 58.780%\n",
            "Epoch 001: Loss: 1.993, Accuracy: 58.866%\n",
            "Epoch 001: Loss: 2.007, Accuracy: 58.452%\n",
            "Epoch 001: Loss: 2.002, Accuracy: 58.333%\n",
            "Epoch 001: Loss: 2.008, Accuracy: 58.220%\n",
            "Epoch 001: Loss: 1.997, Accuracy: 58.710%\n",
            "Epoch 001: Loss: 1.994, Accuracy: 58.854%\n",
            "Epoch 001: Loss: 1.988, Accuracy: 58.992%\n",
            "Epoch 001: Loss: 1.985, Accuracy: 59.062%\n",
            "Epoch 001: Loss: 1.990, Accuracy: 58.762%\n",
            "Epoch 001: Loss: 1.987, Accuracy: 58.834%\n",
            "Epoch 001: Loss: 1.993, Accuracy: 58.550%\n",
            "Epoch 001: Loss: 1.999, Accuracy: 58.333%\n",
            "Epoch 001: Loss: 1.995, Accuracy: 58.466%\n",
            "Epoch 001: Loss: 1.995, Accuracy: 58.538%\n",
            "Epoch 001: Loss: 1.997, Accuracy: 58.443%\n",
            "Epoch 001: Loss: 1.998, Accuracy: 58.297%\n",
            "Epoch 001: Loss: 2.007, Accuracy: 57.998%\n",
            "Epoch 001: Loss: 1.997, Accuracy: 58.177%\n",
            "Epoch 001: Loss: 1.994, Accuracy: 58.299%\n",
            "Epoch 001: Loss: 1.991, Accuracy: 58.417%\n",
            "Epoch 001: Loss: 1.996, Accuracy: 58.333%\n",
            "Epoch 001: Loss: 2.002, Accuracy: 58.203%\n",
            "Epoch 001: Loss: 2.008, Accuracy: 58.125%\n",
            "Epoch 001: Loss: 2.007, Accuracy: 58.144%\n",
            "Epoch 001: Loss: 1.998, Accuracy: 58.396%\n",
            "Epoch 001: Loss: 1.995, Accuracy: 58.410%\n",
            "Epoch 001: Loss: 1.997, Accuracy: 58.333%\n",
            "Epoch 001: Loss: 1.996, Accuracy: 58.304%\n",
            "Epoch 001: Loss: 2.009, Accuracy: 58.187%\n",
            "Epoch 001: Loss: 2.010, Accuracy: 58.333%\n",
            "Epoch 001: Loss: 2.004, Accuracy: 58.433%\n",
            "Epoch 001: Loss: 2.010, Accuracy: 58.404%\n",
            "Epoch 001: Loss: 1.999, Accuracy: 58.708%\n",
            "Epoch 001: Loss: 1.999, Accuracy: 58.676%\n",
            "Epoch 001: Loss: 1.998, Accuracy: 58.726%\n",
            "Epoch 001: Loss: 1.998, Accuracy: 58.694%\n",
            "Epoch 001: Loss: 1.997, Accuracy: 58.742%\n",
            "Epoch 001: Loss: 1.997, Accuracy: 58.711%\n",
            "Epoch 001: Loss: 1.995, Accuracy: 58.796%\n",
            "Epoch 001: Loss: 1.988, Accuracy: 58.918%\n",
            "Epoch 001: Loss: 1.984, Accuracy: 59.036%\n",
            "Epoch 001: Loss: 1.986, Accuracy: 58.929%\n",
            "Epoch 001: Loss: 1.992, Accuracy: 58.713%\n",
            "Epoch 001: Loss: 1.988, Accuracy: 58.903%\n",
            "Epoch 001: Loss: 1.983, Accuracy: 58.980%\n",
            "Epoch 001: Loss: 1.978, Accuracy: 59.162%\n",
            "Epoch 001: Loss: 1.979, Accuracy: 59.094%\n",
            "Epoch 001: Loss: 1.980, Accuracy: 59.028%\n",
            "Epoch 001: Loss: 1.979, Accuracy: 58.997%\n",
            "Epoch 001: Loss: 1.982, Accuracy: 58.967%\n",
            "Epoch 001: Loss: 1.977, Accuracy: 59.039%\n",
            "Epoch 001: Loss: 1.985, Accuracy: 58.876%\n",
            "Epoch 001: Loss: 1.983, Accuracy: 59.013%\n",
            "Epoch 001: Loss: 1.987, Accuracy: 58.887%\n",
            "Epoch 001: Loss: 1.991, Accuracy: 58.763%\n",
            "Epoch 001: Loss: 1.990, Accuracy: 58.897%\n",
            "Epoch 001: Loss: 1.987, Accuracy: 58.996%\n",
            "Epoch 001: Loss: 1.991, Accuracy: 58.906%\n",
            "Epoch 001: Loss: 1.990, Accuracy: 58.911%\n",
            "Epoch 001: Loss: 1.992, Accuracy: 58.793%\n",
            "Epoch 001: Loss: 1.994, Accuracy: 58.768%\n",
            "Epoch 001: Loss: 1.992, Accuracy: 58.834%\n",
            "Epoch 001: Loss: 1.992, Accuracy: 58.780%\n",
            "Epoch 001: Loss: 1.992, Accuracy: 58.785%\n",
            "Epoch 001: Loss: 1.992, Accuracy: 58.820%\n",
            "Epoch 001: Loss: 1.993, Accuracy: 58.738%\n",
            "Epoch 001: Loss: 1.993, Accuracy: 58.744%\n",
            "Epoch 001: Loss: 1.991, Accuracy: 58.778%\n",
            "Epoch 001: Loss: 1.987, Accuracy: 58.896%\n",
            "Epoch 001: Loss: 1.984, Accuracy: 59.040%\n",
            "Epoch 001: Loss: 1.984, Accuracy: 59.043%\n",
            "Epoch 001: Loss: 1.988, Accuracy: 58.936%\n",
            "Epoch 001: Loss: 1.988, Accuracy: 58.940%\n",
            "Epoch 001: Loss: 1.986, Accuracy: 59.025%\n",
            "Epoch 001: Loss: 1.988, Accuracy: 59.001%\n",
            "Epoch 001: Loss: 1.986, Accuracy: 59.084%\n",
            "Epoch 001: Loss: 1.991, Accuracy: 59.007%\n",
            "Epoch 001: Loss: 1.988, Accuracy: 59.141%\n",
            "Epoch 001: Loss: 1.986, Accuracy: 59.168%\n",
            "Epoch 001: Loss: 1.985, Accuracy: 59.170%\n",
            "Epoch 001: Loss: 1.988, Accuracy: 59.096%\n",
            "Epoch 001: Loss: 1.985, Accuracy: 59.173%\n",
            "Epoch 001: Loss: 1.991, Accuracy: 59.075%\n",
            "Epoch 001: Loss: 1.991, Accuracy: 59.053%\n",
            "Epoch 001: Loss: 1.991, Accuracy: 59.031%\n",
            "Epoch 001: Loss: 1.991, Accuracy: 59.009%\n",
            "Epoch 001: Loss: 1.988, Accuracy: 59.133%\n",
            "Epoch 001: Loss: 1.984, Accuracy: 59.207%\n",
            "Epoch 001: Loss: 1.986, Accuracy: 59.089%\n",
            "Epoch 001: Loss: 1.983, Accuracy: 59.162%\n",
            "Epoch 001: Loss: 1.981, Accuracy: 59.234%\n",
            "Epoch 001: Loss: 1.981, Accuracy: 59.258%\n",
            "Epoch 001: Loss: 1.984, Accuracy: 59.213%\n",
            "Epoch 001: Loss: 1.982, Accuracy: 59.283%\n",
            "Epoch 001: Loss: 1.980, Accuracy: 59.375%\n",
            "Epoch 001: Loss: 1.976, Accuracy: 59.511%\n",
            "Epoch 001: Loss: 1.974, Accuracy: 59.555%\n",
            "Epoch 001: Loss: 1.973, Accuracy: 59.554%\n",
            "Epoch 001: Loss: 1.974, Accuracy: 59.486%\n",
            "Epoch 001: Loss: 1.970, Accuracy: 59.507%\n",
            "Epoch 001: Loss: 1.966, Accuracy: 59.572%\n",
            "Epoch 001: Loss: 1.966, Accuracy: 59.570%\n",
            "Epoch 001: Loss: 1.966, Accuracy: 59.612%\n",
            "Epoch 001: Loss: 1.965, Accuracy: 59.675%\n",
            "Epoch 001: Loss: 1.965, Accuracy: 59.651%\n",
            "Epoch 001: Loss: 1.968, Accuracy: 59.607%\n",
            "Epoch 001: Loss: 1.970, Accuracy: 59.648%\n",
            "Epoch 001: Loss: 1.966, Accuracy: 59.771%\n",
            "Epoch 001: Loss: 1.965, Accuracy: 59.830%\n",
            "Epoch 001: Loss: 1.965, Accuracy: 59.807%\n",
            "Epoch 001: Loss: 1.964, Accuracy: 59.824%\n",
            "Epoch 001: Loss: 1.961, Accuracy: 59.943%\n",
            "Epoch 001: Loss: 1.960, Accuracy: 59.919%\n",
            "Epoch 001: Loss: 1.961, Accuracy: 59.956%\n",
            "Epoch 001: Loss: 1.958, Accuracy: 59.992%\n",
            "Epoch 001: Loss: 1.962, Accuracy: 59.929%\n",
            "Epoch 001: Loss: 1.962, Accuracy: 59.925%\n",
            "Epoch 001: Loss: 1.964, Accuracy: 59.863%\n",
            "Epoch 001: Loss: 1.965, Accuracy: 59.880%\n",
            "Epoch 001: Loss: 1.963, Accuracy: 59.934%\n",
            "Epoch 001: Loss: 1.960, Accuracy: 59.969%\n",
            "Epoch 001: Loss: 1.960, Accuracy: 60.004%\n",
            "Epoch 001: Loss: 1.960, Accuracy: 60.076%\n",
            "Epoch 001: Loss: 1.961, Accuracy: 60.090%\n",
            "Epoch 001: Loss: 1.962, Accuracy: 60.049%\n",
            "Epoch 001: Loss: 1.964, Accuracy: 59.970%\n",
            "Epoch 001: Loss: 1.964, Accuracy: 59.967%\n",
            "Epoch 001: Loss: 1.964, Accuracy: 60.000%\n",
            "Epoch 001: Loss: 1.963, Accuracy: 60.051%\n",
            "Epoch 001: Loss: 1.959, Accuracy: 60.193%\n",
            "Epoch 001: Loss: 1.955, Accuracy: 60.296%\n",
            "Epoch 001: Loss: 1.952, Accuracy: 60.327%\n",
            "Epoch 001: Loss: 1.949, Accuracy: 60.446%\n",
            "Epoch 001: Loss: 1.948, Accuracy: 60.476%\n",
            "Epoch 001: Loss: 1.948, Accuracy: 60.487%\n",
            "Epoch 001: Loss: 1.948, Accuracy: 60.516%\n",
            "Epoch 001: Loss: 1.950, Accuracy: 60.440%\n",
            "Epoch 001: Loss: 1.950, Accuracy: 60.417%\n",
            "Epoch 001: Loss: 1.950, Accuracy: 60.342%\n",
            "Epoch 001: Loss: 1.946, Accuracy: 60.457%\n",
            "Epoch 001: Loss: 1.945, Accuracy: 60.502%\n",
            "Epoch 001: Loss: 1.946, Accuracy: 60.479%\n",
            "Epoch 001: Loss: 1.947, Accuracy: 60.473%\n",
            "Epoch 001: Loss: 1.949, Accuracy: 60.433%\n",
            "Epoch 001: Loss: 1.949, Accuracy: 60.428%\n",
            "Epoch 001: Loss: 1.945, Accuracy: 60.539%\n",
            "Epoch 001: Loss: 1.945, Accuracy: 60.532%\n",
            "Epoch 001: Loss: 1.944, Accuracy: 60.576%\n",
            "Epoch 001: Loss: 1.943, Accuracy: 60.586%\n",
            "Epoch 001: Loss: 1.945, Accuracy: 60.612%\n",
            "Epoch 001: Loss: 1.945, Accuracy: 60.573%\n",
            "Epoch 001: Loss: 1.947, Accuracy: 60.535%\n",
            "Epoch 001: Loss: 1.948, Accuracy: 60.481%\n",
            "Epoch 001: Loss: 1.947, Accuracy: 60.523%\n",
            "Epoch 001: Loss: 1.946, Accuracy: 60.565%\n",
            "Epoch 001: Loss: 1.944, Accuracy: 60.622%\n",
            "Epoch 001: Loss: 1.942, Accuracy: 60.647%\n",
            "Epoch 001: Loss: 1.941, Accuracy: 60.703%\n",
            "Epoch 001: Loss: 1.939, Accuracy: 60.759%\n",
            "Epoch 001: Loss: 1.942, Accuracy: 60.705%\n",
            "Epoch 001: Loss: 1.940, Accuracy: 60.730%\n",
            "Epoch 001: Loss: 1.940, Accuracy: 60.723%\n",
            "Epoch 001: Loss: 1.940, Accuracy: 60.686%\n",
            "Epoch 001: Loss: 1.940, Accuracy: 60.695%\n",
            "Epoch 001: Loss: 1.936, Accuracy: 60.764%\n",
            "Epoch 001: Loss: 1.936, Accuracy: 60.817%\n",
            "Epoch 001: Loss: 1.936, Accuracy: 60.825%\n",
            "Epoch 001: Loss: 1.935, Accuracy: 60.833%\n",
            "Epoch 001: Loss: 1.936, Accuracy: 60.812%\n",
            "Epoch 001: Loss: 1.934, Accuracy: 60.820%\n",
            "Epoch 001: Loss: 1.934, Accuracy: 60.754%\n",
            "Epoch 001: Loss: 1.936, Accuracy: 60.718%\n",
            "Epoch 001: Loss: 1.937, Accuracy: 60.683%\n",
            "Epoch 001: Loss: 1.937, Accuracy: 60.706%\n",
            "Epoch 001: Loss: 1.936, Accuracy: 60.685%\n",
            "Epoch 001: Loss: 1.935, Accuracy: 60.708%\n",
            "Epoch 001: Loss: 1.935, Accuracy: 60.745%\n",
            "Epoch 001: Loss: 1.933, Accuracy: 60.824%\n",
            "Epoch 001: Loss: 1.931, Accuracy: 60.831%\n",
            "Epoch 001: Loss: 1.931, Accuracy: 60.853%\n",
            "Epoch 001: Loss: 1.930, Accuracy: 60.902%\n",
            "Epoch 001: Loss: 1.930, Accuracy: 60.924%\n",
            "Epoch 001: Loss: 1.927, Accuracy: 61.014%\n",
            "Epoch 001: Loss: 1.926, Accuracy: 61.007%\n",
            "Epoch 001: Loss: 1.925, Accuracy: 61.055%\n",
            "Epoch 001: Loss: 1.924, Accuracy: 61.088%\n",
            "Epoch 001: Loss: 1.924, Accuracy: 61.094%\n",
            "Epoch 001: Loss: 1.924, Accuracy: 61.073%\n",
            "Epoch 001: Loss: 1.924, Accuracy: 61.080%\n",
            "Epoch 001: Loss: 1.921, Accuracy: 61.166%\n",
            "Epoch 001: Loss: 1.921, Accuracy: 61.145%\n",
            "Epoch 001: Loss: 1.919, Accuracy: 61.165%\n",
            "Epoch 001: Loss: 1.920, Accuracy: 61.130%\n",
            "Epoch 001: Loss: 1.919, Accuracy: 61.189%\n",
            "Epoch 001: Loss: 1.918, Accuracy: 61.234%\n",
            "Epoch 001: Loss: 1.919, Accuracy: 61.187%\n",
            "Epoch 001: Loss: 1.920, Accuracy: 61.166%\n",
            "Epoch 001: Loss: 1.920, Accuracy: 61.185%\n",
            "Epoch 001: Loss: 1.920, Accuracy: 61.164%\n",
            "Epoch 001: Loss: 1.920, Accuracy: 61.183%\n",
            "Epoch 001: Loss: 1.920, Accuracy: 61.227%\n",
            "Epoch 001: Loss: 1.920, Accuracy: 61.258%\n",
            "Epoch 001: Loss: 1.921, Accuracy: 61.250%\n",
            "Epoch 001: Loss: 1.920, Accuracy: 61.319%\n",
            "Epoch 001: Loss: 1.918, Accuracy: 61.349%\n",
            "Epoch 001: Loss: 1.916, Accuracy: 61.366%\n",
            "Epoch 001: Loss: 1.917, Accuracy: 61.333%\n",
            "Epoch 001: Loss: 1.916, Accuracy: 61.387%\n",
            "Epoch 001: Loss: 1.915, Accuracy: 61.454%\n",
            "Epoch 001: Loss: 1.914, Accuracy: 61.471%\n",
            "Epoch 001: Loss: 1.915, Accuracy: 61.438%\n",
            "Epoch 001: Loss: 1.916, Accuracy: 61.417%\n",
            "Epoch 001: Loss: 1.915, Accuracy: 61.446%\n",
            "Epoch 001: Loss: 1.915, Accuracy: 61.475%\n",
            "Epoch 001: Loss: 1.915, Accuracy: 61.503%\n",
            "Epoch 001: Loss: 1.915, Accuracy: 61.555%\n",
            "Epoch 001: Loss: 1.916, Accuracy: 61.571%\n",
            "Epoch 001: Loss: 1.915, Accuracy: 61.587%\n",
            "Epoch 001: Loss: 1.914, Accuracy: 61.626%\n",
            "Epoch 001: Loss: 1.914, Accuracy: 61.641%\n",
            "Epoch 001: Loss: 1.914, Accuracy: 61.621%\n",
            "Epoch 001: Loss: 1.914, Accuracy: 61.600%\n",
            "Epoch 001: Loss: 1.914, Accuracy: 61.592%\n",
            "Epoch 001: Loss: 1.914, Accuracy: 61.607%\n",
            "Epoch 001: Loss: 1.914, Accuracy: 61.610%\n",
            "Epoch 001: Loss: 1.913, Accuracy: 61.649%\n",
            "Epoch 001: Loss: 1.914, Accuracy: 61.629%\n",
            "Epoch 001: Loss: 1.915, Accuracy: 61.574%\n",
            "Epoch 001: Loss: 1.914, Accuracy: 61.624%\n",
            "Epoch 001: Loss: 1.914, Accuracy: 61.627%\n",
            "Epoch 001: Loss: 1.913, Accuracy: 61.641%\n",
            "Epoch 001: Loss: 1.913, Accuracy: 61.679%\n",
            "Epoch 001: Loss: 1.914, Accuracy: 61.705%\n",
            "Epoch 001: Loss: 1.912, Accuracy: 61.753%\n",
            "Epoch 001: Loss: 1.913, Accuracy: 61.710%\n",
            "Epoch 001: Loss: 1.913, Accuracy: 61.758%\n",
            "Epoch 001: Loss: 1.913, Accuracy: 61.817%\n",
            "Epoch 001: Loss: 1.913, Accuracy: 61.842%\n",
            "Epoch 001: Loss: 1.912, Accuracy: 61.868%\n",
            "Epoch 002: Loss: 1.341, Accuracy: 81.250%\n",
            "Epoch 002: Loss: 1.288, Accuracy: 78.125%\n",
            "Epoch 002: Loss: 1.488, Accuracy: 71.875%\n",
            "Epoch 002: Loss: 1.504, Accuracy: 72.656%\n",
            "Epoch 002: Loss: 1.510, Accuracy: 73.750%\n",
            "Epoch 002: Loss: 1.536, Accuracy: 71.354%\n",
            "Epoch 002: Loss: 1.664, Accuracy: 67.411%\n",
            "Epoch 002: Loss: 1.594, Accuracy: 69.531%\n",
            "Epoch 002: Loss: 1.723, Accuracy: 65.625%\n",
            "Epoch 002: Loss: 1.706, Accuracy: 66.250%\n",
            "Epoch 002: Loss: 1.676, Accuracy: 66.761%\n",
            "Epoch 002: Loss: 1.691, Accuracy: 67.188%\n",
            "Epoch 002: Loss: 1.701, Accuracy: 66.827%\n",
            "Epoch 002: Loss: 1.709, Accuracy: 66.964%\n",
            "Epoch 002: Loss: 1.736, Accuracy: 67.083%\n",
            "Epoch 002: Loss: 1.772, Accuracy: 66.406%\n",
            "Epoch 002: Loss: 1.786, Accuracy: 65.993%\n",
            "Epoch 002: Loss: 1.781, Accuracy: 66.146%\n",
            "Epoch 002: Loss: 1.773, Accuracy: 66.283%\n",
            "Epoch 002: Loss: 1.768, Accuracy: 66.094%\n",
            "Epoch 002: Loss: 1.769, Accuracy: 66.071%\n",
            "Epoch 002: Loss: 1.753, Accuracy: 66.335%\n",
            "Epoch 002: Loss: 1.785, Accuracy: 65.897%\n",
            "Epoch 002: Loss: 1.788, Accuracy: 65.885%\n",
            "Epoch 002: Loss: 1.774, Accuracy: 66.375%\n",
            "Epoch 002: Loss: 1.792, Accuracy: 66.346%\n",
            "Epoch 002: Loss: 1.821, Accuracy: 65.278%\n",
            "Epoch 002: Loss: 1.831, Accuracy: 64.621%\n",
            "Epoch 002: Loss: 1.828, Accuracy: 64.763%\n",
            "Epoch 002: Loss: 1.828, Accuracy: 65.000%\n",
            "Epoch 002: Loss: 1.812, Accuracy: 65.423%\n",
            "Epoch 002: Loss: 1.810, Accuracy: 65.527%\n",
            "Epoch 002: Loss: 1.820, Accuracy: 65.341%\n",
            "Epoch 002: Loss: 1.830, Accuracy: 64.982%\n",
            "Epoch 002: Loss: 1.847, Accuracy: 64.821%\n",
            "Epoch 002: Loss: 1.846, Accuracy: 64.844%\n",
            "Epoch 002: Loss: 1.850, Accuracy: 64.865%\n",
            "Epoch 002: Loss: 1.849, Accuracy: 64.967%\n",
            "Epoch 002: Loss: 1.853, Accuracy: 64.984%\n",
            "Epoch 002: Loss: 1.841, Accuracy: 65.234%\n",
            "Epoch 002: Loss: 1.845, Accuracy: 65.320%\n",
            "Epoch 002: Loss: 1.847, Accuracy: 65.327%\n",
            "Epoch 002: Loss: 1.839, Accuracy: 65.552%\n",
            "Epoch 002: Loss: 1.855, Accuracy: 65.128%\n",
            "Epoch 002: Loss: 1.852, Accuracy: 65.139%\n",
            "Epoch 002: Loss: 1.865, Accuracy: 64.742%\n",
            "Epoch 002: Loss: 1.860, Accuracy: 65.027%\n",
            "Epoch 002: Loss: 1.857, Accuracy: 65.169%\n",
            "Epoch 002: Loss: 1.853, Accuracy: 65.115%\n",
            "Epoch 002: Loss: 1.852, Accuracy: 65.062%\n",
            "Epoch 002: Loss: 1.853, Accuracy: 65.074%\n",
            "Epoch 002: Loss: 1.850, Accuracy: 65.264%\n",
            "Epoch 002: Loss: 1.854, Accuracy: 65.094%\n",
            "Epoch 002: Loss: 1.857, Accuracy: 64.988%\n",
            "Epoch 002: Loss: 1.854, Accuracy: 65.000%\n",
            "Epoch 002: Loss: 1.853, Accuracy: 64.955%\n",
            "Epoch 002: Loss: 1.852, Accuracy: 64.967%\n",
            "Epoch 002: Loss: 1.857, Accuracy: 64.763%\n",
            "Epoch 002: Loss: 1.866, Accuracy: 64.566%\n",
            "Epoch 002: Loss: 1.860, Accuracy: 64.688%\n",
            "Epoch 002: Loss: 1.854, Accuracy: 64.754%\n",
            "Epoch 002: Loss: 1.850, Accuracy: 64.667%\n",
            "Epoch 002: Loss: 1.854, Accuracy: 64.583%\n",
            "Epoch 002: Loss: 1.860, Accuracy: 64.404%\n",
            "Epoch 002: Loss: 1.872, Accuracy: 64.231%\n",
            "Epoch 002: Loss: 1.872, Accuracy: 64.299%\n",
            "Epoch 002: Loss: 1.863, Accuracy: 64.552%\n",
            "Epoch 002: Loss: 1.861, Accuracy: 64.614%\n",
            "Epoch 002: Loss: 1.862, Accuracy: 64.674%\n",
            "Epoch 002: Loss: 1.862, Accuracy: 64.688%\n",
            "Epoch 002: Loss: 1.870, Accuracy: 64.525%\n",
            "Epoch 002: Loss: 1.871, Accuracy: 64.497%\n",
            "Epoch 002: Loss: 1.868, Accuracy: 64.555%\n",
            "Epoch 002: Loss: 1.875, Accuracy: 64.443%\n",
            "Epoch 002: Loss: 1.866, Accuracy: 64.708%\n",
            "Epoch 002: Loss: 1.865, Accuracy: 64.762%\n",
            "Epoch 002: Loss: 1.865, Accuracy: 64.732%\n",
            "Epoch 002: Loss: 1.864, Accuracy: 64.623%\n",
            "Epoch 002: Loss: 1.865, Accuracy: 64.597%\n",
            "Epoch 002: Loss: 1.866, Accuracy: 64.609%\n",
            "Epoch 002: Loss: 1.864, Accuracy: 64.738%\n",
            "Epoch 002: Loss: 1.860, Accuracy: 64.825%\n",
            "Epoch 002: Loss: 1.856, Accuracy: 65.023%\n",
            "Epoch 002: Loss: 1.858, Accuracy: 64.993%\n",
            "Epoch 002: Loss: 1.864, Accuracy: 64.853%\n",
            "Epoch 002: Loss: 1.860, Accuracy: 64.935%\n",
            "Epoch 002: Loss: 1.855, Accuracy: 65.086%\n",
            "Epoch 002: Loss: 1.853, Accuracy: 65.270%\n",
            "Epoch 002: Loss: 1.857, Accuracy: 65.169%\n",
            "Epoch 002: Loss: 1.859, Accuracy: 65.139%\n",
            "Epoch 002: Loss: 1.858, Accuracy: 65.041%\n",
            "Epoch 002: Loss: 1.860, Accuracy: 64.946%\n",
            "Epoch 002: Loss: 1.855, Accuracy: 65.054%\n",
            "Epoch 002: Loss: 1.861, Accuracy: 64.960%\n",
            "Epoch 002: Loss: 1.858, Accuracy: 64.901%\n",
            "Epoch 002: Loss: 1.861, Accuracy: 64.779%\n",
            "Epoch 002: Loss: 1.865, Accuracy: 64.691%\n",
            "Epoch 002: Loss: 1.865, Accuracy: 64.732%\n",
            "Epoch 002: Loss: 1.861, Accuracy: 64.804%\n",
            "Epoch 002: Loss: 1.866, Accuracy: 64.719%\n",
            "Epoch 002: Loss: 1.864, Accuracy: 64.790%\n",
            "Epoch 002: Loss: 1.866, Accuracy: 64.706%\n",
            "Epoch 002: Loss: 1.868, Accuracy: 64.715%\n",
            "Epoch 002: Loss: 1.867, Accuracy: 64.784%\n",
            "Epoch 002: Loss: 1.869, Accuracy: 64.821%\n",
            "Epoch 002: Loss: 1.869, Accuracy: 64.829%\n",
            "Epoch 002: Loss: 1.868, Accuracy: 64.778%\n",
            "Epoch 002: Loss: 1.868, Accuracy: 64.786%\n",
            "Epoch 002: Loss: 1.867, Accuracy: 64.822%\n",
            "Epoch 002: Loss: 1.866, Accuracy: 64.801%\n",
            "Epoch 002: Loss: 1.862, Accuracy: 64.921%\n",
            "Epoch 002: Loss: 1.859, Accuracy: 65.011%\n",
            "Epoch 002: Loss: 1.860, Accuracy: 64.989%\n",
            "Epoch 002: Loss: 1.862, Accuracy: 64.912%\n",
            "Epoch 002: Loss: 1.865, Accuracy: 64.891%\n",
            "Epoch 002: Loss: 1.864, Accuracy: 64.898%\n",
            "Epoch 002: Loss: 1.866, Accuracy: 64.850%\n",
            "Epoch 002: Loss: 1.865, Accuracy: 64.883%\n",
            "Epoch 002: Loss: 1.868, Accuracy: 64.758%\n",
            "Epoch 002: Loss: 1.866, Accuracy: 64.818%\n",
            "Epoch 002: Loss: 1.865, Accuracy: 64.850%\n",
            "Epoch 002: Loss: 1.865, Accuracy: 64.805%\n",
            "Epoch 002: Loss: 1.869, Accuracy: 64.787%\n",
            "Epoch 002: Loss: 1.867, Accuracy: 64.793%\n",
            "Epoch 002: Loss: 1.874, Accuracy: 64.675%\n",
            "Epoch 002: Loss: 1.872, Accuracy: 64.658%\n",
            "Epoch 002: Loss: 1.874, Accuracy: 64.641%\n",
            "Epoch 002: Loss: 1.875, Accuracy: 64.624%\n",
            "Epoch 002: Loss: 1.873, Accuracy: 64.704%\n",
            "Epoch 002: Loss: 1.870, Accuracy: 64.856%\n",
            "Epoch 002: Loss: 1.872, Accuracy: 64.814%\n",
            "Epoch 002: Loss: 1.868, Accuracy: 64.867%\n",
            "Epoch 002: Loss: 1.865, Accuracy: 64.944%\n",
            "Epoch 002: Loss: 1.864, Accuracy: 64.949%\n",
            "Epoch 002: Loss: 1.869, Accuracy: 64.861%\n",
            "Epoch 002: Loss: 1.866, Accuracy: 64.844%\n",
            "Epoch 002: Loss: 1.863, Accuracy: 64.941%\n",
            "Epoch 002: Loss: 1.859, Accuracy: 65.082%\n",
            "Epoch 002: Loss: 1.859, Accuracy: 65.063%\n",
            "Epoch 002: Loss: 1.857, Accuracy: 65.067%\n",
            "Epoch 002: Loss: 1.857, Accuracy: 65.004%\n",
            "Epoch 002: Loss: 1.854, Accuracy: 65.075%\n",
            "Epoch 002: Loss: 1.850, Accuracy: 65.166%\n",
            "Epoch 002: Loss: 1.848, Accuracy: 65.191%\n",
            "Epoch 002: Loss: 1.848, Accuracy: 65.172%\n",
            "Epoch 002: Loss: 1.848, Accuracy: 65.197%\n",
            "Epoch 002: Loss: 1.849, Accuracy: 65.200%\n",
            "Epoch 002: Loss: 1.851, Accuracy: 65.139%\n",
            "Epoch 002: Loss: 1.852, Accuracy: 65.122%\n",
            "Epoch 002: Loss: 1.849, Accuracy: 65.250%\n",
            "Epoch 002: Loss: 1.848, Accuracy: 65.252%\n",
            "Epoch 002: Loss: 1.848, Accuracy: 65.255%\n",
            "Epoch 002: Loss: 1.847, Accuracy: 65.278%\n",
            "Epoch 002: Loss: 1.844, Accuracy: 65.361%\n",
            "Epoch 002: Loss: 1.843, Accuracy: 65.403%\n",
            "Epoch 002: Loss: 1.844, Accuracy: 65.445%\n",
            "Epoch 002: Loss: 1.843, Accuracy: 65.446%\n",
            "Epoch 002: Loss: 1.845, Accuracy: 65.368%\n",
            "Epoch 002: Loss: 1.846, Accuracy: 65.350%\n",
            "Epoch 002: Loss: 1.847, Accuracy: 65.273%\n",
            "Epoch 002: Loss: 1.849, Accuracy: 65.276%\n",
            "Epoch 002: Loss: 1.847, Accuracy: 65.297%\n",
            "Epoch 002: Loss: 1.844, Accuracy: 65.376%\n",
            "Epoch 002: Loss: 1.845, Accuracy: 65.320%\n",
            "Epoch 002: Loss: 1.845, Accuracy: 65.341%\n",
            "Epoch 002: Loss: 1.846, Accuracy: 65.343%\n",
            "Epoch 002: Loss: 1.848, Accuracy: 65.288%\n",
            "Epoch 002: Loss: 1.851, Accuracy: 65.197%\n",
            "Epoch 002: Loss: 1.852, Accuracy: 65.218%\n",
            "Epoch 002: Loss: 1.851, Accuracy: 65.239%\n",
            "Epoch 002: Loss: 1.852, Accuracy: 65.296%\n",
            "Epoch 002: Loss: 1.847, Accuracy: 65.407%\n",
            "Epoch 002: Loss: 1.843, Accuracy: 65.462%\n",
            "Epoch 002: Loss: 1.841, Accuracy: 65.517%\n",
            "Epoch 002: Loss: 1.837, Accuracy: 65.589%\n",
            "Epoch 002: Loss: 1.837, Accuracy: 65.589%\n",
            "Epoch 002: Loss: 1.836, Accuracy: 65.572%\n",
            "Epoch 002: Loss: 1.835, Accuracy: 65.607%\n",
            "Epoch 002: Loss: 1.836, Accuracy: 65.573%\n",
            "Epoch 002: Loss: 1.836, Accuracy: 65.590%\n",
            "Epoch 002: Loss: 1.838, Accuracy: 65.539%\n",
            "Epoch 002: Loss: 1.834, Accuracy: 65.608%\n",
            "Epoch 002: Loss: 1.832, Accuracy: 65.659%\n",
            "Epoch 002: Loss: 1.833, Accuracy: 65.642%\n",
            "Epoch 002: Loss: 1.835, Accuracy: 65.608%\n",
            "Epoch 002: Loss: 1.838, Accuracy: 65.575%\n",
            "Epoch 002: Loss: 1.838, Accuracy: 65.592%\n",
            "Epoch 002: Loss: 1.834, Accuracy: 65.675%\n",
            "Epoch 002: Loss: 1.834, Accuracy: 65.658%\n",
            "Epoch 002: Loss: 1.833, Accuracy: 65.691%\n",
            "Epoch 002: Loss: 1.832, Accuracy: 65.707%\n",
            "Epoch 002: Loss: 1.833, Accuracy: 65.706%\n",
            "Epoch 002: Loss: 1.834, Accuracy: 65.690%\n",
            "Epoch 002: Loss: 1.834, Accuracy: 65.706%\n",
            "Epoch 002: Loss: 1.837, Accuracy: 65.657%\n",
            "Epoch 002: Loss: 1.835, Accuracy: 65.689%\n",
            "Epoch 002: Loss: 1.834, Accuracy: 65.736%\n",
            "Epoch 002: Loss: 1.834, Accuracy: 65.720%\n",
            "Epoch 002: Loss: 1.832, Accuracy: 65.751%\n",
            "Epoch 002: Loss: 1.832, Accuracy: 65.797%\n",
            "Epoch 002: Loss: 1.831, Accuracy: 65.796%\n",
            "Epoch 002: Loss: 1.833, Accuracy: 65.702%\n",
            "Epoch 002: Loss: 1.832, Accuracy: 65.717%\n",
            "Epoch 002: Loss: 1.831, Accuracy: 65.717%\n",
            "Epoch 002: Loss: 1.832, Accuracy: 65.671%\n",
            "Epoch 002: Loss: 1.832, Accuracy: 65.671%\n",
            "Epoch 002: Loss: 1.830, Accuracy: 65.700%\n",
            "Epoch 002: Loss: 1.830, Accuracy: 65.685%\n",
            "Epoch 002: Loss: 1.829, Accuracy: 65.730%\n",
            "Epoch 002: Loss: 1.828, Accuracy: 65.759%\n",
            "Epoch 002: Loss: 1.829, Accuracy: 65.729%\n",
            "Epoch 002: Loss: 1.828, Accuracy: 65.743%\n",
            "Epoch 002: Loss: 1.828, Accuracy: 65.728%\n",
            "Epoch 002: Loss: 1.831, Accuracy: 65.698%\n",
            "Epoch 002: Loss: 1.832, Accuracy: 65.712%\n",
            "Epoch 002: Loss: 1.831, Accuracy: 65.726%\n",
            "Epoch 002: Loss: 1.830, Accuracy: 65.740%\n",
            "Epoch 002: Loss: 1.830, Accuracy: 65.754%\n",
            "Epoch 002: Loss: 1.830, Accuracy: 65.753%\n",
            "Epoch 002: Loss: 1.829, Accuracy: 65.810%\n",
            "Epoch 002: Loss: 1.828, Accuracy: 65.823%\n",
            "Epoch 002: Loss: 1.828, Accuracy: 65.808%\n",
            "Epoch 002: Loss: 1.827, Accuracy: 65.835%\n",
            "Epoch 002: Loss: 1.826, Accuracy: 65.848%\n",
            "Epoch 002: Loss: 1.823, Accuracy: 65.903%\n",
            "Epoch 002: Loss: 1.822, Accuracy: 65.915%\n",
            "Epoch 002: Loss: 1.821, Accuracy: 65.928%\n",
            "Epoch 002: Loss: 1.820, Accuracy: 65.995%\n",
            "Epoch 002: Loss: 1.821, Accuracy: 65.993%\n",
            "Epoch 002: Loss: 1.821, Accuracy: 65.992%\n",
            "Epoch 002: Loss: 1.821, Accuracy: 65.990%\n",
            "Epoch 002: Loss: 1.819, Accuracy: 66.070%\n",
            "Epoch 002: Loss: 1.818, Accuracy: 66.094%\n",
            "Epoch 002: Loss: 1.818, Accuracy: 66.092%\n",
            "Epoch 002: Loss: 1.819, Accuracy: 66.051%\n",
            "Epoch 002: Loss: 1.818, Accuracy: 66.102%\n",
            "Epoch 002: Loss: 1.817, Accuracy: 66.139%\n",
            "Epoch 002: Loss: 1.819, Accuracy: 66.124%\n",
            "Epoch 002: Loss: 1.820, Accuracy: 66.083%\n",
            "Epoch 002: Loss: 1.820, Accuracy: 66.081%\n",
            "Epoch 002: Loss: 1.821, Accuracy: 66.079%\n",
            "Epoch 002: Loss: 1.820, Accuracy: 66.090%\n",
            "Epoch 002: Loss: 1.821, Accuracy: 66.088%\n",
            "Epoch 002: Loss: 1.821, Accuracy: 66.086%\n",
            "Epoch 002: Loss: 1.822, Accuracy: 66.084%\n",
            "Epoch 002: Loss: 1.821, Accuracy: 66.108%\n",
            "Epoch 002: Loss: 1.819, Accuracy: 66.144%\n",
            "Epoch 002: Loss: 1.817, Accuracy: 66.179%\n",
            "Epoch 002: Loss: 1.818, Accuracy: 66.152%\n",
            "Epoch 002: Loss: 1.817, Accuracy: 66.188%\n",
            "Epoch 002: Loss: 1.816, Accuracy: 66.198%\n",
            "Epoch 002: Loss: 1.815, Accuracy: 66.208%\n",
            "Epoch 002: Loss: 1.816, Accuracy: 66.156%\n",
            "Epoch 002: Loss: 1.817, Accuracy: 66.142%\n",
            "Epoch 002: Loss: 1.817, Accuracy: 66.127%\n",
            "Epoch 002: Loss: 1.817, Accuracy: 66.138%\n",
            "Epoch 002: Loss: 1.816, Accuracy: 66.136%\n",
            "Epoch 002: Loss: 1.816, Accuracy: 66.158%\n",
            "Epoch 002: Loss: 1.817, Accuracy: 66.144%\n",
            "Epoch 002: Loss: 1.816, Accuracy: 66.154%\n",
            "Epoch 002: Loss: 1.816, Accuracy: 66.152%\n",
            "Epoch 002: Loss: 1.816, Accuracy: 66.150%\n",
            "Epoch 002: Loss: 1.817, Accuracy: 66.136%\n",
            "Epoch 002: Loss: 1.817, Accuracy: 66.158%\n",
            "Epoch 002: Loss: 1.817, Accuracy: 66.156%\n",
            "Epoch 002: Loss: 1.818, Accuracy: 66.177%\n",
            "Epoch 002: Loss: 1.818, Accuracy: 66.163%\n",
            "Epoch 002: Loss: 1.817, Accuracy: 66.196%\n",
            "Epoch 002: Loss: 1.818, Accuracy: 66.171%\n",
            "Epoch 002: Loss: 1.821, Accuracy: 66.100%\n",
            "Epoch 002: Loss: 1.819, Accuracy: 66.155%\n",
            "Epoch 002: Loss: 1.818, Accuracy: 66.188%\n",
            "Epoch 002: Loss: 1.818, Accuracy: 66.209%\n",
            "Epoch 002: Loss: 1.818, Accuracy: 66.252%\n",
            "Epoch 002: Loss: 1.817, Accuracy: 66.284%\n",
            "Epoch 002: Loss: 1.815, Accuracy: 66.338%\n",
            "Epoch 002: Loss: 1.816, Accuracy: 66.302%\n",
            "Epoch 002: Loss: 1.816, Accuracy: 66.311%\n",
            "Epoch 002: Loss: 1.816, Accuracy: 66.342%\n",
            "Epoch 002: Loss: 1.816, Accuracy: 66.339%\n",
            "Epoch 002: Loss: 1.816, Accuracy: 66.344%\n",
            "Epoch 003: Loss: 1.083, Accuracy: 81.250%\n",
            "Epoch 003: Loss: 1.128, Accuracy: 79.688%\n",
            "Epoch 003: Loss: 1.371, Accuracy: 71.875%\n",
            "Epoch 003: Loss: 1.450, Accuracy: 72.656%\n",
            "Epoch 003: Loss: 1.438, Accuracy: 73.750%\n",
            "Epoch 003: Loss: 1.451, Accuracy: 73.958%\n",
            "Epoch 003: Loss: 1.578, Accuracy: 72.768%\n",
            "Epoch 003: Loss: 1.536, Accuracy: 73.438%\n",
            "Epoch 003: Loss: 1.658, Accuracy: 70.139%\n",
            "Epoch 003: Loss: 1.644, Accuracy: 70.625%\n",
            "Epoch 003: Loss: 1.616, Accuracy: 71.307%\n",
            "Epoch 003: Loss: 1.618, Accuracy: 70.833%\n",
            "Epoch 003: Loss: 1.636, Accuracy: 70.433%\n",
            "Epoch 003: Loss: 1.643, Accuracy: 70.312%\n",
            "Epoch 003: Loss: 1.653, Accuracy: 70.208%\n",
            "Epoch 003: Loss: 1.668, Accuracy: 69.141%\n",
            "Epoch 003: Loss: 1.687, Accuracy: 68.750%\n",
            "Epoch 003: Loss: 1.691, Accuracy: 68.924%\n",
            "Epoch 003: Loss: 1.695, Accuracy: 68.914%\n",
            "Epoch 003: Loss: 1.689, Accuracy: 69.063%\n",
            "Epoch 003: Loss: 1.700, Accuracy: 68.899%\n",
            "Epoch 003: Loss: 1.692, Accuracy: 69.176%\n",
            "Epoch 003: Loss: 1.729, Accuracy: 68.342%\n",
            "Epoch 003: Loss: 1.733, Accuracy: 68.229%\n",
            "Epoch 003: Loss: 1.718, Accuracy: 68.500%\n",
            "Epoch 003: Loss: 1.731, Accuracy: 68.269%\n",
            "Epoch 003: Loss: 1.754, Accuracy: 67.708%\n",
            "Epoch 003: Loss: 1.763, Accuracy: 67.076%\n",
            "Epoch 003: Loss: 1.761, Accuracy: 67.349%\n",
            "Epoch 003: Loss: 1.759, Accuracy: 67.396%\n",
            "Epoch 003: Loss: 1.741, Accuracy: 67.944%\n",
            "Epoch 003: Loss: 1.741, Accuracy: 67.969%\n",
            "Epoch 003: Loss: 1.759, Accuracy: 67.708%\n",
            "Epoch 003: Loss: 1.773, Accuracy: 67.371%\n",
            "Epoch 003: Loss: 1.788, Accuracy: 67.143%\n",
            "Epoch 003: Loss: 1.782, Accuracy: 67.014%\n",
            "Epoch 003: Loss: 1.781, Accuracy: 67.061%\n",
            "Epoch 003: Loss: 1.779, Accuracy: 67.188%\n",
            "Epoch 003: Loss: 1.784, Accuracy: 67.388%\n",
            "Epoch 003: Loss: 1.774, Accuracy: 67.656%\n",
            "Epoch 003: Loss: 1.777, Accuracy: 67.683%\n",
            "Epoch 003: Loss: 1.780, Accuracy: 67.708%\n",
            "Epoch 003: Loss: 1.775, Accuracy: 67.951%\n",
            "Epoch 003: Loss: 1.790, Accuracy: 67.401%\n",
            "Epoch 003: Loss: 1.785, Accuracy: 67.500%\n",
            "Epoch 003: Loss: 1.792, Accuracy: 67.255%\n",
            "Epoch 003: Loss: 1.785, Accuracy: 67.487%\n",
            "Epoch 003: Loss: 1.786, Accuracy: 67.448%\n",
            "Epoch 003: Loss: 1.781, Accuracy: 67.474%\n",
            "Epoch 003: Loss: 1.780, Accuracy: 67.437%\n",
            "Epoch 003: Loss: 1.782, Accuracy: 67.341%\n",
            "Epoch 003: Loss: 1.783, Accuracy: 67.668%\n",
            "Epoch 003: Loss: 1.787, Accuracy: 67.630%\n",
            "Epoch 003: Loss: 1.789, Accuracy: 67.650%\n",
            "Epoch 003: Loss: 1.785, Accuracy: 67.614%\n",
            "Epoch 003: Loss: 1.787, Accuracy: 67.634%\n",
            "Epoch 003: Loss: 1.785, Accuracy: 67.654%\n",
            "Epoch 003: Loss: 1.789, Accuracy: 67.619%\n",
            "Epoch 003: Loss: 1.801, Accuracy: 67.373%\n",
            "Epoch 003: Loss: 1.800, Accuracy: 67.552%\n",
            "Epoch 003: Loss: 1.793, Accuracy: 67.623%\n",
            "Epoch 003: Loss: 1.789, Accuracy: 67.641%\n",
            "Epoch 003: Loss: 1.794, Accuracy: 67.460%\n",
            "Epoch 003: Loss: 1.798, Accuracy: 67.432%\n",
            "Epoch 003: Loss: 1.807, Accuracy: 67.260%\n",
            "Epoch 003: Loss: 1.805, Accuracy: 67.377%\n",
            "Epoch 003: Loss: 1.799, Accuracy: 67.584%\n",
            "Epoch 003: Loss: 1.798, Accuracy: 67.647%\n",
            "Epoch 003: Loss: 1.801, Accuracy: 67.618%\n",
            "Epoch 003: Loss: 1.802, Accuracy: 67.723%\n",
            "Epoch 003: Loss: 1.808, Accuracy: 67.430%\n",
            "Epoch 003: Loss: 1.807, Accuracy: 67.578%\n",
            "Epoch 003: Loss: 1.806, Accuracy: 67.594%\n",
            "Epoch 003: Loss: 1.811, Accuracy: 67.525%\n",
            "Epoch 003: Loss: 1.803, Accuracy: 67.750%\n",
            "Epoch 003: Loss: 1.802, Accuracy: 67.681%\n",
            "Epoch 003: Loss: 1.803, Accuracy: 67.695%\n",
            "Epoch 003: Loss: 1.803, Accuracy: 67.588%\n",
            "Epoch 003: Loss: 1.804, Accuracy: 67.524%\n",
            "Epoch 003: Loss: 1.807, Accuracy: 67.461%\n",
            "Epoch 003: Loss: 1.807, Accuracy: 67.515%\n",
            "Epoch 003: Loss: 1.802, Accuracy: 67.530%\n",
            "Epoch 003: Loss: 1.798, Accuracy: 67.658%\n",
            "Epoch 003: Loss: 1.797, Accuracy: 67.634%\n",
            "Epoch 003: Loss: 1.804, Accuracy: 67.574%\n",
            "Epoch 003: Loss: 1.800, Accuracy: 67.624%\n",
            "Epoch 003: Loss: 1.795, Accuracy: 67.672%\n",
            "Epoch 003: Loss: 1.792, Accuracy: 67.791%\n",
            "Epoch 003: Loss: 1.793, Accuracy: 67.697%\n",
            "Epoch 003: Loss: 1.793, Accuracy: 67.639%\n",
            "Epoch 003: Loss: 1.795, Accuracy: 67.720%\n",
            "Epoch 003: Loss: 1.797, Accuracy: 67.663%\n",
            "Epoch 003: Loss: 1.793, Accuracy: 67.776%\n",
            "Epoch 003: Loss: 1.802, Accuracy: 67.686%\n",
            "Epoch 003: Loss: 1.801, Accuracy: 67.763%\n",
            "Epoch 003: Loss: 1.804, Accuracy: 67.643%\n",
            "Epoch 003: Loss: 1.807, Accuracy: 67.558%\n",
            "Epoch 003: Loss: 1.808, Accuracy: 67.570%\n",
            "Epoch 003: Loss: 1.803, Accuracy: 67.645%\n",
            "Epoch 003: Loss: 1.806, Accuracy: 67.531%\n",
            "Epoch 003: Loss: 1.805, Accuracy: 67.574%\n",
            "Epoch 003: Loss: 1.806, Accuracy: 67.586%\n",
            "Epoch 003: Loss: 1.808, Accuracy: 67.536%\n",
            "Epoch 003: Loss: 1.806, Accuracy: 67.608%\n",
            "Epoch 003: Loss: 1.807, Accuracy: 67.589%\n",
            "Epoch 003: Loss: 1.808, Accuracy: 67.541%\n",
            "Epoch 003: Loss: 1.807, Accuracy: 67.582%\n",
            "Epoch 003: Loss: 1.808, Accuracy: 67.506%\n",
            "Epoch 003: Loss: 1.807, Accuracy: 67.489%\n",
            "Epoch 003: Loss: 1.804, Accuracy: 67.472%\n",
            "Epoch 003: Loss: 1.799, Accuracy: 67.624%\n",
            "Epoch 003: Loss: 1.797, Accuracy: 67.718%\n",
            "Epoch 003: Loss: 1.798, Accuracy: 67.671%\n",
            "Epoch 003: Loss: 1.802, Accuracy: 67.654%\n",
            "Epoch 003: Loss: 1.804, Accuracy: 67.609%\n",
            "Epoch 003: Loss: 1.804, Accuracy: 67.645%\n",
            "Epoch 003: Loss: 1.806, Accuracy: 67.575%\n",
            "Epoch 003: Loss: 1.804, Accuracy: 67.611%\n",
            "Epoch 003: Loss: 1.810, Accuracy: 67.489%\n",
            "Epoch 003: Loss: 1.808, Accuracy: 67.552%\n",
            "Epoch 003: Loss: 1.805, Accuracy: 67.588%\n",
            "Epoch 003: Loss: 1.805, Accuracy: 67.572%\n",
            "Epoch 003: Loss: 1.807, Accuracy: 67.454%\n",
            "Epoch 003: Loss: 1.806, Accuracy: 67.515%\n",
            "Epoch 003: Loss: 1.813, Accuracy: 67.400%\n",
            "Epoch 003: Loss: 1.812, Accuracy: 67.386%\n",
            "Epoch 003: Loss: 1.813, Accuracy: 67.298%\n",
            "Epoch 003: Loss: 1.815, Accuracy: 67.236%\n",
            "Epoch 003: Loss: 1.812, Accuracy: 67.345%\n",
            "Epoch 003: Loss: 1.808, Accuracy: 67.452%\n",
            "Epoch 003: Loss: 1.811, Accuracy: 67.390%\n",
            "Epoch 003: Loss: 1.808, Accuracy: 67.472%\n",
            "Epoch 003: Loss: 1.806, Accuracy: 67.528%\n",
            "Epoch 003: Loss: 1.806, Accuracy: 67.491%\n",
            "Epoch 003: Loss: 1.809, Accuracy: 67.431%\n",
            "Epoch 003: Loss: 1.808, Accuracy: 67.440%\n",
            "Epoch 003: Loss: 1.806, Accuracy: 67.450%\n",
            "Epoch 003: Loss: 1.803, Accuracy: 67.572%\n",
            "Epoch 003: Loss: 1.802, Accuracy: 67.558%\n",
            "Epoch 003: Loss: 1.801, Accuracy: 67.545%\n",
            "Epoch 003: Loss: 1.802, Accuracy: 67.509%\n",
            "Epoch 003: Loss: 1.798, Accuracy: 67.650%\n",
            "Epoch 003: Loss: 1.796, Accuracy: 67.723%\n",
            "Epoch 003: Loss: 1.795, Accuracy: 67.730%\n",
            "Epoch 003: Loss: 1.797, Accuracy: 67.716%\n",
            "Epoch 003: Loss: 1.796, Accuracy: 67.765%\n",
            "Epoch 003: Loss: 1.797, Accuracy: 67.772%\n",
            "Epoch 003: Loss: 1.798, Accuracy: 67.715%\n",
            "Epoch 003: Loss: 1.799, Accuracy: 67.680%\n",
            "Epoch 003: Loss: 1.796, Accuracy: 67.792%\n",
            "Epoch 003: Loss: 1.794, Accuracy: 67.839%\n",
            "Epoch 003: Loss: 1.794, Accuracy: 67.866%\n",
            "Epoch 003: Loss: 1.794, Accuracy: 67.872%\n",
            "Epoch 003: Loss: 1.793, Accuracy: 67.938%\n",
            "Epoch 003: Loss: 1.790, Accuracy: 67.944%\n",
            "Epoch 003: Loss: 1.791, Accuracy: 67.949%\n",
            "Epoch 003: Loss: 1.788, Accuracy: 67.974%\n",
            "Epoch 003: Loss: 1.791, Accuracy: 67.900%\n",
            "Epoch 003: Loss: 1.793, Accuracy: 67.885%\n",
            "Epoch 003: Loss: 1.794, Accuracy: 67.773%\n",
            "Epoch 003: Loss: 1.795, Accuracy: 67.760%\n",
            "Epoch 003: Loss: 1.794, Accuracy: 67.785%\n",
            "Epoch 003: Loss: 1.791, Accuracy: 67.849%\n",
            "Epoch 003: Loss: 1.792, Accuracy: 67.797%\n",
            "Epoch 003: Loss: 1.792, Accuracy: 67.860%\n",
            "Epoch 003: Loss: 1.792, Accuracy: 67.884%\n",
            "Epoch 003: Loss: 1.795, Accuracy: 67.833%\n",
            "Epoch 003: Loss: 1.798, Accuracy: 67.764%\n",
            "Epoch 003: Loss: 1.798, Accuracy: 67.770%\n",
            "Epoch 003: Loss: 1.798, Accuracy: 67.776%\n",
            "Epoch 003: Loss: 1.798, Accuracy: 67.800%\n",
            "Epoch 003: Loss: 1.795, Accuracy: 67.896%\n",
            "Epoch 003: Loss: 1.791, Accuracy: 68.027%\n",
            "Epoch 003: Loss: 1.789, Accuracy: 68.103%\n",
            "Epoch 003: Loss: 1.785, Accuracy: 68.179%\n",
            "Epoch 003: Loss: 1.785, Accuracy: 68.200%\n",
            "Epoch 003: Loss: 1.784, Accuracy: 68.220%\n",
            "Epoch 003: Loss: 1.784, Accuracy: 68.206%\n",
            "Epoch 003: Loss: 1.785, Accuracy: 68.191%\n",
            "Epoch 003: Loss: 1.785, Accuracy: 68.229%\n",
            "Epoch 003: Loss: 1.787, Accuracy: 68.215%\n",
            "Epoch 003: Loss: 1.783, Accuracy: 68.321%\n",
            "Epoch 003: Loss: 1.782, Accuracy: 68.374%\n",
            "Epoch 003: Loss: 1.783, Accuracy: 68.342%\n",
            "Epoch 003: Loss: 1.783, Accuracy: 68.311%\n",
            "Epoch 003: Loss: 1.785, Accuracy: 68.246%\n",
            "Epoch 003: Loss: 1.787, Accuracy: 68.215%\n",
            "Epoch 003: Loss: 1.783, Accuracy: 68.318%\n",
            "Epoch 003: Loss: 1.783, Accuracy: 68.337%\n",
            "Epoch 003: Loss: 1.782, Accuracy: 68.355%\n",
            "Epoch 003: Loss: 1.781, Accuracy: 68.357%\n",
            "Epoch 003: Loss: 1.782, Accuracy: 68.343%\n",
            "Epoch 003: Loss: 1.782, Accuracy: 68.297%\n",
            "Epoch 003: Loss: 1.784, Accuracy: 68.299%\n",
            "Epoch 003: Loss: 1.786, Accuracy: 68.221%\n",
            "Epoch 003: Loss: 1.786, Accuracy: 68.224%\n",
            "Epoch 003: Loss: 1.786, Accuracy: 68.242%\n",
            "Epoch 003: Loss: 1.784, Accuracy: 68.292%\n",
            "Epoch 003: Loss: 1.783, Accuracy: 68.310%\n",
            "Epoch 003: Loss: 1.782, Accuracy: 68.344%\n",
            "Epoch 003: Loss: 1.780, Accuracy: 68.377%\n",
            "Epoch 003: Loss: 1.782, Accuracy: 68.301%\n",
            "Epoch 003: Loss: 1.782, Accuracy: 68.288%\n",
            "Epoch 003: Loss: 1.780, Accuracy: 68.275%\n",
            "Epoch 003: Loss: 1.782, Accuracy: 68.201%\n",
            "Epoch 003: Loss: 1.783, Accuracy: 68.174%\n",
            "Epoch 003: Loss: 1.780, Accuracy: 68.222%\n",
            "Epoch 003: Loss: 1.780, Accuracy: 68.224%\n",
            "Epoch 003: Loss: 1.779, Accuracy: 68.227%\n",
            "Epoch 003: Loss: 1.779, Accuracy: 68.259%\n",
            "Epoch 003: Loss: 1.778, Accuracy: 68.276%\n",
            "Epoch 003: Loss: 1.777, Accuracy: 68.278%\n",
            "Epoch 003: Loss: 1.778, Accuracy: 68.266%\n",
            "Epoch 003: Loss: 1.781, Accuracy: 68.224%\n",
            "Epoch 003: Loss: 1.783, Accuracy: 68.198%\n",
            "Epoch 003: Loss: 1.783, Accuracy: 68.186%\n",
            "Epoch 003: Loss: 1.782, Accuracy: 68.174%\n",
            "Epoch 003: Loss: 1.782, Accuracy: 68.162%\n",
            "Epoch 003: Loss: 1.783, Accuracy: 68.165%\n",
            "Epoch 003: Loss: 1.780, Accuracy: 68.224%\n",
            "Epoch 003: Loss: 1.779, Accuracy: 68.227%\n",
            "Epoch 003: Loss: 1.779, Accuracy: 68.215%\n",
            "Epoch 003: Loss: 1.779, Accuracy: 68.232%\n",
            "Epoch 003: Loss: 1.779, Accuracy: 68.206%\n",
            "Epoch 003: Loss: 1.776, Accuracy: 68.264%\n",
            "Epoch 003: Loss: 1.776, Accuracy: 68.225%\n",
            "Epoch 003: Loss: 1.775, Accuracy: 68.227%\n",
            "Epoch 003: Loss: 1.774, Accuracy: 68.257%\n",
            "Epoch 003: Loss: 1.774, Accuracy: 68.231%\n",
            "Epoch 003: Loss: 1.775, Accuracy: 68.220%\n",
            "Epoch 003: Loss: 1.775, Accuracy: 68.209%\n",
            "Epoch 003: Loss: 1.773, Accuracy: 68.252%\n",
            "Epoch 003: Loss: 1.772, Accuracy: 68.254%\n",
            "Epoch 003: Loss: 1.771, Accuracy: 68.269%\n",
            "Epoch 003: Loss: 1.772, Accuracy: 68.218%\n",
            "Epoch 003: Loss: 1.771, Accuracy: 68.247%\n",
            "Epoch 003: Loss: 1.770, Accuracy: 68.275%\n",
            "Epoch 003: Loss: 1.771, Accuracy: 68.264%\n",
            "Epoch 003: Loss: 1.772, Accuracy: 68.240%\n",
            "Epoch 003: Loss: 1.772, Accuracy: 68.242%\n",
            "Epoch 003: Loss: 1.772, Accuracy: 68.179%\n",
            "Epoch 003: Loss: 1.772, Accuracy: 68.182%\n",
            "Epoch 003: Loss: 1.772, Accuracy: 68.197%\n",
            "Epoch 003: Loss: 1.773, Accuracy: 68.174%\n",
            "Epoch 003: Loss: 1.774, Accuracy: 68.163%\n",
            "Epoch 003: Loss: 1.772, Accuracy: 68.178%\n",
            "Epoch 003: Loss: 1.771, Accuracy: 68.206%\n",
            "Epoch 003: Loss: 1.769, Accuracy: 68.208%\n",
            "Epoch 003: Loss: 1.770, Accuracy: 68.173%\n",
            "Epoch 003: Loss: 1.769, Accuracy: 68.187%\n",
            "Epoch 003: Loss: 1.768, Accuracy: 68.227%\n",
            "Epoch 003: Loss: 1.767, Accuracy: 68.279%\n",
            "Epoch 003: Loss: 1.769, Accuracy: 68.256%\n",
            "Epoch 003: Loss: 1.770, Accuracy: 68.233%\n",
            "Epoch 003: Loss: 1.770, Accuracy: 68.248%\n",
            "Epoch 003: Loss: 1.770, Accuracy: 68.250%\n",
            "Epoch 003: Loss: 1.770, Accuracy: 68.276%\n",
            "Epoch 003: Loss: 1.770, Accuracy: 68.302%\n",
            "Epoch 003: Loss: 1.770, Accuracy: 68.292%\n",
            "Epoch 003: Loss: 1.769, Accuracy: 68.293%\n",
            "Epoch 003: Loss: 1.769, Accuracy: 68.319%\n",
            "Epoch 003: Loss: 1.768, Accuracy: 68.297%\n",
            "Epoch 003: Loss: 1.769, Accuracy: 68.275%\n",
            "Epoch 003: Loss: 1.769, Accuracy: 68.265%\n",
            "Epoch 003: Loss: 1.769, Accuracy: 68.267%\n",
            "Epoch 003: Loss: 1.770, Accuracy: 68.292%\n",
            "Epoch 003: Loss: 1.770, Accuracy: 68.270%\n",
            "Epoch 003: Loss: 1.769, Accuracy: 68.295%\n",
            "Epoch 003: Loss: 1.770, Accuracy: 68.285%\n",
            "Epoch 003: Loss: 1.772, Accuracy: 68.218%\n",
            "Epoch 003: Loss: 1.771, Accuracy: 68.266%\n",
            "Epoch 003: Loss: 1.771, Accuracy: 68.279%\n",
            "Epoch 003: Loss: 1.771, Accuracy: 68.292%\n",
            "Epoch 003: Loss: 1.770, Accuracy: 68.294%\n",
            "Epoch 003: Loss: 1.770, Accuracy: 68.295%\n",
            "Epoch 003: Loss: 1.768, Accuracy: 68.354%\n",
            "Epoch 003: Loss: 1.769, Accuracy: 68.299%\n",
            "Epoch 003: Loss: 1.769, Accuracy: 68.289%\n",
            "Epoch 003: Loss: 1.769, Accuracy: 68.324%\n",
            "Epoch 003: Loss: 1.770, Accuracy: 68.326%\n",
            "Epoch 003: Loss: 1.770, Accuracy: 68.337%\n",
            "Epoch 004: Loss: 1.200, Accuracy: 78.125%\n",
            "Epoch 004: Loss: 1.251, Accuracy: 78.125%\n",
            "Epoch 004: Loss: 1.443, Accuracy: 72.917%\n",
            "Epoch 004: Loss: 1.468, Accuracy: 75.781%\n",
            "Epoch 004: Loss: 1.464, Accuracy: 77.500%\n",
            "Epoch 004: Loss: 1.452, Accuracy: 77.083%\n",
            "Epoch 004: Loss: 1.574, Accuracy: 74.554%\n",
            "Epoch 004: Loss: 1.512, Accuracy: 76.562%\n",
            "Epoch 004: Loss: 1.638, Accuracy: 72.917%\n",
            "Epoch 004: Loss: 1.621, Accuracy: 72.812%\n",
            "Epoch 004: Loss: 1.593, Accuracy: 72.443%\n",
            "Epoch 004: Loss: 1.603, Accuracy: 72.656%\n",
            "Epoch 004: Loss: 1.608, Accuracy: 72.356%\n",
            "Epoch 004: Loss: 1.625, Accuracy: 72.098%\n",
            "Epoch 004: Loss: 1.636, Accuracy: 72.083%\n",
            "Epoch 004: Loss: 1.669, Accuracy: 71.289%\n",
            "Epoch 004: Loss: 1.687, Accuracy: 70.588%\n",
            "Epoch 004: Loss: 1.686, Accuracy: 70.660%\n",
            "Epoch 004: Loss: 1.683, Accuracy: 70.395%\n",
            "Epoch 004: Loss: 1.683, Accuracy: 70.781%\n",
            "Epoch 004: Loss: 1.684, Accuracy: 70.536%\n",
            "Epoch 004: Loss: 1.674, Accuracy: 70.881%\n",
            "Epoch 004: Loss: 1.692, Accuracy: 70.245%\n",
            "Epoch 004: Loss: 1.690, Accuracy: 70.052%\n",
            "Epoch 004: Loss: 1.678, Accuracy: 70.500%\n",
            "Epoch 004: Loss: 1.685, Accuracy: 70.192%\n",
            "Epoch 004: Loss: 1.713, Accuracy: 69.560%\n",
            "Epoch 004: Loss: 1.721, Accuracy: 69.196%\n",
            "Epoch 004: Loss: 1.713, Accuracy: 69.181%\n",
            "Epoch 004: Loss: 1.711, Accuracy: 69.063%\n",
            "Epoch 004: Loss: 1.698, Accuracy: 69.456%\n",
            "Epoch 004: Loss: 1.705, Accuracy: 69.434%\n",
            "Epoch 004: Loss: 1.717, Accuracy: 69.223%\n",
            "Epoch 004: Loss: 1.729, Accuracy: 68.934%\n",
            "Epoch 004: Loss: 1.751, Accuracy: 68.571%\n",
            "Epoch 004: Loss: 1.748, Accuracy: 68.403%\n",
            "Epoch 004: Loss: 1.746, Accuracy: 68.666%\n",
            "Epoch 004: Loss: 1.747, Accuracy: 68.750%\n",
            "Epoch 004: Loss: 1.747, Accuracy: 68.750%\n",
            "Epoch 004: Loss: 1.732, Accuracy: 69.219%\n",
            "Epoch 004: Loss: 1.734, Accuracy: 69.131%\n",
            "Epoch 004: Loss: 1.736, Accuracy: 69.048%\n",
            "Epoch 004: Loss: 1.729, Accuracy: 69.331%\n",
            "Epoch 004: Loss: 1.745, Accuracy: 68.892%\n",
            "Epoch 004: Loss: 1.744, Accuracy: 68.958%\n",
            "Epoch 004: Loss: 1.753, Accuracy: 68.546%\n",
            "Epoch 004: Loss: 1.747, Accuracy: 68.816%\n",
            "Epoch 004: Loss: 1.748, Accuracy: 68.750%\n",
            "Epoch 004: Loss: 1.743, Accuracy: 68.750%\n",
            "Epoch 004: Loss: 1.743, Accuracy: 68.687%\n",
            "Epoch 004: Loss: 1.746, Accuracy: 68.750%\n",
            "Epoch 004: Loss: 1.748, Accuracy: 68.870%\n",
            "Epoch 004: Loss: 1.755, Accuracy: 68.750%\n",
            "Epoch 004: Loss: 1.759, Accuracy: 68.634%\n",
            "Epoch 004: Loss: 1.755, Accuracy: 68.580%\n",
            "Epoch 004: Loss: 1.754, Accuracy: 68.583%\n",
            "Epoch 004: Loss: 1.752, Accuracy: 68.586%\n",
            "Epoch 004: Loss: 1.756, Accuracy: 68.481%\n",
            "Epoch 004: Loss: 1.769, Accuracy: 68.114%\n",
            "Epoch 004: Loss: 1.762, Accuracy: 68.385%\n",
            "Epoch 004: Loss: 1.757, Accuracy: 68.494%\n",
            "Epoch 004: Loss: 1.757, Accuracy: 68.498%\n",
            "Epoch 004: Loss: 1.764, Accuracy: 68.353%\n",
            "Epoch 004: Loss: 1.768, Accuracy: 68.359%\n",
            "Epoch 004: Loss: 1.776, Accuracy: 68.173%\n",
            "Epoch 004: Loss: 1.774, Accuracy: 68.277%\n",
            "Epoch 004: Loss: 1.768, Accuracy: 68.377%\n",
            "Epoch 004: Loss: 1.766, Accuracy: 68.336%\n",
            "Epoch 004: Loss: 1.767, Accuracy: 68.297%\n",
            "Epoch 004: Loss: 1.766, Accuracy: 68.259%\n",
            "Epoch 004: Loss: 1.772, Accuracy: 68.090%\n",
            "Epoch 004: Loss: 1.776, Accuracy: 68.056%\n",
            "Epoch 004: Loss: 1.773, Accuracy: 68.065%\n",
            "Epoch 004: Loss: 1.783, Accuracy: 67.990%\n",
            "Epoch 004: Loss: 1.773, Accuracy: 68.250%\n",
            "Epoch 004: Loss: 1.771, Accuracy: 68.174%\n",
            "Epoch 004: Loss: 1.770, Accuracy: 68.222%\n",
            "Epoch 004: Loss: 1.771, Accuracy: 68.149%\n",
            "Epoch 004: Loss: 1.771, Accuracy: 68.078%\n",
            "Epoch 004: Loss: 1.772, Accuracy: 67.930%\n",
            "Epoch 004: Loss: 1.770, Accuracy: 67.978%\n",
            "Epoch 004: Loss: 1.767, Accuracy: 68.064%\n",
            "Epoch 004: Loss: 1.763, Accuracy: 68.185%\n",
            "Epoch 004: Loss: 1.763, Accuracy: 68.155%\n",
            "Epoch 004: Loss: 1.770, Accuracy: 68.015%\n",
            "Epoch 004: Loss: 1.765, Accuracy: 68.205%\n",
            "Epoch 004: Loss: 1.761, Accuracy: 68.319%\n",
            "Epoch 004: Loss: 1.759, Accuracy: 68.430%\n",
            "Epoch 004: Loss: 1.760, Accuracy: 68.399%\n",
            "Epoch 004: Loss: 1.761, Accuracy: 68.368%\n",
            "Epoch 004: Loss: 1.763, Accuracy: 68.304%\n",
            "Epoch 004: Loss: 1.766, Accuracy: 68.207%\n",
            "Epoch 004: Loss: 1.762, Accuracy: 68.280%\n",
            "Epoch 004: Loss: 1.770, Accuracy: 68.185%\n",
            "Epoch 004: Loss: 1.768, Accuracy: 68.257%\n",
            "Epoch 004: Loss: 1.771, Accuracy: 68.132%\n",
            "Epoch 004: Loss: 1.775, Accuracy: 68.073%\n",
            "Epoch 004: Loss: 1.775, Accuracy: 68.112%\n",
            "Epoch 004: Loss: 1.769, Accuracy: 68.213%\n",
            "Epoch 004: Loss: 1.775, Accuracy: 68.125%\n",
            "Epoch 004: Loss: 1.772, Accuracy: 68.193%\n",
            "Epoch 004: Loss: 1.773, Accuracy: 68.137%\n",
            "Epoch 004: Loss: 1.777, Accuracy: 68.083%\n",
            "Epoch 004: Loss: 1.777, Accuracy: 68.059%\n",
            "Epoch 004: Loss: 1.776, Accuracy: 68.155%\n",
            "Epoch 004: Loss: 1.778, Accuracy: 68.072%\n",
            "Epoch 004: Loss: 1.777, Accuracy: 68.049%\n",
            "Epoch 004: Loss: 1.775, Accuracy: 68.027%\n",
            "Epoch 004: Loss: 1.776, Accuracy: 68.062%\n",
            "Epoch 004: Loss: 1.774, Accuracy: 68.125%\n",
            "Epoch 004: Loss: 1.769, Accuracy: 68.271%\n",
            "Epoch 004: Loss: 1.767, Accuracy: 68.304%\n",
            "Epoch 004: Loss: 1.766, Accuracy: 68.308%\n",
            "Epoch 004: Loss: 1.770, Accuracy: 68.257%\n",
            "Epoch 004: Loss: 1.771, Accuracy: 68.261%\n",
            "Epoch 004: Loss: 1.771, Accuracy: 68.319%\n",
            "Epoch 004: Loss: 1.775, Accuracy: 68.189%\n",
            "Epoch 004: Loss: 1.775, Accuracy: 68.220%\n",
            "Epoch 004: Loss: 1.779, Accuracy: 68.067%\n",
            "Epoch 004: Loss: 1.778, Accuracy: 68.125%\n",
            "Epoch 004: Loss: 1.776, Accuracy: 68.079%\n",
            "Epoch 004: Loss: 1.776, Accuracy: 68.058%\n",
            "Epoch 004: Loss: 1.777, Accuracy: 67.962%\n",
            "Epoch 004: Loss: 1.777, Accuracy: 67.994%\n",
            "Epoch 004: Loss: 1.781, Accuracy: 67.900%\n",
            "Epoch 004: Loss: 1.779, Accuracy: 67.882%\n",
            "Epoch 004: Loss: 1.781, Accuracy: 67.864%\n",
            "Epoch 004: Loss: 1.784, Accuracy: 67.798%\n",
            "Epoch 004: Loss: 1.780, Accuracy: 67.854%\n",
            "Epoch 004: Loss: 1.777, Accuracy: 67.885%\n",
            "Epoch 004: Loss: 1.780, Accuracy: 67.796%\n",
            "Epoch 004: Loss: 1.777, Accuracy: 67.874%\n",
            "Epoch 004: Loss: 1.776, Accuracy: 67.904%\n",
            "Epoch 004: Loss: 1.777, Accuracy: 67.934%\n",
            "Epoch 004: Loss: 1.779, Accuracy: 67.894%\n",
            "Epoch 004: Loss: 1.778, Accuracy: 67.946%\n",
            "Epoch 004: Loss: 1.777, Accuracy: 67.997%\n",
            "Epoch 004: Loss: 1.774, Accuracy: 68.116%\n",
            "Epoch 004: Loss: 1.773, Accuracy: 68.121%\n",
            "Epoch 004: Loss: 1.774, Accuracy: 68.103%\n",
            "Epoch 004: Loss: 1.775, Accuracy: 68.063%\n",
            "Epoch 004: Loss: 1.770, Accuracy: 68.178%\n",
            "Epoch 004: Loss: 1.768, Accuracy: 68.226%\n",
            "Epoch 004: Loss: 1.766, Accuracy: 68.273%\n",
            "Epoch 004: Loss: 1.767, Accuracy: 68.254%\n",
            "Epoch 004: Loss: 1.767, Accuracy: 68.301%\n",
            "Epoch 004: Loss: 1.766, Accuracy: 68.325%\n",
            "Epoch 004: Loss: 1.770, Accuracy: 68.285%\n",
            "Epoch 004: Loss: 1.773, Accuracy: 68.289%\n",
            "Epoch 004: Loss: 1.770, Accuracy: 68.354%\n",
            "Epoch 004: Loss: 1.769, Accuracy: 68.377%\n",
            "Epoch 004: Loss: 1.770, Accuracy: 68.359%\n",
            "Epoch 004: Loss: 1.770, Accuracy: 68.362%\n",
            "Epoch 004: Loss: 1.767, Accuracy: 68.446%\n",
            "Epoch 004: Loss: 1.766, Accuracy: 68.488%\n",
            "Epoch 004: Loss: 1.768, Accuracy: 68.450%\n",
            "Epoch 004: Loss: 1.766, Accuracy: 68.471%\n",
            "Epoch 004: Loss: 1.769, Accuracy: 68.414%\n",
            "Epoch 004: Loss: 1.770, Accuracy: 68.377%\n",
            "Epoch 004: Loss: 1.772, Accuracy: 68.281%\n",
            "Epoch 004: Loss: 1.773, Accuracy: 68.265%\n",
            "Epoch 004: Loss: 1.771, Accuracy: 68.287%\n",
            "Epoch 004: Loss: 1.767, Accuracy: 68.328%\n",
            "Epoch 004: Loss: 1.769, Accuracy: 68.312%\n",
            "Epoch 004: Loss: 1.769, Accuracy: 68.333%\n",
            "Epoch 004: Loss: 1.770, Accuracy: 68.317%\n",
            "Epoch 004: Loss: 1.771, Accuracy: 68.245%\n",
            "Epoch 004: Loss: 1.774, Accuracy: 68.155%\n",
            "Epoch 004: Loss: 1.773, Accuracy: 68.140%\n",
            "Epoch 004: Loss: 1.773, Accuracy: 68.162%\n",
            "Epoch 004: Loss: 1.774, Accuracy: 68.165%\n",
            "Epoch 004: Loss: 1.770, Accuracy: 68.241%\n",
            "Epoch 004: Loss: 1.767, Accuracy: 68.353%\n",
            "Epoch 004: Loss: 1.764, Accuracy: 68.427%\n",
            "Epoch 004: Loss: 1.761, Accuracy: 68.482%\n",
            "Epoch 004: Loss: 1.762, Accuracy: 68.501%\n",
            "Epoch 004: Loss: 1.761, Accuracy: 68.538%\n",
            "Epoch 004: Loss: 1.761, Accuracy: 68.557%\n",
            "Epoch 004: Loss: 1.762, Accuracy: 68.541%\n",
            "Epoch 004: Loss: 1.761, Accuracy: 68.559%\n",
            "Epoch 004: Loss: 1.762, Accuracy: 68.508%\n",
            "Epoch 004: Loss: 1.759, Accuracy: 68.613%\n",
            "Epoch 004: Loss: 1.758, Accuracy: 68.682%\n",
            "Epoch 004: Loss: 1.759, Accuracy: 68.648%\n",
            "Epoch 004: Loss: 1.761, Accuracy: 68.615%\n",
            "Epoch 004: Loss: 1.762, Accuracy: 68.548%\n",
            "Epoch 004: Loss: 1.763, Accuracy: 68.549%\n",
            "Epoch 004: Loss: 1.759, Accuracy: 68.650%\n",
            "Epoch 004: Loss: 1.760, Accuracy: 68.651%\n",
            "Epoch 004: Loss: 1.758, Accuracy: 68.651%\n",
            "Epoch 004: Loss: 1.758, Accuracy: 68.652%\n",
            "Epoch 004: Loss: 1.759, Accuracy: 68.636%\n",
            "Epoch 004: Loss: 1.759, Accuracy: 68.556%\n",
            "Epoch 004: Loss: 1.760, Accuracy: 68.557%\n",
            "Epoch 004: Loss: 1.762, Accuracy: 68.462%\n",
            "Epoch 004: Loss: 1.761, Accuracy: 68.447%\n",
            "Epoch 004: Loss: 1.760, Accuracy: 68.464%\n",
            "Epoch 004: Loss: 1.758, Accuracy: 68.466%\n",
            "Epoch 004: Loss: 1.757, Accuracy: 68.483%\n",
            "Epoch 004: Loss: 1.757, Accuracy: 68.531%\n",
            "Epoch 004: Loss: 1.755, Accuracy: 68.563%\n",
            "Epoch 004: Loss: 1.757, Accuracy: 68.487%\n",
            "Epoch 004: Loss: 1.756, Accuracy: 68.458%\n",
            "Epoch 004: Loss: 1.754, Accuracy: 68.490%\n",
            "Epoch 004: Loss: 1.755, Accuracy: 68.476%\n",
            "Epoch 004: Loss: 1.755, Accuracy: 68.477%\n",
            "Epoch 004: Loss: 1.752, Accuracy: 68.539%\n",
            "Epoch 004: Loss: 1.752, Accuracy: 68.555%\n",
            "Epoch 004: Loss: 1.752, Accuracy: 68.586%\n",
            "Epoch 004: Loss: 1.752, Accuracy: 68.616%\n",
            "Epoch 004: Loss: 1.753, Accuracy: 68.602%\n",
            "Epoch 004: Loss: 1.752, Accuracy: 68.632%\n",
            "Epoch 004: Loss: 1.752, Accuracy: 68.618%\n",
            "Epoch 004: Loss: 1.754, Accuracy: 68.575%\n",
            "Epoch 004: Loss: 1.755, Accuracy: 68.561%\n",
            "Epoch 004: Loss: 1.755, Accuracy: 68.547%\n",
            "Epoch 004: Loss: 1.754, Accuracy: 68.534%\n",
            "Epoch 004: Loss: 1.754, Accuracy: 68.535%\n",
            "Epoch 004: Loss: 1.754, Accuracy: 68.522%\n",
            "Epoch 004: Loss: 1.752, Accuracy: 68.594%\n",
            "Epoch 004: Loss: 1.751, Accuracy: 68.566%\n",
            "Epoch 004: Loss: 1.752, Accuracy: 68.553%\n",
            "Epoch 004: Loss: 1.750, Accuracy: 68.610%\n",
            "Epoch 004: Loss: 1.751, Accuracy: 68.569%\n",
            "Epoch 004: Loss: 1.748, Accuracy: 68.639%\n",
            "Epoch 004: Loss: 1.747, Accuracy: 68.639%\n",
            "Epoch 004: Loss: 1.747, Accuracy: 68.667%\n",
            "Epoch 004: Loss: 1.746, Accuracy: 68.723%\n",
            "Epoch 004: Loss: 1.746, Accuracy: 68.709%\n",
            "Epoch 004: Loss: 1.747, Accuracy: 68.709%\n",
            "Epoch 004: Loss: 1.747, Accuracy: 68.723%\n",
            "Epoch 004: Loss: 1.744, Accuracy: 68.790%\n",
            "Epoch 004: Loss: 1.744, Accuracy: 68.804%\n",
            "Epoch 004: Loss: 1.744, Accuracy: 68.803%\n",
            "Epoch 004: Loss: 1.745, Accuracy: 68.777%\n",
            "Epoch 004: Loss: 1.744, Accuracy: 68.803%\n",
            "Epoch 004: Loss: 1.742, Accuracy: 68.829%\n",
            "Epoch 004: Loss: 1.743, Accuracy: 68.803%\n",
            "Epoch 004: Loss: 1.745, Accuracy: 68.750%\n",
            "Epoch 004: Loss: 1.744, Accuracy: 68.750%\n",
            "Epoch 004: Loss: 1.745, Accuracy: 68.711%\n",
            "Epoch 004: Loss: 1.745, Accuracy: 68.711%\n",
            "Epoch 004: Loss: 1.744, Accuracy: 68.737%\n",
            "Epoch 004: Loss: 1.745, Accuracy: 68.737%\n",
            "Epoch 004: Loss: 1.747, Accuracy: 68.724%\n",
            "Epoch 004: Loss: 1.745, Accuracy: 68.750%\n",
            "Epoch 004: Loss: 1.744, Accuracy: 68.775%\n",
            "Epoch 004: Loss: 1.742, Accuracy: 68.775%\n",
            "Epoch 004: Loss: 1.743, Accuracy: 68.737%\n",
            "Epoch 004: Loss: 1.742, Accuracy: 68.775%\n",
            "Epoch 004: Loss: 1.741, Accuracy: 68.800%\n",
            "Epoch 004: Loss: 1.740, Accuracy: 68.837%\n",
            "Epoch 004: Loss: 1.741, Accuracy: 68.812%\n",
            "Epoch 004: Loss: 1.742, Accuracy: 68.787%\n",
            "Epoch 004: Loss: 1.741, Accuracy: 68.799%\n",
            "Epoch 004: Loss: 1.741, Accuracy: 68.823%\n",
            "Epoch 004: Loss: 1.742, Accuracy: 68.847%\n",
            "Epoch 004: Loss: 1.742, Accuracy: 68.859%\n",
            "Epoch 004: Loss: 1.742, Accuracy: 68.859%\n",
            "Epoch 004: Loss: 1.742, Accuracy: 68.846%\n",
            "Epoch 004: Loss: 1.742, Accuracy: 68.858%\n",
            "Epoch 004: Loss: 1.742, Accuracy: 68.857%\n",
            "Epoch 004: Loss: 1.743, Accuracy: 68.845%\n",
            "Epoch 004: Loss: 1.743, Accuracy: 68.845%\n",
            "Epoch 004: Loss: 1.742, Accuracy: 68.856%\n",
            "Epoch 004: Loss: 1.743, Accuracy: 68.891%\n",
            "Epoch 004: Loss: 1.744, Accuracy: 68.890%\n",
            "Epoch 004: Loss: 1.743, Accuracy: 68.902%\n",
            "Epoch 004: Loss: 1.744, Accuracy: 68.866%\n",
            "Epoch 004: Loss: 1.746, Accuracy: 68.785%\n",
            "Epoch 004: Loss: 1.745, Accuracy: 68.819%\n",
            "Epoch 004: Loss: 1.744, Accuracy: 68.819%\n",
            "Epoch 004: Loss: 1.744, Accuracy: 68.830%\n",
            "Epoch 004: Loss: 1.744, Accuracy: 68.841%\n",
            "Epoch 004: Loss: 1.744, Accuracy: 68.830%\n",
            "Epoch 004: Loss: 1.743, Accuracy: 68.886%\n",
            "Epoch 004: Loss: 1.744, Accuracy: 68.840%\n",
            "Epoch 004: Loss: 1.745, Accuracy: 68.806%\n",
            "Epoch 004: Loss: 1.745, Accuracy: 68.840%\n",
            "Epoch 004: Loss: 1.746, Accuracy: 68.817%\n",
            "Epoch 004: Loss: 1.746, Accuracy: 68.815%\n",
            "Epoch 005: Loss: 1.058, Accuracy: 81.250%\n",
            "Epoch 005: Loss: 1.227, Accuracy: 81.250%\n",
            "Epoch 005: Loss: 1.411, Accuracy: 75.000%\n",
            "Epoch 005: Loss: 1.431, Accuracy: 76.562%\n",
            "Epoch 005: Loss: 1.419, Accuracy: 77.500%\n",
            "Epoch 005: Loss: 1.418, Accuracy: 77.604%\n",
            "Epoch 005: Loss: 1.548, Accuracy: 75.446%\n",
            "Epoch 005: Loss: 1.498, Accuracy: 76.953%\n",
            "Epoch 005: Loss: 1.601, Accuracy: 73.611%\n",
            "Epoch 005: Loss: 1.572, Accuracy: 73.750%\n",
            "Epoch 005: Loss: 1.543, Accuracy: 74.148%\n",
            "Epoch 005: Loss: 1.557, Accuracy: 73.698%\n",
            "Epoch 005: Loss: 1.577, Accuracy: 73.317%\n",
            "Epoch 005: Loss: 1.585, Accuracy: 72.768%\n",
            "Epoch 005: Loss: 1.594, Accuracy: 72.500%\n",
            "Epoch 005: Loss: 1.613, Accuracy: 71.875%\n",
            "Epoch 005: Loss: 1.627, Accuracy: 71.691%\n",
            "Epoch 005: Loss: 1.621, Accuracy: 71.701%\n",
            "Epoch 005: Loss: 1.615, Accuracy: 71.711%\n",
            "Epoch 005: Loss: 1.613, Accuracy: 71.875%\n",
            "Epoch 005: Loss: 1.615, Accuracy: 71.429%\n",
            "Epoch 005: Loss: 1.608, Accuracy: 71.733%\n",
            "Epoch 005: Loss: 1.638, Accuracy: 71.060%\n",
            "Epoch 005: Loss: 1.647, Accuracy: 70.833%\n",
            "Epoch 005: Loss: 1.631, Accuracy: 71.250%\n",
            "Epoch 005: Loss: 1.642, Accuracy: 71.034%\n",
            "Epoch 005: Loss: 1.670, Accuracy: 70.602%\n",
            "Epoch 005: Loss: 1.678, Accuracy: 70.201%\n",
            "Epoch 005: Loss: 1.673, Accuracy: 70.151%\n",
            "Epoch 005: Loss: 1.667, Accuracy: 70.208%\n",
            "Epoch 005: Loss: 1.656, Accuracy: 70.665%\n",
            "Epoch 005: Loss: 1.660, Accuracy: 70.703%\n",
            "Epoch 005: Loss: 1.677, Accuracy: 70.549%\n",
            "Epoch 005: Loss: 1.687, Accuracy: 70.404%\n",
            "Epoch 005: Loss: 1.710, Accuracy: 70.179%\n",
            "Epoch 005: Loss: 1.711, Accuracy: 70.312%\n",
            "Epoch 005: Loss: 1.711, Accuracy: 70.270%\n",
            "Epoch 005: Loss: 1.714, Accuracy: 70.395%\n",
            "Epoch 005: Loss: 1.717, Accuracy: 70.513%\n",
            "Epoch 005: Loss: 1.702, Accuracy: 70.781%\n",
            "Epoch 005: Loss: 1.706, Accuracy: 70.732%\n",
            "Epoch 005: Loss: 1.708, Accuracy: 70.610%\n",
            "Epoch 005: Loss: 1.701, Accuracy: 70.785%\n",
            "Epoch 005: Loss: 1.715, Accuracy: 70.241%\n",
            "Epoch 005: Loss: 1.713, Accuracy: 70.139%\n",
            "Epoch 005: Loss: 1.723, Accuracy: 69.837%\n",
            "Epoch 005: Loss: 1.716, Accuracy: 70.013%\n",
            "Epoch 005: Loss: 1.720, Accuracy: 69.922%\n",
            "Epoch 005: Loss: 1.717, Accuracy: 70.026%\n",
            "Epoch 005: Loss: 1.718, Accuracy: 69.875%\n",
            "Epoch 005: Loss: 1.718, Accuracy: 69.853%\n",
            "Epoch 005: Loss: 1.718, Accuracy: 70.072%\n",
            "Epoch 005: Loss: 1.725, Accuracy: 69.929%\n",
            "Epoch 005: Loss: 1.727, Accuracy: 70.023%\n",
            "Epoch 005: Loss: 1.724, Accuracy: 70.000%\n",
            "Epoch 005: Loss: 1.727, Accuracy: 69.922%\n",
            "Epoch 005: Loss: 1.726, Accuracy: 69.901%\n",
            "Epoch 005: Loss: 1.729, Accuracy: 69.935%\n",
            "Epoch 005: Loss: 1.738, Accuracy: 69.756%\n",
            "Epoch 005: Loss: 1.732, Accuracy: 69.948%\n",
            "Epoch 005: Loss: 1.726, Accuracy: 70.133%\n",
            "Epoch 005: Loss: 1.724, Accuracy: 70.212%\n",
            "Epoch 005: Loss: 1.728, Accuracy: 70.139%\n",
            "Epoch 005: Loss: 1.731, Accuracy: 70.020%\n",
            "Epoch 005: Loss: 1.741, Accuracy: 69.808%\n",
            "Epoch 005: Loss: 1.741, Accuracy: 69.886%\n",
            "Epoch 005: Loss: 1.735, Accuracy: 69.963%\n",
            "Epoch 005: Loss: 1.735, Accuracy: 69.991%\n",
            "Epoch 005: Loss: 1.736, Accuracy: 69.928%\n",
            "Epoch 005: Loss: 1.736, Accuracy: 69.955%\n",
            "Epoch 005: Loss: 1.745, Accuracy: 69.806%\n",
            "Epoch 005: Loss: 1.751, Accuracy: 69.922%\n",
            "Epoch 005: Loss: 1.750, Accuracy: 69.949%\n",
            "Epoch 005: Loss: 1.756, Accuracy: 69.806%\n",
            "Epoch 005: Loss: 1.749, Accuracy: 70.042%\n",
            "Epoch 005: Loss: 1.749, Accuracy: 70.025%\n",
            "Epoch 005: Loss: 1.750, Accuracy: 69.968%\n",
            "Epoch 005: Loss: 1.750, Accuracy: 69.872%\n",
            "Epoch 005: Loss: 1.749, Accuracy: 69.897%\n",
            "Epoch 005: Loss: 1.751, Accuracy: 69.727%\n",
            "Epoch 005: Loss: 1.750, Accuracy: 69.753%\n",
            "Epoch 005: Loss: 1.747, Accuracy: 69.779%\n",
            "Epoch 005: Loss: 1.744, Accuracy: 69.917%\n",
            "Epoch 005: Loss: 1.745, Accuracy: 69.940%\n",
            "Epoch 005: Loss: 1.751, Accuracy: 69.853%\n",
            "Epoch 005: Loss: 1.746, Accuracy: 70.022%\n",
            "Epoch 005: Loss: 1.742, Accuracy: 70.079%\n",
            "Epoch 005: Loss: 1.743, Accuracy: 70.170%\n",
            "Epoch 005: Loss: 1.745, Accuracy: 70.119%\n",
            "Epoch 005: Loss: 1.745, Accuracy: 70.035%\n",
            "Epoch 005: Loss: 1.745, Accuracy: 70.021%\n",
            "Epoch 005: Loss: 1.749, Accuracy: 69.939%\n",
            "Epoch 005: Loss: 1.745, Accuracy: 69.993%\n",
            "Epoch 005: Loss: 1.752, Accuracy: 69.914%\n",
            "Epoch 005: Loss: 1.751, Accuracy: 70.000%\n",
            "Epoch 005: Loss: 1.754, Accuracy: 69.889%\n",
            "Epoch 005: Loss: 1.757, Accuracy: 69.845%\n",
            "Epoch 005: Loss: 1.757, Accuracy: 69.834%\n",
            "Epoch 005: Loss: 1.752, Accuracy: 69.886%\n",
            "Epoch 005: Loss: 1.757, Accuracy: 69.750%\n",
            "Epoch 005: Loss: 1.754, Accuracy: 69.802%\n",
            "Epoch 005: Loss: 1.756, Accuracy: 69.761%\n",
            "Epoch 005: Loss: 1.760, Accuracy: 69.630%\n",
            "Epoch 005: Loss: 1.760, Accuracy: 69.651%\n",
            "Epoch 005: Loss: 1.760, Accuracy: 69.732%\n",
            "Epoch 005: Loss: 1.760, Accuracy: 69.693%\n",
            "Epoch 005: Loss: 1.759, Accuracy: 69.714%\n",
            "Epoch 005: Loss: 1.760, Accuracy: 69.676%\n",
            "Epoch 005: Loss: 1.760, Accuracy: 69.639%\n",
            "Epoch 005: Loss: 1.758, Accuracy: 69.631%\n",
            "Epoch 005: Loss: 1.754, Accuracy: 69.792%\n",
            "Epoch 005: Loss: 1.753, Accuracy: 69.866%\n",
            "Epoch 005: Loss: 1.752, Accuracy: 69.856%\n",
            "Epoch 005: Loss: 1.754, Accuracy: 69.819%\n",
            "Epoch 005: Loss: 1.756, Accuracy: 69.864%\n",
            "Epoch 005: Loss: 1.756, Accuracy: 69.855%\n",
            "Epoch 005: Loss: 1.758, Accuracy: 69.738%\n",
            "Epoch 005: Loss: 1.757, Accuracy: 69.783%\n",
            "Epoch 005: Loss: 1.762, Accuracy: 69.643%\n",
            "Epoch 005: Loss: 1.761, Accuracy: 69.661%\n",
            "Epoch 005: Loss: 1.760, Accuracy: 69.680%\n",
            "Epoch 005: Loss: 1.761, Accuracy: 69.595%\n",
            "Epoch 005: Loss: 1.763, Accuracy: 69.538%\n",
            "Epoch 005: Loss: 1.762, Accuracy: 69.556%\n",
            "Epoch 005: Loss: 1.767, Accuracy: 69.450%\n",
            "Epoch 005: Loss: 1.766, Accuracy: 69.469%\n",
            "Epoch 005: Loss: 1.768, Accuracy: 69.488%\n",
            "Epoch 005: Loss: 1.770, Accuracy: 69.434%\n",
            "Epoch 005: Loss: 1.766, Accuracy: 69.549%\n",
            "Epoch 005: Loss: 1.764, Accuracy: 69.519%\n",
            "Epoch 005: Loss: 1.765, Accuracy: 69.418%\n",
            "Epoch 005: Loss: 1.763, Accuracy: 69.508%\n",
            "Epoch 005: Loss: 1.761, Accuracy: 69.525%\n",
            "Epoch 005: Loss: 1.761, Accuracy: 69.590%\n",
            "Epoch 005: Loss: 1.765, Accuracy: 69.514%\n",
            "Epoch 005: Loss: 1.763, Accuracy: 69.554%\n",
            "Epoch 005: Loss: 1.760, Accuracy: 69.640%\n",
            "Epoch 005: Loss: 1.756, Accuracy: 69.769%\n",
            "Epoch 005: Loss: 1.755, Accuracy: 69.829%\n",
            "Epoch 005: Loss: 1.755, Accuracy: 69.821%\n",
            "Epoch 005: Loss: 1.757, Accuracy: 69.747%\n",
            "Epoch 005: Loss: 1.752, Accuracy: 69.850%\n",
            "Epoch 005: Loss: 1.750, Accuracy: 69.952%\n",
            "Epoch 005: Loss: 1.748, Accuracy: 69.965%\n",
            "Epoch 005: Loss: 1.748, Accuracy: 69.957%\n",
            "Epoch 005: Loss: 1.748, Accuracy: 69.970%\n",
            "Epoch 005: Loss: 1.747, Accuracy: 69.983%\n",
            "Epoch 005: Loss: 1.748, Accuracy: 69.890%\n",
            "Epoch 005: Loss: 1.750, Accuracy: 69.883%\n",
            "Epoch 005: Loss: 1.747, Accuracy: 69.937%\n",
            "Epoch 005: Loss: 1.746, Accuracy: 69.950%\n",
            "Epoch 005: Loss: 1.747, Accuracy: 69.922%\n",
            "Epoch 005: Loss: 1.747, Accuracy: 69.894%\n",
            "Epoch 005: Loss: 1.747, Accuracy: 69.927%\n",
            "Epoch 005: Loss: 1.745, Accuracy: 69.940%\n",
            "Epoch 005: Loss: 1.747, Accuracy: 69.952%\n",
            "Epoch 005: Loss: 1.745, Accuracy: 69.984%\n",
            "Epoch 005: Loss: 1.749, Accuracy: 69.877%\n",
            "Epoch 005: Loss: 1.750, Accuracy: 69.870%\n",
            "Epoch 005: Loss: 1.753, Accuracy: 69.824%\n",
            "Epoch 005: Loss: 1.754, Accuracy: 69.818%\n",
            "Epoch 005: Loss: 1.752, Accuracy: 69.792%\n",
            "Epoch 005: Loss: 1.750, Accuracy: 69.824%\n",
            "Epoch 005: Loss: 1.751, Accuracy: 69.760%\n",
            "Epoch 005: Loss: 1.752, Accuracy: 69.754%\n",
            "Epoch 005: Loss: 1.752, Accuracy: 69.785%\n",
            "Epoch 005: Loss: 1.756, Accuracy: 69.742%\n",
            "Epoch 005: Loss: 1.758, Accuracy: 69.624%\n",
            "Epoch 005: Loss: 1.759, Accuracy: 69.619%\n",
            "Epoch 005: Loss: 1.758, Accuracy: 69.614%\n",
            "Epoch 005: Loss: 1.758, Accuracy: 69.627%\n",
            "Epoch 005: Loss: 1.755, Accuracy: 69.731%\n",
            "Epoch 005: Loss: 1.751, Accuracy: 69.798%\n",
            "Epoch 005: Loss: 1.747, Accuracy: 69.864%\n",
            "Epoch 005: Loss: 1.744, Accuracy: 69.929%\n",
            "Epoch 005: Loss: 1.744, Accuracy: 69.940%\n",
            "Epoch 005: Loss: 1.743, Accuracy: 69.986%\n",
            "Epoch 005: Loss: 1.742, Accuracy: 69.996%\n",
            "Epoch 005: Loss: 1.744, Accuracy: 69.990%\n",
            "Epoch 005: Loss: 1.744, Accuracy: 69.983%\n",
            "Epoch 005: Loss: 1.745, Accuracy: 69.959%\n",
            "Epoch 005: Loss: 1.741, Accuracy: 70.055%\n",
            "Epoch 005: Loss: 1.740, Accuracy: 70.116%\n",
            "Epoch 005: Loss: 1.741, Accuracy: 70.058%\n",
            "Epoch 005: Loss: 1.743, Accuracy: 70.000%\n",
            "Epoch 005: Loss: 1.744, Accuracy: 69.926%\n",
            "Epoch 005: Loss: 1.745, Accuracy: 69.920%\n",
            "Epoch 005: Loss: 1.741, Accuracy: 69.997%\n",
            "Epoch 005: Loss: 1.741, Accuracy: 69.990%\n",
            "Epoch 005: Loss: 1.740, Accuracy: 70.000%\n",
            "Epoch 005: Loss: 1.739, Accuracy: 69.977%\n",
            "Epoch 005: Loss: 1.741, Accuracy: 69.922%\n",
            "Epoch 005: Loss: 1.741, Accuracy: 69.883%\n",
            "Epoch 005: Loss: 1.742, Accuracy: 69.861%\n",
            "Epoch 005: Loss: 1.745, Accuracy: 69.792%\n",
            "Epoch 005: Loss: 1.744, Accuracy: 69.818%\n",
            "Epoch 005: Loss: 1.743, Accuracy: 69.829%\n",
            "Epoch 005: Loss: 1.743, Accuracy: 69.839%\n",
            "Epoch 005: Loss: 1.742, Accuracy: 69.865%\n",
            "Epoch 005: Loss: 1.742, Accuracy: 69.891%\n",
            "Epoch 005: Loss: 1.740, Accuracy: 69.947%\n",
            "Epoch 005: Loss: 1.743, Accuracy: 69.864%\n",
            "Epoch 005: Loss: 1.742, Accuracy: 69.905%\n",
            "Epoch 005: Loss: 1.740, Accuracy: 69.945%\n",
            "Epoch 005: Loss: 1.741, Accuracy: 69.893%\n",
            "Epoch 005: Loss: 1.741, Accuracy: 69.903%\n",
            "Epoch 005: Loss: 1.738, Accuracy: 69.943%\n",
            "Epoch 005: Loss: 1.737, Accuracy: 69.937%\n",
            "Epoch 005: Loss: 1.736, Accuracy: 69.961%\n",
            "Epoch 005: Loss: 1.737, Accuracy: 69.970%\n",
            "Epoch 005: Loss: 1.737, Accuracy: 69.950%\n",
            "Epoch 005: Loss: 1.736, Accuracy: 69.959%\n",
            "Epoch 005: Loss: 1.737, Accuracy: 69.938%\n",
            "Epoch 005: Loss: 1.740, Accuracy: 69.874%\n",
            "Epoch 005: Loss: 1.741, Accuracy: 69.855%\n",
            "Epoch 005: Loss: 1.740, Accuracy: 69.850%\n",
            "Epoch 005: Loss: 1.740, Accuracy: 69.859%\n",
            "Epoch 005: Loss: 1.739, Accuracy: 69.854%\n",
            "Epoch 005: Loss: 1.740, Accuracy: 69.820%\n",
            "Epoch 005: Loss: 1.737, Accuracy: 69.886%\n",
            "Epoch 005: Loss: 1.736, Accuracy: 69.881%\n",
            "Epoch 005: Loss: 1.736, Accuracy: 69.876%\n",
            "Epoch 005: Loss: 1.736, Accuracy: 69.913%\n",
            "Epoch 005: Loss: 1.736, Accuracy: 69.908%\n",
            "Epoch 005: Loss: 1.734, Accuracy: 69.972%\n",
            "Epoch 005: Loss: 1.733, Accuracy: 69.967%\n",
            "Epoch 005: Loss: 1.732, Accuracy: 69.975%\n",
            "Epoch 005: Loss: 1.731, Accuracy: 70.025%\n",
            "Epoch 005: Loss: 1.731, Accuracy: 69.978%\n",
            "Epoch 005: Loss: 1.731, Accuracy: 69.959%\n",
            "Epoch 005: Loss: 1.731, Accuracy: 69.954%\n",
            "Epoch 005: Loss: 1.729, Accuracy: 70.016%\n",
            "Epoch 005: Loss: 1.728, Accuracy: 70.051%\n",
            "Epoch 005: Loss: 1.727, Accuracy: 70.072%\n",
            "Epoch 005: Loss: 1.728, Accuracy: 70.027%\n",
            "Epoch 005: Loss: 1.727, Accuracy: 70.034%\n",
            "Epoch 005: Loss: 1.725, Accuracy: 70.055%\n",
            "Epoch 005: Loss: 1.726, Accuracy: 70.050%\n",
            "Epoch 005: Loss: 1.727, Accuracy: 70.018%\n",
            "Epoch 005: Loss: 1.727, Accuracy: 70.000%\n",
            "Epoch 005: Loss: 1.728, Accuracy: 69.982%\n",
            "Epoch 005: Loss: 1.728, Accuracy: 70.003%\n",
            "Epoch 005: Loss: 1.728, Accuracy: 70.010%\n",
            "Epoch 005: Loss: 1.728, Accuracy: 69.992%\n",
            "Epoch 005: Loss: 1.728, Accuracy: 69.987%\n",
            "Epoch 005: Loss: 1.727, Accuracy: 70.020%\n",
            "Epoch 005: Loss: 1.725, Accuracy: 70.053%\n",
            "Epoch 005: Loss: 1.724, Accuracy: 70.048%\n",
            "Epoch 005: Loss: 1.725, Accuracy: 70.005%\n",
            "Epoch 005: Loss: 1.724, Accuracy: 70.012%\n",
            "Epoch 005: Loss: 1.723, Accuracy: 70.045%\n",
            "Epoch 005: Loss: 1.723, Accuracy: 70.052%\n",
            "Epoch 005: Loss: 1.724, Accuracy: 69.985%\n",
            "Epoch 005: Loss: 1.725, Accuracy: 69.943%\n",
            "Epoch 005: Loss: 1.725, Accuracy: 69.914%\n",
            "Epoch 005: Loss: 1.725, Accuracy: 69.873%\n",
            "Epoch 005: Loss: 1.725, Accuracy: 69.905%\n",
            "Epoch 005: Loss: 1.724, Accuracy: 69.889%\n",
            "Epoch 005: Loss: 1.725, Accuracy: 69.896%\n",
            "Epoch 005: Loss: 1.724, Accuracy: 69.892%\n",
            "Epoch 005: Loss: 1.724, Accuracy: 69.923%\n",
            "Epoch 005: Loss: 1.724, Accuracy: 69.919%\n",
            "Epoch 005: Loss: 1.725, Accuracy: 69.903%\n",
            "Epoch 005: Loss: 1.725, Accuracy: 69.898%\n",
            "Epoch 005: Loss: 1.725, Accuracy: 69.894%\n",
            "Epoch 005: Loss: 1.725, Accuracy: 69.913%\n",
            "Epoch 005: Loss: 1.726, Accuracy: 69.897%\n",
            "Epoch 005: Loss: 1.725, Accuracy: 69.881%\n",
            "Epoch 005: Loss: 1.726, Accuracy: 69.865%\n",
            "Epoch 005: Loss: 1.728, Accuracy: 69.792%\n",
            "Epoch 005: Loss: 1.727, Accuracy: 69.834%\n",
            "Epoch 005: Loss: 1.728, Accuracy: 69.830%\n",
            "Epoch 005: Loss: 1.727, Accuracy: 69.837%\n",
            "Epoch 005: Loss: 1.727, Accuracy: 69.868%\n",
            "Epoch 005: Loss: 1.727, Accuracy: 69.909%\n",
            "Epoch 005: Loss: 1.725, Accuracy: 69.962%\n",
            "Epoch 005: Loss: 1.726, Accuracy: 69.912%\n",
            "Epoch 005: Loss: 1.727, Accuracy: 69.908%\n",
            "Epoch 005: Loss: 1.727, Accuracy: 69.937%\n",
            "Epoch 005: Loss: 1.727, Accuracy: 69.922%\n",
            "Epoch 005: Loss: 1.727, Accuracy: 69.918%\n",
            "Epoch 006: Loss: 1.147, Accuracy: 84.375%\n",
            "Epoch 006: Loss: 1.203, Accuracy: 82.812%\n",
            "Epoch 006: Loss: 1.379, Accuracy: 76.042%\n",
            "Epoch 006: Loss: 1.416, Accuracy: 76.562%\n",
            "Epoch 006: Loss: 1.406, Accuracy: 76.250%\n",
            "Epoch 006: Loss: 1.407, Accuracy: 75.521%\n",
            "Epoch 006: Loss: 1.516, Accuracy: 74.107%\n",
            "Epoch 006: Loss: 1.452, Accuracy: 76.172%\n",
            "Epoch 006: Loss: 1.572, Accuracy: 72.917%\n",
            "Epoch 006: Loss: 1.546, Accuracy: 73.438%\n",
            "Epoch 006: Loss: 1.522, Accuracy: 73.864%\n",
            "Epoch 006: Loss: 1.534, Accuracy: 73.438%\n",
            "Epoch 006: Loss: 1.547, Accuracy: 72.837%\n",
            "Epoch 006: Loss: 1.559, Accuracy: 72.768%\n",
            "Epoch 006: Loss: 1.574, Accuracy: 72.500%\n",
            "Epoch 006: Loss: 1.606, Accuracy: 71.680%\n",
            "Epoch 006: Loss: 1.623, Accuracy: 71.324%\n",
            "Epoch 006: Loss: 1.622, Accuracy: 71.181%\n",
            "Epoch 006: Loss: 1.635, Accuracy: 70.888%\n",
            "Epoch 006: Loss: 1.635, Accuracy: 71.094%\n",
            "Epoch 006: Loss: 1.648, Accuracy: 70.685%\n",
            "Epoch 006: Loss: 1.640, Accuracy: 71.023%\n",
            "Epoch 006: Loss: 1.671, Accuracy: 70.380%\n",
            "Epoch 006: Loss: 1.671, Accuracy: 70.312%\n",
            "Epoch 006: Loss: 1.658, Accuracy: 70.625%\n",
            "Epoch 006: Loss: 1.670, Accuracy: 70.433%\n",
            "Epoch 006: Loss: 1.695, Accuracy: 69.676%\n",
            "Epoch 006: Loss: 1.703, Accuracy: 69.308%\n",
            "Epoch 006: Loss: 1.704, Accuracy: 69.397%\n",
            "Epoch 006: Loss: 1.699, Accuracy: 69.583%\n",
            "Epoch 006: Loss: 1.685, Accuracy: 69.859%\n",
            "Epoch 006: Loss: 1.685, Accuracy: 69.629%\n",
            "Epoch 006: Loss: 1.700, Accuracy: 69.508%\n",
            "Epoch 006: Loss: 1.706, Accuracy: 69.210%\n",
            "Epoch 006: Loss: 1.722, Accuracy: 69.018%\n",
            "Epoch 006: Loss: 1.724, Accuracy: 69.271%\n",
            "Epoch 006: Loss: 1.729, Accuracy: 69.088%\n",
            "Epoch 006: Loss: 1.730, Accuracy: 69.243%\n",
            "Epoch 006: Loss: 1.730, Accuracy: 69.391%\n",
            "Epoch 006: Loss: 1.717, Accuracy: 69.844%\n",
            "Epoch 006: Loss: 1.716, Accuracy: 69.817%\n",
            "Epoch 006: Loss: 1.719, Accuracy: 69.717%\n",
            "Epoch 006: Loss: 1.716, Accuracy: 69.985%\n",
            "Epoch 006: Loss: 1.730, Accuracy: 69.389%\n",
            "Epoch 006: Loss: 1.725, Accuracy: 69.375%\n",
            "Epoch 006: Loss: 1.730, Accuracy: 69.022%\n",
            "Epoch 006: Loss: 1.725, Accuracy: 69.282%\n",
            "Epoch 006: Loss: 1.726, Accuracy: 69.271%\n",
            "Epoch 006: Loss: 1.722, Accuracy: 69.388%\n",
            "Epoch 006: Loss: 1.719, Accuracy: 69.437%\n",
            "Epoch 006: Loss: 1.720, Accuracy: 69.485%\n",
            "Epoch 006: Loss: 1.722, Accuracy: 69.651%\n",
            "Epoch 006: Loss: 1.727, Accuracy: 69.458%\n",
            "Epoch 006: Loss: 1.730, Accuracy: 69.502%\n",
            "Epoch 006: Loss: 1.725, Accuracy: 69.489%\n",
            "Epoch 006: Loss: 1.724, Accuracy: 69.420%\n",
            "Epoch 006: Loss: 1.723, Accuracy: 69.353%\n",
            "Epoch 006: Loss: 1.724, Accuracy: 69.235%\n",
            "Epoch 006: Loss: 1.738, Accuracy: 69.068%\n",
            "Epoch 006: Loss: 1.734, Accuracy: 69.271%\n",
            "Epoch 006: Loss: 1.729, Accuracy: 69.518%\n",
            "Epoch 006: Loss: 1.727, Accuracy: 69.506%\n",
            "Epoch 006: Loss: 1.733, Accuracy: 69.395%\n",
            "Epoch 006: Loss: 1.735, Accuracy: 69.336%\n",
            "Epoch 006: Loss: 1.741, Accuracy: 69.135%\n",
            "Epoch 006: Loss: 1.739, Accuracy: 69.176%\n",
            "Epoch 006: Loss: 1.736, Accuracy: 69.356%\n",
            "Epoch 006: Loss: 1.733, Accuracy: 69.485%\n",
            "Epoch 006: Loss: 1.738, Accuracy: 69.429%\n",
            "Epoch 006: Loss: 1.736, Accuracy: 69.464%\n",
            "Epoch 006: Loss: 1.746, Accuracy: 69.322%\n",
            "Epoch 006: Loss: 1.743, Accuracy: 69.401%\n",
            "Epoch 006: Loss: 1.740, Accuracy: 69.478%\n",
            "Epoch 006: Loss: 1.748, Accuracy: 69.341%\n",
            "Epoch 006: Loss: 1.740, Accuracy: 69.500%\n",
            "Epoch 006: Loss: 1.739, Accuracy: 69.490%\n",
            "Epoch 006: Loss: 1.740, Accuracy: 69.481%\n",
            "Epoch 006: Loss: 1.742, Accuracy: 69.351%\n",
            "Epoch 006: Loss: 1.743, Accuracy: 69.304%\n",
            "Epoch 006: Loss: 1.745, Accuracy: 69.258%\n",
            "Epoch 006: Loss: 1.742, Accuracy: 69.329%\n",
            "Epoch 006: Loss: 1.740, Accuracy: 69.436%\n",
            "Epoch 006: Loss: 1.737, Accuracy: 69.503%\n",
            "Epoch 006: Loss: 1.739, Accuracy: 69.457%\n",
            "Epoch 006: Loss: 1.744, Accuracy: 69.375%\n",
            "Epoch 006: Loss: 1.740, Accuracy: 69.513%\n",
            "Epoch 006: Loss: 1.734, Accuracy: 69.576%\n",
            "Epoch 006: Loss: 1.733, Accuracy: 69.602%\n",
            "Epoch 006: Loss: 1.734, Accuracy: 69.417%\n",
            "Epoch 006: Loss: 1.736, Accuracy: 69.444%\n",
            "Epoch 006: Loss: 1.737, Accuracy: 69.437%\n",
            "Epoch 006: Loss: 1.742, Accuracy: 69.395%\n",
            "Epoch 006: Loss: 1.738, Accuracy: 69.489%\n",
            "Epoch 006: Loss: 1.744, Accuracy: 69.382%\n",
            "Epoch 006: Loss: 1.744, Accuracy: 69.474%\n",
            "Epoch 006: Loss: 1.750, Accuracy: 69.434%\n",
            "Epoch 006: Loss: 1.752, Accuracy: 69.362%\n",
            "Epoch 006: Loss: 1.754, Accuracy: 69.356%\n",
            "Epoch 006: Loss: 1.750, Accuracy: 69.476%\n",
            "Epoch 006: Loss: 1.754, Accuracy: 69.406%\n",
            "Epoch 006: Loss: 1.752, Accuracy: 69.431%\n",
            "Epoch 006: Loss: 1.753, Accuracy: 69.393%\n",
            "Epoch 006: Loss: 1.758, Accuracy: 69.387%\n",
            "Epoch 006: Loss: 1.758, Accuracy: 69.381%\n",
            "Epoch 006: Loss: 1.758, Accuracy: 69.464%\n",
            "Epoch 006: Loss: 1.760, Accuracy: 69.428%\n",
            "Epoch 006: Loss: 1.758, Accuracy: 69.480%\n",
            "Epoch 006: Loss: 1.758, Accuracy: 69.502%\n",
            "Epoch 006: Loss: 1.757, Accuracy: 69.553%\n",
            "Epoch 006: Loss: 1.757, Accuracy: 69.574%\n",
            "Epoch 006: Loss: 1.753, Accuracy: 69.707%\n",
            "Epoch 006: Loss: 1.752, Accuracy: 69.810%\n",
            "Epoch 006: Loss: 1.751, Accuracy: 69.773%\n",
            "Epoch 006: Loss: 1.754, Accuracy: 69.709%\n",
            "Epoch 006: Loss: 1.754, Accuracy: 69.701%\n",
            "Epoch 006: Loss: 1.752, Accuracy: 69.747%\n",
            "Epoch 006: Loss: 1.754, Accuracy: 69.631%\n",
            "Epoch 006: Loss: 1.752, Accuracy: 69.677%\n",
            "Epoch 006: Loss: 1.757, Accuracy: 69.538%\n",
            "Epoch 006: Loss: 1.755, Accuracy: 69.557%\n",
            "Epoch 006: Loss: 1.754, Accuracy: 69.576%\n",
            "Epoch 006: Loss: 1.754, Accuracy: 69.493%\n",
            "Epoch 006: Loss: 1.756, Accuracy: 69.385%\n",
            "Epoch 006: Loss: 1.755, Accuracy: 69.380%\n",
            "Epoch 006: Loss: 1.762, Accuracy: 69.325%\n",
            "Epoch 006: Loss: 1.760, Accuracy: 69.296%\n",
            "Epoch 006: Loss: 1.762, Accuracy: 69.316%\n",
            "Epoch 006: Loss: 1.764, Accuracy: 69.287%\n",
            "Epoch 006: Loss: 1.761, Accuracy: 69.404%\n",
            "Epoch 006: Loss: 1.758, Accuracy: 69.495%\n",
            "Epoch 006: Loss: 1.761, Accuracy: 69.442%\n",
            "Epoch 006: Loss: 1.757, Accuracy: 69.508%\n",
            "Epoch 006: Loss: 1.755, Accuracy: 69.549%\n",
            "Epoch 006: Loss: 1.755, Accuracy: 69.543%\n",
            "Epoch 006: Loss: 1.759, Accuracy: 69.514%\n",
            "Epoch 006: Loss: 1.758, Accuracy: 69.577%\n",
            "Epoch 006: Loss: 1.756, Accuracy: 69.617%\n",
            "Epoch 006: Loss: 1.752, Accuracy: 69.701%\n",
            "Epoch 006: Loss: 1.753, Accuracy: 69.739%\n",
            "Epoch 006: Loss: 1.752, Accuracy: 69.710%\n",
            "Epoch 006: Loss: 1.752, Accuracy: 69.659%\n",
            "Epoch 006: Loss: 1.747, Accuracy: 69.784%\n",
            "Epoch 006: Loss: 1.745, Accuracy: 69.799%\n",
            "Epoch 006: Loss: 1.744, Accuracy: 69.813%\n",
            "Epoch 006: Loss: 1.746, Accuracy: 69.806%\n",
            "Epoch 006: Loss: 1.746, Accuracy: 69.820%\n",
            "Epoch 006: Loss: 1.746, Accuracy: 69.792%\n",
            "Epoch 006: Loss: 1.748, Accuracy: 69.700%\n",
            "Epoch 006: Loss: 1.750, Accuracy: 69.694%\n",
            "Epoch 006: Loss: 1.747, Accuracy: 69.792%\n",
            "Epoch 006: Loss: 1.747, Accuracy: 69.805%\n",
            "Epoch 006: Loss: 1.746, Accuracy: 69.778%\n",
            "Epoch 006: Loss: 1.746, Accuracy: 69.751%\n",
            "Epoch 006: Loss: 1.745, Accuracy: 69.805%\n",
            "Epoch 006: Loss: 1.744, Accuracy: 69.839%\n",
            "Epoch 006: Loss: 1.746, Accuracy: 69.812%\n",
            "Epoch 006: Loss: 1.743, Accuracy: 69.885%\n",
            "Epoch 006: Loss: 1.745, Accuracy: 69.818%\n",
            "Epoch 006: Loss: 1.746, Accuracy: 69.772%\n",
            "Epoch 006: Loss: 1.747, Accuracy: 69.707%\n",
            "Epoch 006: Loss: 1.749, Accuracy: 69.682%\n",
            "Epoch 006: Loss: 1.747, Accuracy: 69.676%\n",
            "Epoch 006: Loss: 1.744, Accuracy: 69.709%\n",
            "Epoch 006: Loss: 1.745, Accuracy: 69.684%\n",
            "Epoch 006: Loss: 1.745, Accuracy: 69.697%\n",
            "Epoch 006: Loss: 1.745, Accuracy: 69.710%\n",
            "Epoch 006: Loss: 1.748, Accuracy: 69.629%\n",
            "Epoch 006: Loss: 1.750, Accuracy: 69.550%\n",
            "Epoch 006: Loss: 1.750, Accuracy: 69.527%\n",
            "Epoch 006: Loss: 1.750, Accuracy: 69.540%\n",
            "Epoch 006: Loss: 1.749, Accuracy: 69.536%\n",
            "Epoch 006: Loss: 1.746, Accuracy: 69.622%\n",
            "Epoch 006: Loss: 1.743, Accuracy: 69.743%\n",
            "Epoch 006: Loss: 1.740, Accuracy: 69.810%\n",
            "Epoch 006: Loss: 1.737, Accuracy: 69.875%\n",
            "Epoch 006: Loss: 1.737, Accuracy: 69.886%\n",
            "Epoch 006: Loss: 1.736, Accuracy: 69.933%\n",
            "Epoch 006: Loss: 1.736, Accuracy: 69.944%\n",
            "Epoch 006: Loss: 1.736, Accuracy: 69.937%\n",
            "Epoch 006: Loss: 1.736, Accuracy: 69.931%\n",
            "Epoch 006: Loss: 1.737, Accuracy: 69.872%\n",
            "Epoch 006: Loss: 1.733, Accuracy: 69.935%\n",
            "Epoch 006: Loss: 1.732, Accuracy: 70.014%\n",
            "Epoch 006: Loss: 1.733, Accuracy: 69.973%\n",
            "Epoch 006: Loss: 1.734, Accuracy: 69.949%\n",
            "Epoch 006: Loss: 1.736, Accuracy: 69.909%\n",
            "Epoch 006: Loss: 1.737, Accuracy: 69.870%\n",
            "Epoch 006: Loss: 1.733, Accuracy: 69.963%\n",
            "Epoch 006: Loss: 1.734, Accuracy: 69.974%\n",
            "Epoch 006: Loss: 1.734, Accuracy: 70.000%\n",
            "Epoch 006: Loss: 1.734, Accuracy: 69.993%\n",
            "Epoch 006: Loss: 1.734, Accuracy: 69.971%\n",
            "Epoch 006: Loss: 1.735, Accuracy: 69.948%\n",
            "Epoch 006: Loss: 1.735, Accuracy: 69.958%\n",
            "Epoch 006: Loss: 1.737, Accuracy: 69.856%\n",
            "Epoch 006: Loss: 1.736, Accuracy: 69.866%\n",
            "Epoch 006: Loss: 1.736, Accuracy: 69.876%\n",
            "Epoch 006: Loss: 1.735, Accuracy: 69.918%\n",
            "Epoch 006: Loss: 1.734, Accuracy: 69.928%\n",
            "Epoch 006: Loss: 1.734, Accuracy: 69.969%\n",
            "Epoch 006: Loss: 1.732, Accuracy: 70.025%\n",
            "Epoch 006: Loss: 1.734, Accuracy: 69.957%\n",
            "Epoch 006: Loss: 1.733, Accuracy: 69.982%\n",
            "Epoch 006: Loss: 1.732, Accuracy: 70.021%\n",
            "Epoch 006: Loss: 1.734, Accuracy: 69.985%\n",
            "Epoch 006: Loss: 1.734, Accuracy: 69.994%\n",
            "Epoch 006: Loss: 1.732, Accuracy: 70.048%\n",
            "Epoch 006: Loss: 1.732, Accuracy: 70.072%\n",
            "Epoch 006: Loss: 1.730, Accuracy: 70.081%\n",
            "Epoch 006: Loss: 1.731, Accuracy: 70.089%\n",
            "Epoch 006: Loss: 1.731, Accuracy: 70.083%\n",
            "Epoch 006: Loss: 1.730, Accuracy: 70.077%\n",
            "Epoch 006: Loss: 1.731, Accuracy: 70.100%\n",
            "Epoch 006: Loss: 1.734, Accuracy: 70.050%\n",
            "Epoch 006: Loss: 1.735, Accuracy: 70.015%\n",
            "Epoch 006: Loss: 1.735, Accuracy: 70.038%\n",
            "Epoch 006: Loss: 1.734, Accuracy: 70.060%\n",
            "Epoch 006: Loss: 1.734, Accuracy: 70.083%\n",
            "Epoch 006: Loss: 1.734, Accuracy: 70.091%\n",
            "Epoch 006: Loss: 1.732, Accuracy: 70.170%\n",
            "Epoch 006: Loss: 1.731, Accuracy: 70.206%\n",
            "Epoch 006: Loss: 1.731, Accuracy: 70.186%\n",
            "Epoch 006: Loss: 1.730, Accuracy: 70.207%\n",
            "Epoch 006: Loss: 1.730, Accuracy: 70.229%\n",
            "Epoch 006: Loss: 1.728, Accuracy: 70.292%\n",
            "Epoch 006: Loss: 1.727, Accuracy: 70.285%\n",
            "Epoch 006: Loss: 1.726, Accuracy: 70.306%\n",
            "Epoch 006: Loss: 1.725, Accuracy: 70.326%\n",
            "Epoch 006: Loss: 1.725, Accuracy: 70.292%\n",
            "Epoch 006: Loss: 1.726, Accuracy: 70.272%\n",
            "Epoch 006: Loss: 1.726, Accuracy: 70.265%\n",
            "Epoch 006: Loss: 1.724, Accuracy: 70.326%\n",
            "Epoch 006: Loss: 1.722, Accuracy: 70.333%\n",
            "Epoch 006: Loss: 1.721, Accuracy: 70.339%\n",
            "Epoch 006: Loss: 1.722, Accuracy: 70.306%\n",
            "Epoch 006: Loss: 1.721, Accuracy: 70.339%\n",
            "Epoch 006: Loss: 1.720, Accuracy: 70.345%\n",
            "Epoch 006: Loss: 1.721, Accuracy: 70.326%\n",
            "Epoch 006: Loss: 1.722, Accuracy: 70.280%\n",
            "Epoch 006: Loss: 1.722, Accuracy: 70.286%\n",
            "Epoch 006: Loss: 1.723, Accuracy: 70.254%\n",
            "Epoch 006: Loss: 1.722, Accuracy: 70.248%\n",
            "Epoch 006: Loss: 1.722, Accuracy: 70.280%\n",
            "Epoch 006: Loss: 1.723, Accuracy: 70.261%\n",
            "Epoch 006: Loss: 1.724, Accuracy: 70.242%\n",
            "Epoch 006: Loss: 1.722, Accuracy: 70.249%\n",
            "Epoch 006: Loss: 1.721, Accuracy: 70.268%\n",
            "Epoch 006: Loss: 1.720, Accuracy: 70.287%\n",
            "Epoch 006: Loss: 1.722, Accuracy: 70.256%\n",
            "Epoch 006: Loss: 1.721, Accuracy: 70.288%\n",
            "Epoch 006: Loss: 1.720, Accuracy: 70.306%\n",
            "Epoch 006: Loss: 1.719, Accuracy: 70.337%\n",
            "Epoch 006: Loss: 1.720, Accuracy: 70.282%\n",
            "Epoch 006: Loss: 1.721, Accuracy: 70.251%\n",
            "Epoch 006: Loss: 1.721, Accuracy: 70.257%\n",
            "Epoch 006: Loss: 1.722, Accuracy: 70.264%\n",
            "Epoch 006: Loss: 1.722, Accuracy: 70.270%\n",
            "Epoch 006: Loss: 1.721, Accuracy: 70.288%\n",
            "Epoch 006: Loss: 1.722, Accuracy: 70.306%\n",
            "Epoch 006: Loss: 1.722, Accuracy: 70.300%\n",
            "Epoch 006: Loss: 1.722, Accuracy: 70.318%\n",
            "Epoch 006: Loss: 1.722, Accuracy: 70.289%\n",
            "Epoch 006: Loss: 1.723, Accuracy: 70.259%\n",
            "Epoch 006: Loss: 1.723, Accuracy: 70.277%\n",
            "Epoch 006: Loss: 1.723, Accuracy: 70.283%\n",
            "Epoch 006: Loss: 1.723, Accuracy: 70.301%\n",
            "Epoch 006: Loss: 1.724, Accuracy: 70.295%\n",
            "Epoch 006: Loss: 1.723, Accuracy: 70.312%\n",
            "Epoch 006: Loss: 1.724, Accuracy: 70.307%\n",
            "Epoch 006: Loss: 1.726, Accuracy: 70.231%\n",
            "Epoch 006: Loss: 1.725, Accuracy: 70.284%\n",
            "Epoch 006: Loss: 1.725, Accuracy: 70.290%\n",
            "Epoch 006: Loss: 1.725, Accuracy: 70.295%\n",
            "Epoch 006: Loss: 1.724, Accuracy: 70.301%\n",
            "Epoch 006: Loss: 1.724, Accuracy: 70.307%\n",
            "Epoch 006: Loss: 1.722, Accuracy: 70.346%\n",
            "Epoch 006: Loss: 1.724, Accuracy: 70.296%\n",
            "Epoch 006: Loss: 1.725, Accuracy: 70.279%\n",
            "Epoch 006: Loss: 1.725, Accuracy: 70.307%\n",
            "Epoch 006: Loss: 1.725, Accuracy: 70.301%\n",
            "Epoch 006: Loss: 1.726, Accuracy: 70.318%\n",
            "Epoch 007: Loss: 1.037, Accuracy: 87.500%\n",
            "Epoch 007: Loss: 1.122, Accuracy: 82.812%\n",
            "Epoch 007: Loss: 1.328, Accuracy: 77.083%\n",
            "Epoch 007: Loss: 1.379, Accuracy: 78.125%\n",
            "Epoch 007: Loss: 1.381, Accuracy: 78.750%\n",
            "Epoch 007: Loss: 1.413, Accuracy: 77.083%\n",
            "Epoch 007: Loss: 1.532, Accuracy: 74.554%\n",
            "Epoch 007: Loss: 1.494, Accuracy: 75.781%\n",
            "Epoch 007: Loss: 1.620, Accuracy: 72.569%\n",
            "Epoch 007: Loss: 1.589, Accuracy: 72.812%\n",
            "Epoch 007: Loss: 1.560, Accuracy: 73.580%\n",
            "Epoch 007: Loss: 1.566, Accuracy: 73.177%\n",
            "Epoch 007: Loss: 1.599, Accuracy: 72.596%\n",
            "Epoch 007: Loss: 1.602, Accuracy: 72.321%\n",
            "Epoch 007: Loss: 1.620, Accuracy: 72.292%\n",
            "Epoch 007: Loss: 1.639, Accuracy: 71.484%\n",
            "Epoch 007: Loss: 1.652, Accuracy: 70.956%\n",
            "Epoch 007: Loss: 1.650, Accuracy: 71.007%\n",
            "Epoch 007: Loss: 1.652, Accuracy: 70.888%\n",
            "Epoch 007: Loss: 1.645, Accuracy: 70.938%\n",
            "Epoch 007: Loss: 1.645, Accuracy: 70.833%\n",
            "Epoch 007: Loss: 1.633, Accuracy: 71.023%\n",
            "Epoch 007: Loss: 1.658, Accuracy: 70.245%\n",
            "Epoch 007: Loss: 1.661, Accuracy: 70.182%\n",
            "Epoch 007: Loss: 1.642, Accuracy: 70.500%\n",
            "Epoch 007: Loss: 1.652, Accuracy: 70.072%\n",
            "Epoch 007: Loss: 1.671, Accuracy: 69.097%\n",
            "Epoch 007: Loss: 1.679, Accuracy: 69.085%\n",
            "Epoch 007: Loss: 1.674, Accuracy: 69.181%\n",
            "Epoch 007: Loss: 1.674, Accuracy: 69.479%\n",
            "Epoch 007: Loss: 1.659, Accuracy: 69.758%\n",
            "Epoch 007: Loss: 1.658, Accuracy: 69.727%\n",
            "Epoch 007: Loss: 1.670, Accuracy: 69.508%\n",
            "Epoch 007: Loss: 1.682, Accuracy: 69.393%\n",
            "Epoch 007: Loss: 1.703, Accuracy: 69.107%\n",
            "Epoch 007: Loss: 1.696, Accuracy: 69.358%\n",
            "Epoch 007: Loss: 1.695, Accuracy: 69.341%\n",
            "Epoch 007: Loss: 1.697, Accuracy: 69.243%\n",
            "Epoch 007: Loss: 1.698, Accuracy: 69.391%\n",
            "Epoch 007: Loss: 1.685, Accuracy: 69.766%\n",
            "Epoch 007: Loss: 1.685, Accuracy: 69.741%\n",
            "Epoch 007: Loss: 1.688, Accuracy: 69.643%\n",
            "Epoch 007: Loss: 1.684, Accuracy: 69.840%\n",
            "Epoch 007: Loss: 1.696, Accuracy: 69.318%\n",
            "Epoch 007: Loss: 1.694, Accuracy: 69.236%\n",
            "Epoch 007: Loss: 1.704, Accuracy: 68.954%\n",
            "Epoch 007: Loss: 1.697, Accuracy: 69.215%\n",
            "Epoch 007: Loss: 1.702, Accuracy: 69.076%\n",
            "Epoch 007: Loss: 1.698, Accuracy: 69.260%\n",
            "Epoch 007: Loss: 1.697, Accuracy: 69.250%\n",
            "Epoch 007: Loss: 1.696, Accuracy: 69.240%\n",
            "Epoch 007: Loss: 1.696, Accuracy: 69.351%\n",
            "Epoch 007: Loss: 1.703, Accuracy: 69.222%\n",
            "Epoch 007: Loss: 1.706, Accuracy: 69.213%\n",
            "Epoch 007: Loss: 1.704, Accuracy: 69.375%\n",
            "Epoch 007: Loss: 1.707, Accuracy: 69.308%\n",
            "Epoch 007: Loss: 1.710, Accuracy: 69.298%\n",
            "Epoch 007: Loss: 1.714, Accuracy: 69.181%\n",
            "Epoch 007: Loss: 1.722, Accuracy: 68.909%\n",
            "Epoch 007: Loss: 1.712, Accuracy: 69.167%\n",
            "Epoch 007: Loss: 1.706, Accuracy: 69.314%\n",
            "Epoch 007: Loss: 1.701, Accuracy: 69.405%\n",
            "Epoch 007: Loss: 1.707, Accuracy: 69.296%\n",
            "Epoch 007: Loss: 1.711, Accuracy: 69.287%\n",
            "Epoch 007: Loss: 1.722, Accuracy: 69.087%\n",
            "Epoch 007: Loss: 1.721, Accuracy: 69.223%\n",
            "Epoch 007: Loss: 1.715, Accuracy: 69.356%\n",
            "Epoch 007: Loss: 1.712, Accuracy: 69.531%\n",
            "Epoch 007: Loss: 1.714, Accuracy: 69.429%\n",
            "Epoch 007: Loss: 1.713, Accuracy: 69.464%\n",
            "Epoch 007: Loss: 1.722, Accuracy: 69.322%\n",
            "Epoch 007: Loss: 1.723, Accuracy: 69.358%\n",
            "Epoch 007: Loss: 1.721, Accuracy: 69.349%\n",
            "Epoch 007: Loss: 1.730, Accuracy: 69.257%\n",
            "Epoch 007: Loss: 1.721, Accuracy: 69.458%\n",
            "Epoch 007: Loss: 1.720, Accuracy: 69.449%\n",
            "Epoch 007: Loss: 1.720, Accuracy: 69.440%\n",
            "Epoch 007: Loss: 1.720, Accuracy: 69.271%\n",
            "Epoch 007: Loss: 1.720, Accuracy: 69.146%\n",
            "Epoch 007: Loss: 1.722, Accuracy: 69.141%\n",
            "Epoch 007: Loss: 1.720, Accuracy: 69.174%\n",
            "Epoch 007: Loss: 1.718, Accuracy: 69.207%\n",
            "Epoch 007: Loss: 1.715, Accuracy: 69.239%\n",
            "Epoch 007: Loss: 1.717, Accuracy: 69.271%\n",
            "Epoch 007: Loss: 1.722, Accuracy: 69.228%\n",
            "Epoch 007: Loss: 1.716, Accuracy: 69.331%\n",
            "Epoch 007: Loss: 1.710, Accuracy: 69.432%\n",
            "Epoch 007: Loss: 1.709, Accuracy: 69.496%\n",
            "Epoch 007: Loss: 1.712, Accuracy: 69.417%\n",
            "Epoch 007: Loss: 1.713, Accuracy: 69.375%\n",
            "Epoch 007: Loss: 1.715, Accuracy: 69.368%\n",
            "Epoch 007: Loss: 1.719, Accuracy: 69.361%\n",
            "Epoch 007: Loss: 1.715, Accuracy: 69.422%\n",
            "Epoch 007: Loss: 1.723, Accuracy: 69.315%\n",
            "Epoch 007: Loss: 1.720, Accuracy: 69.309%\n",
            "Epoch 007: Loss: 1.724, Accuracy: 69.303%\n",
            "Epoch 007: Loss: 1.727, Accuracy: 69.265%\n",
            "Epoch 007: Loss: 1.727, Accuracy: 69.260%\n",
            "Epoch 007: Loss: 1.723, Accuracy: 69.287%\n",
            "Epoch 007: Loss: 1.728, Accuracy: 69.156%\n",
            "Epoch 007: Loss: 1.726, Accuracy: 69.214%\n",
            "Epoch 007: Loss: 1.727, Accuracy: 69.148%\n",
            "Epoch 007: Loss: 1.731, Accuracy: 69.053%\n",
            "Epoch 007: Loss: 1.731, Accuracy: 69.111%\n",
            "Epoch 007: Loss: 1.730, Accuracy: 69.196%\n",
            "Epoch 007: Loss: 1.730, Accuracy: 69.192%\n",
            "Epoch 007: Loss: 1.729, Accuracy: 69.188%\n",
            "Epoch 007: Loss: 1.728, Accuracy: 69.126%\n",
            "Epoch 007: Loss: 1.728, Accuracy: 69.123%\n",
            "Epoch 007: Loss: 1.727, Accuracy: 69.148%\n",
            "Epoch 007: Loss: 1.722, Accuracy: 69.257%\n",
            "Epoch 007: Loss: 1.720, Accuracy: 69.308%\n",
            "Epoch 007: Loss: 1.719, Accuracy: 69.303%\n",
            "Epoch 007: Loss: 1.722, Accuracy: 69.243%\n",
            "Epoch 007: Loss: 1.723, Accuracy: 69.266%\n",
            "Epoch 007: Loss: 1.724, Accuracy: 69.262%\n",
            "Epoch 007: Loss: 1.726, Accuracy: 69.177%\n",
            "Epoch 007: Loss: 1.725, Accuracy: 69.200%\n",
            "Epoch 007: Loss: 1.730, Accuracy: 69.091%\n",
            "Epoch 007: Loss: 1.730, Accuracy: 69.115%\n",
            "Epoch 007: Loss: 1.728, Accuracy: 69.163%\n",
            "Epoch 007: Loss: 1.729, Accuracy: 69.160%\n",
            "Epoch 007: Loss: 1.730, Accuracy: 69.106%\n",
            "Epoch 007: Loss: 1.729, Accuracy: 69.128%\n",
            "Epoch 007: Loss: 1.735, Accuracy: 69.050%\n",
            "Epoch 007: Loss: 1.733, Accuracy: 69.072%\n",
            "Epoch 007: Loss: 1.735, Accuracy: 69.045%\n",
            "Epoch 007: Loss: 1.736, Accuracy: 68.994%\n",
            "Epoch 007: Loss: 1.734, Accuracy: 69.041%\n",
            "Epoch 007: Loss: 1.731, Accuracy: 69.159%\n",
            "Epoch 007: Loss: 1.733, Accuracy: 69.132%\n",
            "Epoch 007: Loss: 1.730, Accuracy: 69.223%\n",
            "Epoch 007: Loss: 1.728, Accuracy: 69.267%\n",
            "Epoch 007: Loss: 1.727, Accuracy: 69.286%\n",
            "Epoch 007: Loss: 1.730, Accuracy: 69.259%\n",
            "Epoch 007: Loss: 1.728, Accuracy: 69.301%\n",
            "Epoch 007: Loss: 1.726, Accuracy: 69.389%\n",
            "Epoch 007: Loss: 1.723, Accuracy: 69.497%\n",
            "Epoch 007: Loss: 1.721, Accuracy: 69.537%\n",
            "Epoch 007: Loss: 1.721, Accuracy: 69.531%\n",
            "Epoch 007: Loss: 1.720, Accuracy: 69.592%\n",
            "Epoch 007: Loss: 1.716, Accuracy: 69.718%\n",
            "Epoch 007: Loss: 1.714, Accuracy: 69.755%\n",
            "Epoch 007: Loss: 1.713, Accuracy: 69.770%\n",
            "Epoch 007: Loss: 1.714, Accuracy: 69.720%\n",
            "Epoch 007: Loss: 1.715, Accuracy: 69.713%\n",
            "Epoch 007: Loss: 1.714, Accuracy: 69.685%\n",
            "Epoch 007: Loss: 1.716, Accuracy: 69.595%\n",
            "Epoch 007: Loss: 1.718, Accuracy: 69.589%\n",
            "Epoch 007: Loss: 1.715, Accuracy: 69.667%\n",
            "Epoch 007: Loss: 1.715, Accuracy: 69.723%\n",
            "Epoch 007: Loss: 1.716, Accuracy: 69.737%\n",
            "Epoch 007: Loss: 1.715, Accuracy: 69.751%\n",
            "Epoch 007: Loss: 1.713, Accuracy: 69.825%\n",
            "Epoch 007: Loss: 1.712, Accuracy: 69.859%\n",
            "Epoch 007: Loss: 1.714, Accuracy: 69.852%\n",
            "Epoch 007: Loss: 1.712, Accuracy: 69.885%\n",
            "Epoch 007: Loss: 1.715, Accuracy: 69.838%\n",
            "Epoch 007: Loss: 1.715, Accuracy: 69.811%\n",
            "Epoch 007: Loss: 1.717, Accuracy: 69.707%\n",
            "Epoch 007: Loss: 1.718, Accuracy: 69.682%\n",
            "Epoch 007: Loss: 1.717, Accuracy: 69.695%\n",
            "Epoch 007: Loss: 1.714, Accuracy: 69.747%\n",
            "Epoch 007: Loss: 1.716, Accuracy: 69.722%\n",
            "Epoch 007: Loss: 1.715, Accuracy: 69.811%\n",
            "Epoch 007: Loss: 1.716, Accuracy: 69.804%\n",
            "Epoch 007: Loss: 1.718, Accuracy: 69.760%\n",
            "Epoch 007: Loss: 1.720, Accuracy: 69.661%\n",
            "Epoch 007: Loss: 1.720, Accuracy: 69.638%\n",
            "Epoch 007: Loss: 1.719, Accuracy: 69.596%\n",
            "Epoch 007: Loss: 1.719, Accuracy: 69.591%\n",
            "Epoch 007: Loss: 1.715, Accuracy: 69.695%\n",
            "Epoch 007: Loss: 1.712, Accuracy: 69.816%\n",
            "Epoch 007: Loss: 1.709, Accuracy: 69.881%\n",
            "Epoch 007: Loss: 1.705, Accuracy: 69.964%\n",
            "Epoch 007: Loss: 1.706, Accuracy: 69.993%\n",
            "Epoch 007: Loss: 1.704, Accuracy: 70.021%\n",
            "Epoch 007: Loss: 1.704, Accuracy: 70.032%\n",
            "Epoch 007: Loss: 1.705, Accuracy: 70.042%\n",
            "Epoch 007: Loss: 1.705, Accuracy: 70.035%\n",
            "Epoch 007: Loss: 1.706, Accuracy: 69.976%\n",
            "Epoch 007: Loss: 1.703, Accuracy: 70.055%\n",
            "Epoch 007: Loss: 1.702, Accuracy: 70.116%\n",
            "Epoch 007: Loss: 1.703, Accuracy: 70.075%\n",
            "Epoch 007: Loss: 1.704, Accuracy: 70.051%\n",
            "Epoch 007: Loss: 1.706, Accuracy: 69.976%\n",
            "Epoch 007: Loss: 1.707, Accuracy: 69.987%\n",
            "Epoch 007: Loss: 1.703, Accuracy: 70.063%\n",
            "Epoch 007: Loss: 1.702, Accuracy: 70.040%\n",
            "Epoch 007: Loss: 1.701, Accuracy: 70.066%\n",
            "Epoch 007: Loss: 1.700, Accuracy: 70.075%\n",
            "Epoch 007: Loss: 1.700, Accuracy: 70.052%\n",
            "Epoch 007: Loss: 1.702, Accuracy: 69.997%\n",
            "Epoch 007: Loss: 1.703, Accuracy: 70.006%\n",
            "Epoch 007: Loss: 1.704, Accuracy: 69.936%\n",
            "Epoch 007: Loss: 1.704, Accuracy: 69.946%\n",
            "Epoch 007: Loss: 1.703, Accuracy: 69.971%\n",
            "Epoch 007: Loss: 1.703, Accuracy: 69.997%\n",
            "Epoch 007: Loss: 1.702, Accuracy: 70.022%\n",
            "Epoch 007: Loss: 1.701, Accuracy: 70.063%\n",
            "Epoch 007: Loss: 1.700, Accuracy: 70.072%\n",
            "Epoch 007: Loss: 1.702, Accuracy: 70.003%\n",
            "Epoch 007: Loss: 1.700, Accuracy: 70.028%\n",
            "Epoch 007: Loss: 1.699, Accuracy: 70.052%\n",
            "Epoch 007: Loss: 1.700, Accuracy: 70.015%\n",
            "Epoch 007: Loss: 1.701, Accuracy: 70.009%\n",
            "Epoch 007: Loss: 1.699, Accuracy: 70.063%\n",
            "Epoch 007: Loss: 1.699, Accuracy: 70.072%\n",
            "Epoch 007: Loss: 1.698, Accuracy: 70.111%\n",
            "Epoch 007: Loss: 1.698, Accuracy: 70.149%\n",
            "Epoch 007: Loss: 1.698, Accuracy: 70.142%\n",
            "Epoch 007: Loss: 1.697, Accuracy: 70.150%\n",
            "Epoch 007: Loss: 1.698, Accuracy: 70.100%\n",
            "Epoch 007: Loss: 1.700, Accuracy: 70.035%\n",
            "Epoch 007: Loss: 1.701, Accuracy: 70.015%\n",
            "Epoch 007: Loss: 1.701, Accuracy: 70.009%\n",
            "Epoch 007: Loss: 1.700, Accuracy: 70.032%\n",
            "Epoch 007: Loss: 1.699, Accuracy: 70.026%\n",
            "Epoch 007: Loss: 1.700, Accuracy: 70.063%\n",
            "Epoch 007: Loss: 1.697, Accuracy: 70.114%\n",
            "Epoch 007: Loss: 1.696, Accuracy: 70.107%\n",
            "Epoch 007: Loss: 1.695, Accuracy: 70.101%\n",
            "Epoch 007: Loss: 1.696, Accuracy: 70.151%\n",
            "Epoch 007: Loss: 1.695, Accuracy: 70.145%\n",
            "Epoch 007: Loss: 1.693, Accuracy: 70.181%\n",
            "Epoch 007: Loss: 1.692, Accuracy: 70.174%\n",
            "Epoch 007: Loss: 1.691, Accuracy: 70.195%\n",
            "Epoch 007: Loss: 1.690, Accuracy: 70.230%\n",
            "Epoch 007: Loss: 1.691, Accuracy: 70.210%\n",
            "Epoch 007: Loss: 1.692, Accuracy: 70.204%\n",
            "Epoch 007: Loss: 1.693, Accuracy: 70.184%\n",
            "Epoch 007: Loss: 1.692, Accuracy: 70.232%\n",
            "Epoch 007: Loss: 1.692, Accuracy: 70.252%\n",
            "Epoch 007: Loss: 1.691, Accuracy: 70.299%\n",
            "Epoch 007: Loss: 1.692, Accuracy: 70.306%\n",
            "Epoch 007: Loss: 1.691, Accuracy: 70.339%\n",
            "Epoch 007: Loss: 1.690, Accuracy: 70.372%\n",
            "Epoch 007: Loss: 1.690, Accuracy: 70.365%\n",
            "Epoch 007: Loss: 1.691, Accuracy: 70.332%\n",
            "Epoch 007: Loss: 1.691, Accuracy: 70.326%\n",
            "Epoch 007: Loss: 1.691, Accuracy: 70.267%\n",
            "Epoch 007: Loss: 1.691, Accuracy: 70.274%\n",
            "Epoch 007: Loss: 1.691, Accuracy: 70.293%\n",
            "Epoch 007: Loss: 1.691, Accuracy: 70.274%\n",
            "Epoch 007: Loss: 1.692, Accuracy: 70.268%\n",
            "Epoch 007: Loss: 1.691, Accuracy: 70.300%\n",
            "Epoch 007: Loss: 1.689, Accuracy: 70.306%\n",
            "Epoch 007: Loss: 1.688, Accuracy: 70.338%\n",
            "Epoch 007: Loss: 1.689, Accuracy: 70.319%\n",
            "Epoch 007: Loss: 1.688, Accuracy: 70.375%\n",
            "Epoch 007: Loss: 1.687, Accuracy: 70.418%\n",
            "Epoch 007: Loss: 1.687, Accuracy: 70.461%\n",
            "Epoch 007: Loss: 1.688, Accuracy: 70.417%\n",
            "Epoch 007: Loss: 1.689, Accuracy: 70.386%\n",
            "Epoch 007: Loss: 1.689, Accuracy: 70.392%\n",
            "Epoch 007: Loss: 1.689, Accuracy: 70.398%\n",
            "Epoch 007: Loss: 1.689, Accuracy: 70.404%\n",
            "Epoch 007: Loss: 1.690, Accuracy: 70.422%\n",
            "Epoch 007: Loss: 1.691, Accuracy: 70.391%\n",
            "Epoch 007: Loss: 1.690, Accuracy: 70.385%\n",
            "Epoch 007: Loss: 1.689, Accuracy: 70.402%\n",
            "Epoch 007: Loss: 1.690, Accuracy: 70.372%\n",
            "Epoch 007: Loss: 1.692, Accuracy: 70.354%\n",
            "Epoch 007: Loss: 1.692, Accuracy: 70.348%\n",
            "Epoch 007: Loss: 1.693, Accuracy: 70.342%\n",
            "Epoch 007: Loss: 1.693, Accuracy: 70.336%\n",
            "Epoch 007: Loss: 1.694, Accuracy: 70.330%\n",
            "Epoch 007: Loss: 1.693, Accuracy: 70.347%\n",
            "Epoch 007: Loss: 1.694, Accuracy: 70.330%\n",
            "Epoch 007: Loss: 1.696, Accuracy: 70.255%\n",
            "Epoch 007: Loss: 1.694, Accuracy: 70.284%\n",
            "Epoch 007: Loss: 1.694, Accuracy: 70.290%\n",
            "Epoch 007: Loss: 1.694, Accuracy: 70.284%\n",
            "Epoch 007: Loss: 1.694, Accuracy: 70.290%\n",
            "Epoch 007: Loss: 1.694, Accuracy: 70.284%\n",
            "Epoch 007: Loss: 1.692, Accuracy: 70.324%\n",
            "Epoch 007: Loss: 1.693, Accuracy: 70.273%\n",
            "Epoch 007: Loss: 1.694, Accuracy: 70.268%\n",
            "Epoch 007: Loss: 1.695, Accuracy: 70.284%\n",
            "Epoch 007: Loss: 1.695, Accuracy: 70.279%\n",
            "Epoch 007: Loss: 1.695, Accuracy: 70.285%\n",
            "Epoch 008: Loss: 1.156, Accuracy: 90.625%\n",
            "Epoch 008: Loss: 1.243, Accuracy: 87.500%\n",
            "Epoch 008: Loss: 1.451, Accuracy: 81.250%\n",
            "Epoch 008: Loss: 1.494, Accuracy: 82.031%\n",
            "Epoch 008: Loss: 1.461, Accuracy: 81.875%\n",
            "Epoch 008: Loss: 1.463, Accuracy: 80.729%\n",
            "Epoch 008: Loss: 1.567, Accuracy: 78.571%\n",
            "Epoch 008: Loss: 1.509, Accuracy: 79.688%\n",
            "Epoch 008: Loss: 1.621, Accuracy: 75.694%\n",
            "Epoch 008: Loss: 1.598, Accuracy: 75.625%\n",
            "Epoch 008: Loss: 1.565, Accuracy: 75.852%\n",
            "Epoch 008: Loss: 1.573, Accuracy: 75.521%\n",
            "Epoch 008: Loss: 1.583, Accuracy: 74.760%\n",
            "Epoch 008: Loss: 1.582, Accuracy: 74.330%\n",
            "Epoch 008: Loss: 1.599, Accuracy: 74.167%\n",
            "Epoch 008: Loss: 1.636, Accuracy: 73.047%\n",
            "Epoch 008: Loss: 1.652, Accuracy: 72.610%\n",
            "Epoch 008: Loss: 1.649, Accuracy: 72.569%\n",
            "Epoch 008: Loss: 1.651, Accuracy: 72.697%\n",
            "Epoch 008: Loss: 1.646, Accuracy: 72.656%\n",
            "Epoch 008: Loss: 1.648, Accuracy: 72.768%\n",
            "Epoch 008: Loss: 1.644, Accuracy: 72.869%\n",
            "Epoch 008: Loss: 1.669, Accuracy: 72.011%\n",
            "Epoch 008: Loss: 1.676, Accuracy: 72.005%\n",
            "Epoch 008: Loss: 1.658, Accuracy: 72.250%\n",
            "Epoch 008: Loss: 1.672, Accuracy: 71.755%\n",
            "Epoch 008: Loss: 1.693, Accuracy: 71.065%\n",
            "Epoch 008: Loss: 1.699, Accuracy: 70.536%\n",
            "Epoch 008: Loss: 1.697, Accuracy: 70.582%\n",
            "Epoch 008: Loss: 1.691, Accuracy: 70.938%\n",
            "Epoch 008: Loss: 1.676, Accuracy: 71.270%\n",
            "Epoch 008: Loss: 1.680, Accuracy: 70.898%\n",
            "Epoch 008: Loss: 1.697, Accuracy: 70.739%\n",
            "Epoch 008: Loss: 1.705, Accuracy: 70.588%\n",
            "Epoch 008: Loss: 1.719, Accuracy: 70.357%\n",
            "Epoch 008: Loss: 1.714, Accuracy: 70.399%\n",
            "Epoch 008: Loss: 1.712, Accuracy: 70.355%\n",
            "Epoch 008: Loss: 1.714, Accuracy: 70.477%\n",
            "Epoch 008: Loss: 1.717, Accuracy: 70.593%\n",
            "Epoch 008: Loss: 1.703, Accuracy: 71.016%\n",
            "Epoch 008: Loss: 1.706, Accuracy: 70.884%\n",
            "Epoch 008: Loss: 1.710, Accuracy: 70.833%\n",
            "Epoch 008: Loss: 1.705, Accuracy: 71.076%\n",
            "Epoch 008: Loss: 1.718, Accuracy: 70.597%\n",
            "Epoch 008: Loss: 1.713, Accuracy: 70.694%\n",
            "Epoch 008: Loss: 1.720, Accuracy: 70.380%\n",
            "Epoch 008: Loss: 1.713, Accuracy: 70.678%\n",
            "Epoch 008: Loss: 1.712, Accuracy: 70.703%\n",
            "Epoch 008: Loss: 1.711, Accuracy: 70.791%\n",
            "Epoch 008: Loss: 1.712, Accuracy: 70.812%\n",
            "Epoch 008: Loss: 1.712, Accuracy: 70.895%\n",
            "Epoch 008: Loss: 1.708, Accuracy: 71.154%\n",
            "Epoch 008: Loss: 1.715, Accuracy: 70.991%\n",
            "Epoch 008: Loss: 1.714, Accuracy: 71.007%\n",
            "Epoch 008: Loss: 1.710, Accuracy: 71.136%\n",
            "Epoch 008: Loss: 1.709, Accuracy: 70.982%\n",
            "Epoch 008: Loss: 1.706, Accuracy: 70.943%\n",
            "Epoch 008: Loss: 1.711, Accuracy: 70.905%\n",
            "Epoch 008: Loss: 1.721, Accuracy: 70.604%\n",
            "Epoch 008: Loss: 1.716, Accuracy: 70.781%\n",
            "Epoch 008: Loss: 1.710, Accuracy: 70.953%\n",
            "Epoch 008: Loss: 1.711, Accuracy: 71.069%\n",
            "Epoch 008: Loss: 1.719, Accuracy: 70.933%\n",
            "Epoch 008: Loss: 1.722, Accuracy: 70.850%\n",
            "Epoch 008: Loss: 1.730, Accuracy: 70.625%\n",
            "Epoch 008: Loss: 1.730, Accuracy: 70.691%\n",
            "Epoch 008: Loss: 1.723, Accuracy: 70.849%\n",
            "Epoch 008: Loss: 1.720, Accuracy: 70.956%\n",
            "Epoch 008: Loss: 1.721, Accuracy: 70.969%\n",
            "Epoch 008: Loss: 1.721, Accuracy: 71.027%\n",
            "Epoch 008: Loss: 1.725, Accuracy: 70.863%\n",
            "Epoch 008: Loss: 1.726, Accuracy: 70.920%\n",
            "Epoch 008: Loss: 1.723, Accuracy: 70.976%\n",
            "Epoch 008: Loss: 1.732, Accuracy: 70.904%\n",
            "Epoch 008: Loss: 1.722, Accuracy: 71.083%\n",
            "Epoch 008: Loss: 1.722, Accuracy: 71.012%\n",
            "Epoch 008: Loss: 1.722, Accuracy: 71.023%\n",
            "Epoch 008: Loss: 1.723, Accuracy: 70.913%\n",
            "Epoch 008: Loss: 1.723, Accuracy: 70.847%\n",
            "Epoch 008: Loss: 1.725, Accuracy: 70.820%\n",
            "Epoch 008: Loss: 1.723, Accuracy: 70.795%\n",
            "Epoch 008: Loss: 1.720, Accuracy: 70.846%\n",
            "Epoch 008: Loss: 1.716, Accuracy: 70.934%\n",
            "Epoch 008: Loss: 1.720, Accuracy: 70.871%\n",
            "Epoch 008: Loss: 1.726, Accuracy: 70.772%\n",
            "Epoch 008: Loss: 1.722, Accuracy: 70.858%\n",
            "Epoch 008: Loss: 1.717, Accuracy: 70.941%\n",
            "Epoch 008: Loss: 1.716, Accuracy: 71.023%\n",
            "Epoch 008: Loss: 1.718, Accuracy: 70.997%\n",
            "Epoch 008: Loss: 1.717, Accuracy: 70.972%\n",
            "Epoch 008: Loss: 1.717, Accuracy: 70.948%\n",
            "Epoch 008: Loss: 1.720, Accuracy: 70.924%\n",
            "Epoch 008: Loss: 1.717, Accuracy: 71.001%\n",
            "Epoch 008: Loss: 1.723, Accuracy: 70.878%\n",
            "Epoch 008: Loss: 1.722, Accuracy: 70.954%\n",
            "Epoch 008: Loss: 1.726, Accuracy: 70.866%\n",
            "Epoch 008: Loss: 1.729, Accuracy: 70.812%\n",
            "Epoch 008: Loss: 1.729, Accuracy: 70.823%\n",
            "Epoch 008: Loss: 1.725, Accuracy: 70.833%\n",
            "Epoch 008: Loss: 1.729, Accuracy: 70.781%\n",
            "Epoch 008: Loss: 1.727, Accuracy: 70.885%\n",
            "Epoch 008: Loss: 1.728, Accuracy: 70.803%\n",
            "Epoch 008: Loss: 1.733, Accuracy: 70.752%\n",
            "Epoch 008: Loss: 1.732, Accuracy: 70.733%\n",
            "Epoch 008: Loss: 1.730, Accuracy: 70.804%\n",
            "Epoch 008: Loss: 1.730, Accuracy: 70.755%\n",
            "Epoch 008: Loss: 1.728, Accuracy: 70.736%\n",
            "Epoch 008: Loss: 1.728, Accuracy: 70.747%\n",
            "Epoch 008: Loss: 1.728, Accuracy: 70.757%\n",
            "Epoch 008: Loss: 1.725, Accuracy: 70.767%\n",
            "Epoch 008: Loss: 1.721, Accuracy: 70.890%\n",
            "Epoch 008: Loss: 1.719, Accuracy: 70.982%\n",
            "Epoch 008: Loss: 1.718, Accuracy: 71.018%\n",
            "Epoch 008: Loss: 1.722, Accuracy: 70.943%\n",
            "Epoch 008: Loss: 1.723, Accuracy: 70.924%\n",
            "Epoch 008: Loss: 1.722, Accuracy: 70.932%\n",
            "Epoch 008: Loss: 1.726, Accuracy: 70.807%\n",
            "Epoch 008: Loss: 1.727, Accuracy: 70.842%\n",
            "Epoch 008: Loss: 1.733, Accuracy: 70.720%\n",
            "Epoch 008: Loss: 1.732, Accuracy: 70.729%\n",
            "Epoch 008: Loss: 1.732, Accuracy: 70.739%\n",
            "Epoch 008: Loss: 1.731, Accuracy: 70.722%\n",
            "Epoch 008: Loss: 1.734, Accuracy: 70.655%\n",
            "Epoch 008: Loss: 1.733, Accuracy: 70.665%\n",
            "Epoch 008: Loss: 1.739, Accuracy: 70.550%\n",
            "Epoch 008: Loss: 1.738, Accuracy: 70.511%\n",
            "Epoch 008: Loss: 1.738, Accuracy: 70.448%\n",
            "Epoch 008: Loss: 1.740, Accuracy: 70.337%\n",
            "Epoch 008: Loss: 1.737, Accuracy: 70.422%\n",
            "Epoch 008: Loss: 1.734, Accuracy: 70.505%\n",
            "Epoch 008: Loss: 1.738, Accuracy: 70.396%\n",
            "Epoch 008: Loss: 1.734, Accuracy: 70.502%\n",
            "Epoch 008: Loss: 1.733, Accuracy: 70.536%\n",
            "Epoch 008: Loss: 1.732, Accuracy: 70.546%\n",
            "Epoch 008: Loss: 1.736, Accuracy: 70.532%\n",
            "Epoch 008: Loss: 1.735, Accuracy: 70.565%\n",
            "Epoch 008: Loss: 1.733, Accuracy: 70.643%\n",
            "Epoch 008: Loss: 1.729, Accuracy: 70.765%\n",
            "Epoch 008: Loss: 1.729, Accuracy: 70.751%\n",
            "Epoch 008: Loss: 1.729, Accuracy: 70.737%\n",
            "Epoch 008: Loss: 1.728, Accuracy: 70.700%\n",
            "Epoch 008: Loss: 1.724, Accuracy: 70.841%\n",
            "Epoch 008: Loss: 1.722, Accuracy: 70.892%\n",
            "Epoch 008: Loss: 1.721, Accuracy: 70.920%\n",
            "Epoch 008: Loss: 1.722, Accuracy: 70.905%\n",
            "Epoch 008: Loss: 1.722, Accuracy: 70.912%\n",
            "Epoch 008: Loss: 1.723, Accuracy: 70.897%\n",
            "Epoch 008: Loss: 1.724, Accuracy: 70.798%\n",
            "Epoch 008: Loss: 1.726, Accuracy: 70.784%\n",
            "Epoch 008: Loss: 1.723, Accuracy: 70.854%\n",
            "Epoch 008: Loss: 1.722, Accuracy: 70.882%\n",
            "Epoch 008: Loss: 1.722, Accuracy: 70.847%\n",
            "Epoch 008: Loss: 1.722, Accuracy: 70.813%\n",
            "Epoch 008: Loss: 1.721, Accuracy: 70.840%\n",
            "Epoch 008: Loss: 1.720, Accuracy: 70.887%\n",
            "Epoch 008: Loss: 1.721, Accuracy: 70.873%\n",
            "Epoch 008: Loss: 1.719, Accuracy: 70.939%\n",
            "Epoch 008: Loss: 1.722, Accuracy: 70.866%\n",
            "Epoch 008: Loss: 1.722, Accuracy: 70.873%\n",
            "Epoch 008: Loss: 1.725, Accuracy: 70.762%\n",
            "Epoch 008: Loss: 1.727, Accuracy: 70.749%\n",
            "Epoch 008: Loss: 1.727, Accuracy: 70.775%\n",
            "Epoch 008: Loss: 1.725, Accuracy: 70.782%\n",
            "Epoch 008: Loss: 1.726, Accuracy: 70.770%\n",
            "Epoch 008: Loss: 1.727, Accuracy: 70.814%\n",
            "Epoch 008: Loss: 1.727, Accuracy: 70.802%\n",
            "Epoch 008: Loss: 1.729, Accuracy: 70.734%\n",
            "Epoch 008: Loss: 1.732, Accuracy: 70.629%\n",
            "Epoch 008: Loss: 1.731, Accuracy: 70.599%\n",
            "Epoch 008: Loss: 1.731, Accuracy: 70.625%\n",
            "Epoch 008: Loss: 1.731, Accuracy: 70.651%\n",
            "Epoch 008: Loss: 1.729, Accuracy: 70.712%\n",
            "Epoch 008: Loss: 1.726, Accuracy: 70.809%\n",
            "Epoch 008: Loss: 1.723, Accuracy: 70.869%\n",
            "Epoch 008: Loss: 1.720, Accuracy: 70.929%\n",
            "Epoch 008: Loss: 1.720, Accuracy: 70.934%\n",
            "Epoch 008: Loss: 1.719, Accuracy: 70.975%\n",
            "Epoch 008: Loss: 1.719, Accuracy: 70.997%\n",
            "Epoch 008: Loss: 1.720, Accuracy: 71.002%\n",
            "Epoch 008: Loss: 1.719, Accuracy: 70.990%\n",
            "Epoch 008: Loss: 1.721, Accuracy: 70.943%\n",
            "Epoch 008: Loss: 1.718, Accuracy: 71.034%\n",
            "Epoch 008: Loss: 1.716, Accuracy: 71.089%\n",
            "Epoch 008: Loss: 1.716, Accuracy: 71.043%\n",
            "Epoch 008: Loss: 1.718, Accuracy: 71.014%\n",
            "Epoch 008: Loss: 1.720, Accuracy: 70.951%\n",
            "Epoch 008: Loss: 1.720, Accuracy: 70.889%\n",
            "Epoch 008: Loss: 1.717, Accuracy: 70.994%\n",
            "Epoch 008: Loss: 1.716, Accuracy: 71.015%\n",
            "Epoch 008: Loss: 1.714, Accuracy: 71.003%\n",
            "Epoch 008: Loss: 1.714, Accuracy: 70.991%\n",
            "Epoch 008: Loss: 1.714, Accuracy: 70.931%\n",
            "Epoch 008: Loss: 1.714, Accuracy: 70.839%\n",
            "Epoch 008: Loss: 1.714, Accuracy: 70.860%\n",
            "Epoch 008: Loss: 1.717, Accuracy: 70.785%\n",
            "Epoch 008: Loss: 1.715, Accuracy: 70.791%\n",
            "Epoch 008: Loss: 1.715, Accuracy: 70.796%\n",
            "Epoch 008: Loss: 1.713, Accuracy: 70.818%\n",
            "Epoch 008: Loss: 1.712, Accuracy: 70.823%\n",
            "Epoch 008: Loss: 1.711, Accuracy: 70.844%\n",
            "Epoch 008: Loss: 1.710, Accuracy: 70.896%\n",
            "Epoch 008: Loss: 1.712, Accuracy: 70.823%\n",
            "Epoch 008: Loss: 1.711, Accuracy: 70.844%\n",
            "Epoch 008: Loss: 1.710, Accuracy: 70.864%\n",
            "Epoch 008: Loss: 1.710, Accuracy: 70.823%\n",
            "Epoch 008: Loss: 1.711, Accuracy: 70.828%\n",
            "Epoch 008: Loss: 1.708, Accuracy: 70.864%\n",
            "Epoch 008: Loss: 1.708, Accuracy: 70.883%\n",
            "Epoch 008: Loss: 1.706, Accuracy: 70.918%\n",
            "Epoch 008: Loss: 1.705, Accuracy: 70.938%\n",
            "Epoch 008: Loss: 1.705, Accuracy: 70.972%\n",
            "Epoch 008: Loss: 1.704, Accuracy: 70.961%\n",
            "Epoch 008: Loss: 1.704, Accuracy: 70.936%\n",
            "Epoch 008: Loss: 1.707, Accuracy: 70.882%\n",
            "Epoch 008: Loss: 1.708, Accuracy: 70.843%\n",
            "Epoch 008: Loss: 1.708, Accuracy: 70.848%\n",
            "Epoch 008: Loss: 1.707, Accuracy: 70.824%\n",
            "Epoch 008: Loss: 1.706, Accuracy: 70.843%\n",
            "Epoch 008: Loss: 1.706, Accuracy: 70.833%\n",
            "Epoch 008: Loss: 1.704, Accuracy: 70.881%\n",
            "Epoch 008: Loss: 1.703, Accuracy: 70.885%\n",
            "Epoch 008: Loss: 1.703, Accuracy: 70.876%\n",
            "Epoch 008: Loss: 1.702, Accuracy: 70.908%\n",
            "Epoch 008: Loss: 1.702, Accuracy: 70.912%\n",
            "Epoch 008: Loss: 1.700, Accuracy: 70.972%\n",
            "Epoch 008: Loss: 1.699, Accuracy: 70.962%\n",
            "Epoch 008: Loss: 1.698, Accuracy: 70.994%\n",
            "Epoch 008: Loss: 1.697, Accuracy: 71.039%\n",
            "Epoch 008: Loss: 1.698, Accuracy: 70.988%\n",
            "Epoch 008: Loss: 1.699, Accuracy: 70.965%\n",
            "Epoch 008: Loss: 1.699, Accuracy: 70.982%\n",
            "Epoch 008: Loss: 1.696, Accuracy: 71.013%\n",
            "Epoch 008: Loss: 1.695, Accuracy: 71.030%\n",
            "Epoch 008: Loss: 1.693, Accuracy: 71.034%\n",
            "Epoch 008: Loss: 1.695, Accuracy: 70.997%\n",
            "Epoch 008: Loss: 1.694, Accuracy: 71.014%\n",
            "Epoch 008: Loss: 1.693, Accuracy: 71.044%\n",
            "Epoch 008: Loss: 1.695, Accuracy: 71.022%\n",
            "Epoch 008: Loss: 1.696, Accuracy: 70.999%\n",
            "Epoch 008: Loss: 1.695, Accuracy: 70.977%\n",
            "Epoch 008: Loss: 1.695, Accuracy: 70.954%\n",
            "Epoch 008: Loss: 1.695, Accuracy: 70.971%\n",
            "Epoch 008: Loss: 1.695, Accuracy: 70.975%\n",
            "Epoch 008: Loss: 1.695, Accuracy: 70.940%\n",
            "Epoch 008: Loss: 1.695, Accuracy: 70.931%\n",
            "Epoch 008: Loss: 1.694, Accuracy: 70.960%\n",
            "Epoch 008: Loss: 1.693, Accuracy: 70.977%\n",
            "Epoch 008: Loss: 1.692, Accuracy: 70.980%\n",
            "Epoch 008: Loss: 1.693, Accuracy: 70.971%\n",
            "Epoch 008: Loss: 1.691, Accuracy: 71.013%\n",
            "Epoch 008: Loss: 1.690, Accuracy: 71.041%\n",
            "Epoch 008: Loss: 1.689, Accuracy: 71.069%\n",
            "Epoch 008: Loss: 1.690, Accuracy: 71.035%\n",
            "Epoch 008: Loss: 1.691, Accuracy: 71.014%\n",
            "Epoch 008: Loss: 1.691, Accuracy: 71.005%\n",
            "Epoch 008: Loss: 1.691, Accuracy: 71.033%\n",
            "Epoch 008: Loss: 1.691, Accuracy: 71.036%\n",
            "Epoch 008: Loss: 1.692, Accuracy: 71.039%\n",
            "Epoch 008: Loss: 1.693, Accuracy: 71.006%\n",
            "Epoch 008: Loss: 1.693, Accuracy: 70.998%\n",
            "Epoch 008: Loss: 1.692, Accuracy: 71.001%\n",
            "Epoch 008: Loss: 1.692, Accuracy: 70.992%\n",
            "Epoch 008: Loss: 1.695, Accuracy: 70.972%\n",
            "Epoch 008: Loss: 1.695, Accuracy: 70.987%\n",
            "Epoch 008: Loss: 1.695, Accuracy: 70.967%\n",
            "Epoch 008: Loss: 1.695, Accuracy: 70.970%\n",
            "Epoch 008: Loss: 1.696, Accuracy: 70.950%\n",
            "Epoch 008: Loss: 1.695, Accuracy: 70.954%\n",
            "Epoch 008: Loss: 1.696, Accuracy: 70.946%\n",
            "Epoch 008: Loss: 1.698, Accuracy: 70.868%\n",
            "Epoch 008: Loss: 1.697, Accuracy: 70.895%\n",
            "Epoch 008: Loss: 1.697, Accuracy: 70.887%\n",
            "Epoch 008: Loss: 1.697, Accuracy: 70.902%\n",
            "Epoch 008: Loss: 1.697, Accuracy: 70.917%\n",
            "Epoch 008: Loss: 1.697, Accuracy: 70.920%\n",
            "Epoch 008: Loss: 1.696, Accuracy: 70.958%\n",
            "Epoch 008: Loss: 1.697, Accuracy: 70.905%\n",
            "Epoch 008: Loss: 1.698, Accuracy: 70.897%\n",
            "Epoch 008: Loss: 1.697, Accuracy: 70.912%\n",
            "Epoch 008: Loss: 1.698, Accuracy: 70.893%\n",
            "Epoch 008: Loss: 1.698, Accuracy: 70.897%\n",
            "Epoch 009: Loss: 1.089, Accuracy: 84.375%\n",
            "Epoch 009: Loss: 1.188, Accuracy: 81.250%\n",
            "Epoch 009: Loss: 1.414, Accuracy: 76.042%\n",
            "Epoch 009: Loss: 1.410, Accuracy: 76.562%\n",
            "Epoch 009: Loss: 1.384, Accuracy: 75.625%\n",
            "Epoch 009: Loss: 1.390, Accuracy: 76.562%\n",
            "Epoch 009: Loss: 1.507, Accuracy: 75.000%\n",
            "Epoch 009: Loss: 1.470, Accuracy: 75.781%\n",
            "Epoch 009: Loss: 1.603, Accuracy: 72.569%\n",
            "Epoch 009: Loss: 1.582, Accuracy: 72.188%\n",
            "Epoch 009: Loss: 1.556, Accuracy: 72.443%\n",
            "Epoch 009: Loss: 1.564, Accuracy: 72.656%\n",
            "Epoch 009: Loss: 1.584, Accuracy: 72.356%\n",
            "Epoch 009: Loss: 1.593, Accuracy: 71.875%\n",
            "Epoch 009: Loss: 1.608, Accuracy: 71.875%\n",
            "Epoch 009: Loss: 1.629, Accuracy: 71.094%\n",
            "Epoch 009: Loss: 1.643, Accuracy: 71.324%\n",
            "Epoch 009: Loss: 1.648, Accuracy: 71.007%\n",
            "Epoch 009: Loss: 1.641, Accuracy: 71.217%\n",
            "Epoch 009: Loss: 1.636, Accuracy: 71.250%\n",
            "Epoch 009: Loss: 1.639, Accuracy: 71.131%\n",
            "Epoch 009: Loss: 1.633, Accuracy: 71.449%\n",
            "Epoch 009: Loss: 1.657, Accuracy: 70.924%\n",
            "Epoch 009: Loss: 1.656, Accuracy: 71.094%\n",
            "Epoch 009: Loss: 1.638, Accuracy: 71.375%\n",
            "Epoch 009: Loss: 1.652, Accuracy: 71.154%\n",
            "Epoch 009: Loss: 1.676, Accuracy: 70.370%\n",
            "Epoch 009: Loss: 1.681, Accuracy: 70.089%\n",
            "Epoch 009: Loss: 1.675, Accuracy: 70.259%\n",
            "Epoch 009: Loss: 1.670, Accuracy: 70.625%\n",
            "Epoch 009: Loss: 1.653, Accuracy: 70.968%\n",
            "Epoch 009: Loss: 1.651, Accuracy: 70.801%\n",
            "Epoch 009: Loss: 1.666, Accuracy: 70.549%\n",
            "Epoch 009: Loss: 1.674, Accuracy: 70.404%\n",
            "Epoch 009: Loss: 1.689, Accuracy: 70.000%\n",
            "Epoch 009: Loss: 1.686, Accuracy: 70.226%\n",
            "Epoch 009: Loss: 1.683, Accuracy: 70.355%\n",
            "Epoch 009: Loss: 1.684, Accuracy: 70.312%\n",
            "Epoch 009: Loss: 1.685, Accuracy: 70.433%\n",
            "Epoch 009: Loss: 1.669, Accuracy: 70.859%\n",
            "Epoch 009: Loss: 1.669, Accuracy: 70.808%\n",
            "Epoch 009: Loss: 1.672, Accuracy: 70.610%\n",
            "Epoch 009: Loss: 1.667, Accuracy: 70.858%\n",
            "Epoch 009: Loss: 1.681, Accuracy: 70.312%\n",
            "Epoch 009: Loss: 1.681, Accuracy: 70.347%\n",
            "Epoch 009: Loss: 1.693, Accuracy: 69.973%\n",
            "Epoch 009: Loss: 1.686, Accuracy: 70.213%\n",
            "Epoch 009: Loss: 1.688, Accuracy: 70.247%\n",
            "Epoch 009: Loss: 1.684, Accuracy: 70.344%\n",
            "Epoch 009: Loss: 1.682, Accuracy: 70.438%\n",
            "Epoch 009: Loss: 1.684, Accuracy: 70.466%\n",
            "Epoch 009: Loss: 1.683, Accuracy: 70.673%\n",
            "Epoch 009: Loss: 1.692, Accuracy: 70.460%\n",
            "Epoch 009: Loss: 1.692, Accuracy: 70.428%\n",
            "Epoch 009: Loss: 1.690, Accuracy: 70.455%\n",
            "Epoch 009: Loss: 1.690, Accuracy: 70.368%\n",
            "Epoch 009: Loss: 1.690, Accuracy: 70.395%\n",
            "Epoch 009: Loss: 1.693, Accuracy: 70.420%\n",
            "Epoch 009: Loss: 1.702, Accuracy: 70.233%\n",
            "Epoch 009: Loss: 1.701, Accuracy: 70.521%\n",
            "Epoch 009: Loss: 1.696, Accuracy: 70.748%\n",
            "Epoch 009: Loss: 1.695, Accuracy: 70.817%\n",
            "Epoch 009: Loss: 1.699, Accuracy: 70.585%\n",
            "Epoch 009: Loss: 1.702, Accuracy: 70.557%\n",
            "Epoch 009: Loss: 1.712, Accuracy: 70.337%\n",
            "Epoch 009: Loss: 1.710, Accuracy: 70.360%\n",
            "Epoch 009: Loss: 1.704, Accuracy: 70.429%\n",
            "Epoch 009: Loss: 1.702, Accuracy: 70.450%\n",
            "Epoch 009: Loss: 1.704, Accuracy: 70.380%\n",
            "Epoch 009: Loss: 1.705, Accuracy: 70.357%\n",
            "Epoch 009: Loss: 1.708, Accuracy: 70.202%\n",
            "Epoch 009: Loss: 1.709, Accuracy: 70.269%\n",
            "Epoch 009: Loss: 1.708, Accuracy: 70.291%\n",
            "Epoch 009: Loss: 1.715, Accuracy: 70.144%\n",
            "Epoch 009: Loss: 1.707, Accuracy: 70.417%\n",
            "Epoch 009: Loss: 1.708, Accuracy: 70.436%\n",
            "Epoch 009: Loss: 1.707, Accuracy: 70.455%\n",
            "Epoch 009: Loss: 1.708, Accuracy: 70.433%\n",
            "Epoch 009: Loss: 1.709, Accuracy: 70.411%\n",
            "Epoch 009: Loss: 1.711, Accuracy: 70.391%\n",
            "Epoch 009: Loss: 1.710, Accuracy: 70.448%\n",
            "Epoch 009: Loss: 1.706, Accuracy: 70.503%\n",
            "Epoch 009: Loss: 1.702, Accuracy: 70.633%\n",
            "Epoch 009: Loss: 1.706, Accuracy: 70.647%\n",
            "Epoch 009: Loss: 1.716, Accuracy: 70.551%\n",
            "Epoch 009: Loss: 1.710, Accuracy: 70.712%\n",
            "Epoch 009: Loss: 1.705, Accuracy: 70.797%\n",
            "Epoch 009: Loss: 1.703, Accuracy: 70.881%\n",
            "Epoch 009: Loss: 1.704, Accuracy: 70.787%\n",
            "Epoch 009: Loss: 1.705, Accuracy: 70.764%\n",
            "Epoch 009: Loss: 1.704, Accuracy: 70.742%\n",
            "Epoch 009: Loss: 1.707, Accuracy: 70.686%\n",
            "Epoch 009: Loss: 1.703, Accuracy: 70.766%\n",
            "Epoch 009: Loss: 1.710, Accuracy: 70.645%\n",
            "Epoch 009: Loss: 1.709, Accuracy: 70.724%\n",
            "Epoch 009: Loss: 1.712, Accuracy: 70.638%\n",
            "Epoch 009: Loss: 1.714, Accuracy: 70.554%\n",
            "Epoch 009: Loss: 1.714, Accuracy: 70.536%\n",
            "Epoch 009: Loss: 1.710, Accuracy: 70.581%\n",
            "Epoch 009: Loss: 1.715, Accuracy: 70.469%\n",
            "Epoch 009: Loss: 1.712, Accuracy: 70.545%\n",
            "Epoch 009: Loss: 1.714, Accuracy: 70.496%\n",
            "Epoch 009: Loss: 1.717, Accuracy: 70.419%\n",
            "Epoch 009: Loss: 1.716, Accuracy: 70.433%\n",
            "Epoch 009: Loss: 1.715, Accuracy: 70.506%\n",
            "Epoch 009: Loss: 1.717, Accuracy: 70.519%\n",
            "Epoch 009: Loss: 1.715, Accuracy: 70.502%\n",
            "Epoch 009: Loss: 1.716, Accuracy: 70.515%\n",
            "Epoch 009: Loss: 1.716, Accuracy: 70.556%\n",
            "Epoch 009: Loss: 1.715, Accuracy: 70.540%\n",
            "Epoch 009: Loss: 1.710, Accuracy: 70.664%\n",
            "Epoch 009: Loss: 1.708, Accuracy: 70.675%\n",
            "Epoch 009: Loss: 1.708, Accuracy: 70.713%\n",
            "Epoch 009: Loss: 1.710, Accuracy: 70.669%\n",
            "Epoch 009: Loss: 1.711, Accuracy: 70.652%\n",
            "Epoch 009: Loss: 1.711, Accuracy: 70.690%\n",
            "Epoch 009: Loss: 1.714, Accuracy: 70.620%\n",
            "Epoch 009: Loss: 1.713, Accuracy: 70.657%\n",
            "Epoch 009: Loss: 1.719, Accuracy: 70.509%\n",
            "Epoch 009: Loss: 1.718, Accuracy: 70.547%\n",
            "Epoch 009: Loss: 1.717, Accuracy: 70.558%\n",
            "Epoch 009: Loss: 1.718, Accuracy: 70.543%\n",
            "Epoch 009: Loss: 1.719, Accuracy: 70.452%\n",
            "Epoch 009: Loss: 1.718, Accuracy: 70.464%\n",
            "Epoch 009: Loss: 1.722, Accuracy: 70.375%\n",
            "Epoch 009: Loss: 1.721, Accuracy: 70.387%\n",
            "Epoch 009: Loss: 1.723, Accuracy: 70.374%\n",
            "Epoch 009: Loss: 1.724, Accuracy: 70.361%\n",
            "Epoch 009: Loss: 1.723, Accuracy: 70.446%\n",
            "Epoch 009: Loss: 1.719, Accuracy: 70.529%\n",
            "Epoch 009: Loss: 1.721, Accuracy: 70.444%\n",
            "Epoch 009: Loss: 1.718, Accuracy: 70.526%\n",
            "Epoch 009: Loss: 1.717, Accuracy: 70.536%\n",
            "Epoch 009: Loss: 1.717, Accuracy: 70.546%\n",
            "Epoch 009: Loss: 1.720, Accuracy: 70.509%\n",
            "Epoch 009: Loss: 1.718, Accuracy: 70.542%\n",
            "Epoch 009: Loss: 1.717, Accuracy: 70.598%\n",
            "Epoch 009: Loss: 1.713, Accuracy: 70.697%\n",
            "Epoch 009: Loss: 1.711, Accuracy: 70.728%\n",
            "Epoch 009: Loss: 1.710, Accuracy: 70.714%\n",
            "Epoch 009: Loss: 1.711, Accuracy: 70.723%\n",
            "Epoch 009: Loss: 1.708, Accuracy: 70.841%\n",
            "Epoch 009: Loss: 1.705, Accuracy: 70.935%\n",
            "Epoch 009: Loss: 1.704, Accuracy: 70.920%\n",
            "Epoch 009: Loss: 1.706, Accuracy: 70.905%\n",
            "Epoch 009: Loss: 1.706, Accuracy: 70.890%\n",
            "Epoch 009: Loss: 1.706, Accuracy: 70.876%\n",
            "Epoch 009: Loss: 1.709, Accuracy: 70.798%\n",
            "Epoch 009: Loss: 1.710, Accuracy: 70.784%\n",
            "Epoch 009: Loss: 1.707, Accuracy: 70.875%\n",
            "Epoch 009: Loss: 1.707, Accuracy: 70.944%\n",
            "Epoch 009: Loss: 1.706, Accuracy: 70.929%\n",
            "Epoch 009: Loss: 1.707, Accuracy: 70.935%\n",
            "Epoch 009: Loss: 1.705, Accuracy: 70.942%\n",
            "Epoch 009: Loss: 1.704, Accuracy: 70.948%\n",
            "Epoch 009: Loss: 1.707, Accuracy: 70.954%\n",
            "Epoch 009: Loss: 1.706, Accuracy: 70.999%\n",
            "Epoch 009: Loss: 1.709, Accuracy: 70.926%\n",
            "Epoch 009: Loss: 1.708, Accuracy: 70.932%\n",
            "Epoch 009: Loss: 1.711, Accuracy: 70.840%\n",
            "Epoch 009: Loss: 1.712, Accuracy: 70.827%\n",
            "Epoch 009: Loss: 1.711, Accuracy: 70.853%\n",
            "Epoch 009: Loss: 1.708, Accuracy: 70.878%\n",
            "Epoch 009: Loss: 1.710, Accuracy: 70.827%\n",
            "Epoch 009: Loss: 1.711, Accuracy: 70.890%\n",
            "Epoch 009: Loss: 1.712, Accuracy: 70.896%\n",
            "Epoch 009: Loss: 1.715, Accuracy: 70.883%\n",
            "Epoch 009: Loss: 1.718, Accuracy: 70.778%\n",
            "Epoch 009: Loss: 1.720, Accuracy: 70.766%\n",
            "Epoch 009: Loss: 1.719, Accuracy: 70.772%\n",
            "Epoch 009: Loss: 1.719, Accuracy: 70.779%\n",
            "Epoch 009: Loss: 1.716, Accuracy: 70.876%\n",
            "Epoch 009: Loss: 1.713, Accuracy: 70.954%\n",
            "Epoch 009: Loss: 1.710, Accuracy: 71.013%\n",
            "Epoch 009: Loss: 1.707, Accuracy: 71.089%\n",
            "Epoch 009: Loss: 1.708, Accuracy: 71.112%\n",
            "Epoch 009: Loss: 1.707, Accuracy: 71.133%\n",
            "Epoch 009: Loss: 1.707, Accuracy: 71.155%\n",
            "Epoch 009: Loss: 1.707, Accuracy: 71.142%\n",
            "Epoch 009: Loss: 1.707, Accuracy: 71.146%\n",
            "Epoch 009: Loss: 1.708, Accuracy: 71.081%\n",
            "Epoch 009: Loss: 1.705, Accuracy: 71.154%\n",
            "Epoch 009: Loss: 1.704, Accuracy: 71.192%\n",
            "Epoch 009: Loss: 1.705, Accuracy: 71.145%\n",
            "Epoch 009: Loss: 1.706, Accuracy: 71.115%\n",
            "Epoch 009: Loss: 1.708, Accuracy: 71.035%\n",
            "Epoch 009: Loss: 1.709, Accuracy: 71.023%\n",
            "Epoch 009: Loss: 1.706, Accuracy: 71.110%\n",
            "Epoch 009: Loss: 1.706, Accuracy: 71.131%\n",
            "Epoch 009: Loss: 1.705, Accuracy: 71.135%\n",
            "Epoch 009: Loss: 1.704, Accuracy: 71.139%\n",
            "Epoch 009: Loss: 1.705, Accuracy: 71.110%\n",
            "Epoch 009: Loss: 1.706, Accuracy: 71.049%\n",
            "Epoch 009: Loss: 1.707, Accuracy: 71.037%\n",
            "Epoch 009: Loss: 1.709, Accuracy: 70.994%\n",
            "Epoch 009: Loss: 1.709, Accuracy: 70.998%\n",
            "Epoch 009: Loss: 1.709, Accuracy: 71.003%\n",
            "Epoch 009: Loss: 1.708, Accuracy: 71.023%\n",
            "Epoch 009: Loss: 1.707, Accuracy: 71.027%\n",
            "Epoch 009: Loss: 1.707, Accuracy: 71.062%\n",
            "Epoch 009: Loss: 1.706, Accuracy: 71.082%\n",
            "Epoch 009: Loss: 1.708, Accuracy: 71.024%\n",
            "Epoch 009: Loss: 1.707, Accuracy: 71.013%\n",
            "Epoch 009: Loss: 1.706, Accuracy: 71.032%\n",
            "Epoch 009: Loss: 1.707, Accuracy: 70.991%\n",
            "Epoch 009: Loss: 1.707, Accuracy: 71.010%\n",
            "Epoch 009: Loss: 1.705, Accuracy: 71.060%\n",
            "Epoch 009: Loss: 1.705, Accuracy: 71.079%\n",
            "Epoch 009: Loss: 1.705, Accuracy: 71.097%\n",
            "Epoch 009: Loss: 1.704, Accuracy: 71.131%\n",
            "Epoch 009: Loss: 1.705, Accuracy: 71.075%\n",
            "Epoch 009: Loss: 1.704, Accuracy: 71.064%\n",
            "Epoch 009: Loss: 1.705, Accuracy: 71.053%\n",
            "Epoch 009: Loss: 1.707, Accuracy: 71.028%\n",
            "Epoch 009: Loss: 1.707, Accuracy: 70.974%\n",
            "Epoch 009: Loss: 1.707, Accuracy: 70.978%\n",
            "Epoch 009: Loss: 1.706, Accuracy: 71.011%\n",
            "Epoch 009: Loss: 1.706, Accuracy: 71.001%\n",
            "Epoch 009: Loss: 1.706, Accuracy: 71.005%\n",
            "Epoch 009: Loss: 1.703, Accuracy: 71.037%\n",
            "Epoch 009: Loss: 1.702, Accuracy: 71.027%\n",
            "Epoch 009: Loss: 1.702, Accuracy: 71.030%\n",
            "Epoch 009: Loss: 1.702, Accuracy: 71.076%\n",
            "Epoch 009: Loss: 1.702, Accuracy: 71.066%\n",
            "Epoch 009: Loss: 1.699, Accuracy: 71.125%\n",
            "Epoch 009: Loss: 1.698, Accuracy: 71.142%\n",
            "Epoch 009: Loss: 1.697, Accuracy: 71.145%\n",
            "Epoch 009: Loss: 1.696, Accuracy: 71.190%\n",
            "Epoch 009: Loss: 1.696, Accuracy: 71.179%\n",
            "Epoch 009: Loss: 1.698, Accuracy: 71.141%\n",
            "Epoch 009: Loss: 1.698, Accuracy: 71.117%\n",
            "Epoch 009: Loss: 1.696, Accuracy: 71.175%\n",
            "Epoch 009: Loss: 1.695, Accuracy: 71.218%\n",
            "Epoch 009: Loss: 1.694, Accuracy: 71.247%\n",
            "Epoch 009: Loss: 1.695, Accuracy: 71.223%\n",
            "Epoch 009: Loss: 1.694, Accuracy: 71.266%\n",
            "Epoch 009: Loss: 1.692, Accuracy: 71.295%\n",
            "Epoch 009: Loss: 1.693, Accuracy: 71.258%\n",
            "Epoch 009: Loss: 1.694, Accuracy: 71.221%\n",
            "Epoch 009: Loss: 1.694, Accuracy: 71.237%\n",
            "Epoch 009: Loss: 1.694, Accuracy: 71.214%\n",
            "Epoch 009: Loss: 1.694, Accuracy: 71.216%\n",
            "Epoch 009: Loss: 1.695, Accuracy: 71.258%\n",
            "Epoch 009: Loss: 1.696, Accuracy: 71.235%\n",
            "Epoch 009: Loss: 1.696, Accuracy: 71.224%\n",
            "Epoch 009: Loss: 1.695, Accuracy: 71.265%\n",
            "Epoch 009: Loss: 1.694, Accuracy: 71.306%\n",
            "Epoch 009: Loss: 1.693, Accuracy: 71.346%\n",
            "Epoch 009: Loss: 1.695, Accuracy: 71.335%\n",
            "Epoch 009: Loss: 1.694, Accuracy: 71.375%\n",
            "Epoch 009: Loss: 1.693, Accuracy: 71.377%\n",
            "Epoch 009: Loss: 1.693, Accuracy: 71.367%\n",
            "Epoch 009: Loss: 1.694, Accuracy: 71.344%\n",
            "Epoch 009: Loss: 1.695, Accuracy: 71.321%\n",
            "Epoch 009: Loss: 1.695, Accuracy: 71.348%\n",
            "Epoch 009: Loss: 1.695, Accuracy: 71.326%\n",
            "Epoch 009: Loss: 1.695, Accuracy: 71.352%\n",
            "Epoch 009: Loss: 1.695, Accuracy: 71.354%\n",
            "Epoch 009: Loss: 1.696, Accuracy: 71.368%\n",
            "Epoch 009: Loss: 1.696, Accuracy: 71.358%\n",
            "Epoch 009: Loss: 1.696, Accuracy: 71.384%\n",
            "Epoch 009: Loss: 1.696, Accuracy: 71.386%\n",
            "Epoch 009: Loss: 1.698, Accuracy: 71.388%\n",
            "Epoch 009: Loss: 1.698, Accuracy: 71.390%\n",
            "Epoch 009: Loss: 1.698, Accuracy: 71.392%\n",
            "Epoch 009: Loss: 1.699, Accuracy: 71.393%\n",
            "Epoch 009: Loss: 1.700, Accuracy: 71.360%\n",
            "Epoch 009: Loss: 1.700, Accuracy: 71.385%\n",
            "Epoch 009: Loss: 1.701, Accuracy: 71.375%\n",
            "Epoch 009: Loss: 1.702, Accuracy: 71.296%\n",
            "Epoch 009: Loss: 1.701, Accuracy: 71.333%\n",
            "Epoch 009: Loss: 1.701, Accuracy: 71.335%\n",
            "Epoch 009: Loss: 1.700, Accuracy: 71.326%\n",
            "Epoch 009: Loss: 1.699, Accuracy: 71.339%\n",
            "Epoch 009: Loss: 1.699, Accuracy: 71.330%\n",
            "Epoch 009: Loss: 1.697, Accuracy: 71.365%\n",
            "Epoch 009: Loss: 1.698, Accuracy: 71.322%\n",
            "Epoch 009: Loss: 1.699, Accuracy: 71.313%\n",
            "Epoch 009: Loss: 1.699, Accuracy: 71.326%\n",
            "Epoch 009: Loss: 1.699, Accuracy: 71.306%\n",
            "Epoch 009: Loss: 1.700, Accuracy: 71.298%\n",
            "Duration :186.531\n"
          ]
        }
      ],
      "source": [
        "# Implement the training loop\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, train_labels))\n",
        "train_dataset = train_dataset.batch(32)\n",
        "\n",
        "# keep results for plotting \n",
        "train_loss_results = []\n",
        "train_accuracy_results = []\n",
        "num_epochs = 10 \n",
        "weight_decay = 0.005\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  epoch_loss_avg = tf.keras.metrics.Mean()\n",
        "  epoch_accuracy = tf.keras.metrics.CategoricalAccuracy()\n",
        "\n",
        "  # training loop \n",
        "  for x, y in train_dataset:\n",
        "    # optimize the model \n",
        "    loss_value, grads = grad(model, x, y, weight_decay)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))   \n",
        "\n",
        "    # compute current loss\n",
        "    epoch_loss_avg(loss_value)\n",
        "    epoch_accuracy(to_categorical(y), model(x))\n",
        "\n",
        "    # end epoch \n",
        "    train_loss_results.append(epoch_loss_avg.result())\n",
        "    train_accuracy_results.append(epoch_accuracy.result())\n",
        "\n",
        "    print(\"Epoch {:03d}: Loss: {:.3f}, Accuracy: {:.3%}\".format(epoch, \n",
        "                                                                epoch_loss_avg.result(),\n",
        "                                                                epoch_accuracy.result()))\n",
        "    \n",
        "print(\"Duration :{:.3f}\".format(time.time() - start_time))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HimLzAD8PwuL"
      },
      "source": [
        "#### Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "s6dh8OxMPwuL"
      },
      "outputs": [],
      "source": [
        "# Create a Dataset object for the test set\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, test_labels))\n",
        "test_dataset = test_dataset.batch(32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "C8P2SQn3PwuL"
      },
      "outputs": [],
      "source": [
        "# Collect average loss and accuracy\n",
        "\n",
        "epoch_loss_avg = tf.keras.metrics.Mean()\n",
        "epoch_accuracy = tf.keras.metrics.CategoricalAccuracy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "Ls1WS9LdPwuL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6acd0eb1-f752-42ed-add2-76324de4796b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 1.835\n",
            "Test accuracy: 67.542%\n"
          ]
        }
      ],
      "source": [
        "# Loop over the test set and print scores\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "for x, y in test_dataset:\n",
        "    # Optimize the model\n",
        "    loss_value = loss(model, x, y, weight_decay)    \n",
        "    # Compute current loss\n",
        "    epoch_loss_avg(loss_value)  \n",
        "    # Compare predicted label to actual label\n",
        "    epoch_accuracy(to_categorical(y), model(x))\n",
        "\n",
        "print(\"Test loss: {:.3f}\".format(epoch_loss_avg.result().numpy()))\n",
        "print(\"Test accuracy: {:.3%}\".format(epoch_accuracy.result().numpy()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-itvkYY3PwuL"
      },
      "source": [
        "#### Plot the learning curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "3nICzra3PwuL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 558
        },
        "outputId": "136c91c2-16ec-4efe-9856-68bf0539b613"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x576 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Plot the training loss and accuracy\n",
        "\n",
        "fig, axes = plt.subplots(2, sharex=True, figsize=(12, 8))\n",
        "fig.suptitle('Training Metrics')\n",
        "\n",
        "axes[0].set_ylabel(\"Loss\", fontsize=14)\n",
        "axes[0].plot(train_loss_results)\n",
        "\n",
        "axes[1].set_ylabel(\"Accuracy\", fontsize=14)\n",
        "axes[1].set_xlabel(\"Epoch\", fontsize=14)\n",
        "axes[1].plot(train_accuracy_results)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3pa5OVmPwuM"
      },
      "source": [
        "#### Predict from the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "SRq6ej-2PwuM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09f61ada-7378-499c-d441-fe90651443db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction: earn\n",
            "     Label: earn\n"
          ]
        }
      ],
      "source": [
        "# Get the model prediction for an example input\n",
        "\n",
        "predicted_label = np.argmax(model(x_train[np.newaxis,0]),axis=1)[0]\n",
        "print(\"Prediction: {}\".format(class_names[predicted_label]))\n",
        "print(\"     Label: {}\".format(class_names[train_labels[0]]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwUxjlZ2PwuM"
      },
      "source": [
        "***\n",
        "<a id=\"coding_tutorial_5\"></a>\n",
        "## tf.function decorator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "4B2B-VtDPwuM"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Layer, Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.datasets import reuters\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MSAFqf2PwuM"
      },
      "source": [
        "#### Build the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "6TNtit1oPwuM"
      },
      "outputs": [],
      "source": [
        "# Initialize a new model\n",
        "\n",
        "\n",
        "model = MyModel(64,64,46)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_kDJ_IePwuM"
      },
      "source": [
        "#### Redefine the grad function using the @tf.function decorator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "uPL9mUk4PwuN"
      },
      "outputs": [],
      "source": [
        "# Use the @tf.function decorator\n",
        "\n",
        "@tf.function\n",
        "def grad(model, inputs, targets, wd):\n",
        "    with tf.GradientTape() as tape:\n",
        "        loss_value = loss(model, inputs, targets, wd)\n",
        "    return loss_value, tape.gradient(loss_value, model.trainable_variables)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tazx_7fWPwuN"
      },
      "source": [
        "#### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "5S1VDKtSPwuN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1cbb349-944a-4dab-f762-a64380f860ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 000: Loss: 11.893, Accuracy: 12.500%\n",
            "Epoch 000: Loss: 11.507, Accuracy: 23.438%\n",
            "Epoch 000: Loss: 11.070, Accuracy: 26.042%\n",
            "Epoch 000: Loss: 10.594, Accuracy: 29.688%\n",
            "Epoch 000: Loss: 10.103, Accuracy: 33.125%\n",
            "Epoch 000: Loss: 9.632, Accuracy: 33.854%\n",
            "Epoch 000: Loss: 9.185, Accuracy: 31.250%\n",
            "Epoch 000: Loss: 8.695, Accuracy: 33.203%\n",
            "Epoch 000: Loss: 8.337, Accuracy: 32.639%\n",
            "Epoch 000: Loss: 7.966, Accuracy: 33.125%\n",
            "Epoch 000: Loss: 7.608, Accuracy: 36.080%\n",
            "Epoch 000: Loss: 7.311, Accuracy: 36.979%\n",
            "Epoch 000: Loss: 7.019, Accuracy: 37.500%\n",
            "Epoch 000: Loss: 6.743, Accuracy: 38.616%\n",
            "Epoch 000: Loss: 6.502, Accuracy: 39.583%\n",
            "Epoch 000: Loss: 6.292, Accuracy: 40.039%\n",
            "Epoch 000: Loss: 6.112, Accuracy: 39.706%\n",
            "Epoch 000: Loss: 5.923, Accuracy: 40.278%\n",
            "Epoch 000: Loss: 5.760, Accuracy: 40.789%\n",
            "Epoch 000: Loss: 5.603, Accuracy: 41.250%\n",
            "Epoch 000: Loss: 5.480, Accuracy: 41.220%\n",
            "Epoch 000: Loss: 5.352, Accuracy: 41.903%\n",
            "Epoch 000: Loss: 5.260, Accuracy: 41.440%\n",
            "Epoch 000: Loss: 5.159, Accuracy: 41.536%\n",
            "Epoch 000: Loss: 5.049, Accuracy: 42.125%\n",
            "Epoch 000: Loss: 4.976, Accuracy: 42.067%\n",
            "Epoch 000: Loss: 4.915, Accuracy: 42.014%\n",
            "Epoch 000: Loss: 4.856, Accuracy: 41.406%\n",
            "Epoch 000: Loss: 4.776, Accuracy: 41.810%\n",
            "Epoch 000: Loss: 4.710, Accuracy: 41.979%\n",
            "Epoch 000: Loss: 4.624, Accuracy: 42.742%\n",
            "Epoch 000: Loss: 4.562, Accuracy: 42.676%\n",
            "Epoch 000: Loss: 4.512, Accuracy: 42.708%\n",
            "Epoch 000: Loss: 4.467, Accuracy: 42.923%\n",
            "Epoch 000: Loss: 4.426, Accuracy: 43.036%\n",
            "Epoch 000: Loss: 4.379, Accuracy: 43.316%\n",
            "Epoch 000: Loss: 4.328, Accuracy: 43.328%\n",
            "Epoch 000: Loss: 4.285, Accuracy: 43.668%\n",
            "Epoch 000: Loss: 4.247, Accuracy: 43.510%\n",
            "Epoch 000: Loss: 4.195, Accuracy: 43.672%\n",
            "Epoch 000: Loss: 4.148, Accuracy: 43.979%\n",
            "Epoch 000: Loss: 4.105, Accuracy: 44.345%\n",
            "Epoch 000: Loss: 4.057, Accuracy: 44.767%\n",
            "Epoch 000: Loss: 4.035, Accuracy: 44.531%\n",
            "Epoch 000: Loss: 3.998, Accuracy: 44.722%\n",
            "Epoch 000: Loss: 3.972, Accuracy: 44.769%\n",
            "Epoch 000: Loss: 3.928, Accuracy: 45.279%\n",
            "Epoch 000: Loss: 3.892, Accuracy: 45.508%\n",
            "Epoch 000: Loss: 3.857, Accuracy: 45.663%\n",
            "Epoch 000: Loss: 3.823, Accuracy: 45.812%\n",
            "Epoch 000: Loss: 3.799, Accuracy: 45.650%\n",
            "Epoch 000: Loss: 3.772, Accuracy: 45.733%\n",
            "Epoch 000: Loss: 3.751, Accuracy: 45.696%\n",
            "Epoch 000: Loss: 3.735, Accuracy: 45.602%\n",
            "Epoch 000: Loss: 3.703, Accuracy: 45.909%\n",
            "Epoch 000: Loss: 3.681, Accuracy: 45.871%\n",
            "Epoch 000: Loss: 3.659, Accuracy: 45.833%\n",
            "Epoch 000: Loss: 3.638, Accuracy: 45.744%\n",
            "Epoch 000: Loss: 3.629, Accuracy: 45.604%\n",
            "Epoch 000: Loss: 3.597, Accuracy: 45.937%\n",
            "Epoch 000: Loss: 3.572, Accuracy: 46.004%\n",
            "Epoch 000: Loss: 3.547, Accuracy: 46.220%\n",
            "Epoch 000: Loss: 3.535, Accuracy: 46.280%\n",
            "Epoch 000: Loss: 3.523, Accuracy: 46.240%\n",
            "Epoch 000: Loss: 3.514, Accuracy: 46.298%\n",
            "Epoch 000: Loss: 3.494, Accuracy: 46.402%\n",
            "Epoch 000: Loss: 3.468, Accuracy: 46.782%\n",
            "Epoch 000: Loss: 3.446, Accuracy: 46.967%\n",
            "Epoch 000: Loss: 3.433, Accuracy: 46.966%\n",
            "Epoch 000: Loss: 3.417, Accuracy: 46.964%\n",
            "Epoch 000: Loss: 3.409, Accuracy: 46.963%\n",
            "Epoch 000: Loss: 3.392, Accuracy: 47.049%\n",
            "Epoch 000: Loss: 3.372, Accuracy: 47.346%\n",
            "Epoch 000: Loss: 3.364, Accuracy: 47.382%\n",
            "Epoch 000: Loss: 3.342, Accuracy: 47.667%\n",
            "Epoch 000: Loss: 3.331, Accuracy: 47.697%\n",
            "Epoch 000: Loss: 3.317, Accuracy: 47.849%\n",
            "Epoch 000: Loss: 3.306, Accuracy: 47.837%\n",
            "Epoch 000: Loss: 3.294, Accuracy: 47.824%\n",
            "Epoch 000: Loss: 3.284, Accuracy: 47.852%\n",
            "Epoch 000: Loss: 3.268, Accuracy: 48.032%\n",
            "Epoch 000: Loss: 3.248, Accuracy: 48.285%\n",
            "Epoch 000: Loss: 3.233, Accuracy: 48.494%\n",
            "Epoch 000: Loss: 3.223, Accuracy: 48.438%\n",
            "Epoch 000: Loss: 3.218, Accuracy: 48.309%\n",
            "Epoch 000: Loss: 3.202, Accuracy: 48.438%\n",
            "Epoch 000: Loss: 3.187, Accuracy: 48.599%\n",
            "Epoch 000: Loss: 3.171, Accuracy: 48.864%\n",
            "Epoch 000: Loss: 3.163, Accuracy: 48.841%\n",
            "Epoch 000: Loss: 3.154, Accuracy: 48.819%\n",
            "Epoch 000: Loss: 3.142, Accuracy: 49.004%\n",
            "Epoch 000: Loss: 3.135, Accuracy: 48.947%\n",
            "Epoch 000: Loss: 3.119, Accuracy: 49.126%\n",
            "Epoch 000: Loss: 3.119, Accuracy: 49.036%\n",
            "Epoch 000: Loss: 3.108, Accuracy: 49.211%\n",
            "Epoch 000: Loss: 3.105, Accuracy: 49.219%\n",
            "Epoch 000: Loss: 3.100, Accuracy: 49.195%\n",
            "Epoch 000: Loss: 3.089, Accuracy: 49.298%\n",
            "Epoch 000: Loss: 3.076, Accuracy: 49.369%\n",
            "Epoch 000: Loss: 3.073, Accuracy: 49.344%\n",
            "Epoch 000: Loss: 3.062, Accuracy: 49.474%\n",
            "Epoch 000: Loss: 3.057, Accuracy: 49.449%\n",
            "Epoch 000: Loss: 3.050, Accuracy: 49.484%\n",
            "Epoch 000: Loss: 3.040, Accuracy: 49.609%\n",
            "Epoch 000: Loss: 3.033, Accuracy: 49.613%\n",
            "Epoch 000: Loss: 3.028, Accuracy: 49.587%\n",
            "Epoch 000: Loss: 3.020, Accuracy: 49.620%\n",
            "Epoch 000: Loss: 3.012, Accuracy: 49.595%\n",
            "Epoch 000: Loss: 3.005, Accuracy: 49.627%\n",
            "Epoch 000: Loss: 2.997, Accuracy: 49.687%\n",
            "Epoch 000: Loss: 2.985, Accuracy: 49.887%\n",
            "Epoch 000: Loss: 2.976, Accuracy: 49.944%\n",
            "Epoch 000: Loss: 2.970, Accuracy: 50.000%\n",
            "Epoch 000: Loss: 2.968, Accuracy: 49.945%\n",
            "Epoch 000: Loss: 2.962, Accuracy: 50.000%\n",
            "Epoch 000: Loss: 2.954, Accuracy: 50.081%\n",
            "Epoch 000: Loss: 2.948, Accuracy: 50.107%\n",
            "Epoch 000: Loss: 2.941, Accuracy: 50.185%\n",
            "Epoch 000: Loss: 2.939, Accuracy: 50.210%\n",
            "Epoch 000: Loss: 2.931, Accuracy: 50.339%\n",
            "Epoch 000: Loss: 2.922, Accuracy: 50.413%\n",
            "Epoch 000: Loss: 2.916, Accuracy: 50.461%\n",
            "Epoch 000: Loss: 2.914, Accuracy: 50.407%\n",
            "Epoch 000: Loss: 2.907, Accuracy: 50.479%\n",
            "Epoch 000: Loss: 2.906, Accuracy: 50.475%\n",
            "Epoch 000: Loss: 2.900, Accuracy: 50.521%\n",
            "Epoch 000: Loss: 2.894, Accuracy: 50.517%\n",
            "Epoch 000: Loss: 2.889, Accuracy: 50.586%\n",
            "Epoch 000: Loss: 2.880, Accuracy: 50.751%\n",
            "Epoch 000: Loss: 2.873, Accuracy: 50.865%\n",
            "Epoch 000: Loss: 2.870, Accuracy: 50.835%\n",
            "Epoch 000: Loss: 2.862, Accuracy: 50.994%\n",
            "Epoch 000: Loss: 2.855, Accuracy: 51.128%\n",
            "Epoch 000: Loss: 2.851, Accuracy: 51.143%\n",
            "Epoch 000: Loss: 2.849, Accuracy: 51.088%\n",
            "Epoch 000: Loss: 2.844, Accuracy: 51.172%\n",
            "Epoch 000: Loss: 2.837, Accuracy: 51.300%\n",
            "Epoch 000: Loss: 2.828, Accuracy: 51.472%\n",
            "Epoch 000: Loss: 2.822, Accuracy: 51.506%\n",
            "Epoch 000: Loss: 2.817, Accuracy: 51.562%\n",
            "Epoch 000: Loss: 2.813, Accuracy: 51.551%\n",
            "Epoch 000: Loss: 2.806, Accuracy: 51.607%\n",
            "Epoch 000: Loss: 2.798, Accuracy: 51.770%\n",
            "Epoch 000: Loss: 2.792, Accuracy: 51.780%\n",
            "Epoch 000: Loss: 2.787, Accuracy: 51.832%\n",
            "Epoch 000: Loss: 2.783, Accuracy: 51.905%\n",
            "Epoch 000: Loss: 2.779, Accuracy: 51.935%\n",
            "Epoch 000: Loss: 2.776, Accuracy: 51.964%\n",
            "Epoch 000: Loss: 2.773, Accuracy: 52.013%\n",
            "Epoch 000: Loss: 2.765, Accuracy: 52.167%\n",
            "Epoch 000: Loss: 2.760, Accuracy: 52.214%\n",
            "Epoch 000: Loss: 2.756, Accuracy: 52.241%\n",
            "Epoch 000: Loss: 2.752, Accuracy: 52.288%\n",
            "Epoch 000: Loss: 2.744, Accuracy: 52.455%\n",
            "Epoch 000: Loss: 2.739, Accuracy: 52.520%\n",
            "Epoch 000: Loss: 2.736, Accuracy: 52.604%\n",
            "Epoch 000: Loss: 2.731, Accuracy: 52.647%\n",
            "Epoch 000: Loss: 2.730, Accuracy: 52.571%\n",
            "Epoch 000: Loss: 2.727, Accuracy: 52.614%\n",
            "Epoch 000: Loss: 2.726, Accuracy: 52.559%\n",
            "Epoch 000: Loss: 2.723, Accuracy: 52.601%\n",
            "Epoch 000: Loss: 2.717, Accuracy: 52.701%\n",
            "Epoch 000: Loss: 2.710, Accuracy: 52.799%\n",
            "Epoch 000: Loss: 2.707, Accuracy: 52.858%\n",
            "Epoch 000: Loss: 2.704, Accuracy: 52.917%\n",
            "Epoch 000: Loss: 2.701, Accuracy: 52.918%\n",
            "Epoch 000: Loss: 2.699, Accuracy: 52.919%\n",
            "Epoch 000: Loss: 2.698, Accuracy: 52.902%\n",
            "Epoch 000: Loss: 2.696, Accuracy: 52.922%\n",
            "Epoch 000: Loss: 2.692, Accuracy: 52.923%\n",
            "Epoch 000: Loss: 2.688, Accuracy: 52.942%\n",
            "Epoch 000: Loss: 2.682, Accuracy: 53.034%\n",
            "Epoch 000: Loss: 2.673, Accuracy: 53.215%\n",
            "Epoch 000: Loss: 2.668, Accuracy: 53.233%\n",
            "Epoch 000: Loss: 2.661, Accuracy: 53.393%\n",
            "Epoch 000: Loss: 2.658, Accuracy: 53.427%\n",
            "Epoch 000: Loss: 2.653, Accuracy: 53.460%\n",
            "Epoch 000: Loss: 2.650, Accuracy: 53.511%\n",
            "Epoch 000: Loss: 2.648, Accuracy: 53.492%\n",
            "Epoch 000: Loss: 2.645, Accuracy: 53.524%\n",
            "Epoch 000: Loss: 2.643, Accuracy: 53.470%\n",
            "Epoch 000: Loss: 2.637, Accuracy: 53.606%\n",
            "Epoch 000: Loss: 2.635, Accuracy: 53.689%\n",
            "Epoch 000: Loss: 2.633, Accuracy: 53.702%\n",
            "Epoch 000: Loss: 2.631, Accuracy: 53.682%\n",
            "Epoch 000: Loss: 2.631, Accuracy: 53.629%\n",
            "Epoch 000: Loss: 2.629, Accuracy: 53.660%\n",
            "Epoch 000: Loss: 2.623, Accuracy: 53.773%\n",
            "Epoch 000: Loss: 2.621, Accuracy: 53.753%\n",
            "Epoch 000: Loss: 2.617, Accuracy: 53.766%\n",
            "Epoch 000: Loss: 2.614, Accuracy: 53.779%\n",
            "Epoch 000: Loss: 2.615, Accuracy: 53.760%\n",
            "Epoch 000: Loss: 2.612, Accuracy: 53.740%\n",
            "Epoch 000: Loss: 2.612, Accuracy: 53.689%\n",
            "Epoch 000: Loss: 2.612, Accuracy: 53.686%\n",
            "Epoch 000: Loss: 2.609, Accuracy: 53.731%\n",
            "Epoch 000: Loss: 2.604, Accuracy: 53.823%\n",
            "Epoch 000: Loss: 2.601, Accuracy: 53.867%\n",
            "Epoch 000: Loss: 2.597, Accuracy: 53.926%\n",
            "Epoch 000: Loss: 2.594, Accuracy: 54.000%\n",
            "Epoch 000: Loss: 2.590, Accuracy: 54.089%\n",
            "Epoch 000: Loss: 2.591, Accuracy: 54.053%\n",
            "Epoch 000: Loss: 2.587, Accuracy: 54.079%\n",
            "Epoch 000: Loss: 2.584, Accuracy: 54.059%\n",
            "Epoch 000: Loss: 2.584, Accuracy: 54.024%\n",
            "Epoch 000: Loss: 2.582, Accuracy: 54.035%\n",
            "Epoch 000: Loss: 2.576, Accuracy: 54.136%\n",
            "Epoch 000: Loss: 2.573, Accuracy: 54.162%\n",
            "Epoch 000: Loss: 2.571, Accuracy: 54.172%\n",
            "Epoch 000: Loss: 2.568, Accuracy: 54.211%\n",
            "Epoch 000: Loss: 2.567, Accuracy: 54.236%\n",
            "Epoch 000: Loss: 2.564, Accuracy: 54.275%\n",
            "Epoch 000: Loss: 2.563, Accuracy: 54.284%\n",
            "Epoch 000: Loss: 2.563, Accuracy: 54.264%\n",
            "Epoch 000: Loss: 2.560, Accuracy: 54.273%\n",
            "Epoch 000: Loss: 2.558, Accuracy: 54.311%\n",
            "Epoch 000: Loss: 2.555, Accuracy: 54.378%\n",
            "Epoch 000: Loss: 2.552, Accuracy: 54.429%\n",
            "Epoch 000: Loss: 2.550, Accuracy: 54.481%\n",
            "Epoch 000: Loss: 2.545, Accuracy: 54.588%\n",
            "Epoch 000: Loss: 2.542, Accuracy: 54.610%\n",
            "Epoch 000: Loss: 2.540, Accuracy: 54.617%\n",
            "Epoch 000: Loss: 2.537, Accuracy: 54.723%\n",
            "Epoch 000: Loss: 2.535, Accuracy: 54.743%\n",
            "Epoch 000: Loss: 2.530, Accuracy: 54.833%\n",
            "Epoch 000: Loss: 2.528, Accuracy: 54.853%\n",
            "Epoch 000: Loss: 2.525, Accuracy: 54.901%\n",
            "Epoch 000: Loss: 2.522, Accuracy: 54.962%\n",
            "Epoch 000: Loss: 2.521, Accuracy: 54.967%\n",
            "Epoch 000: Loss: 2.518, Accuracy: 54.973%\n",
            "Epoch 000: Loss: 2.517, Accuracy: 54.992%\n",
            "Epoch 000: Loss: 2.513, Accuracy: 55.092%\n",
            "Epoch 000: Loss: 2.510, Accuracy: 55.123%\n",
            "Epoch 000: Loss: 2.506, Accuracy: 55.182%\n",
            "Epoch 000: Loss: 2.504, Accuracy: 55.226%\n",
            "Epoch 000: Loss: 2.501, Accuracy: 55.323%\n",
            "Epoch 000: Loss: 2.497, Accuracy: 55.393%\n",
            "Epoch 000: Loss: 2.498, Accuracy: 55.344%\n",
            "Epoch 000: Loss: 2.497, Accuracy: 55.361%\n",
            "Epoch 000: Loss: 2.495, Accuracy: 55.391%\n",
            "Epoch 000: Loss: 2.494, Accuracy: 55.394%\n",
            "Epoch 000: Loss: 2.492, Accuracy: 55.462%\n",
            "Epoch 000: Loss: 2.491, Accuracy: 55.517%\n",
            "Epoch 000: Loss: 2.489, Accuracy: 55.571%\n",
            "Epoch 000: Loss: 2.488, Accuracy: 55.599%\n",
            "Epoch 000: Loss: 2.486, Accuracy: 55.666%\n",
            "Epoch 000: Loss: 2.483, Accuracy: 55.706%\n",
            "Epoch 000: Loss: 2.479, Accuracy: 55.759%\n",
            "Epoch 000: Loss: 2.479, Accuracy: 55.735%\n",
            "Epoch 000: Loss: 2.476, Accuracy: 55.800%\n",
            "Epoch 000: Loss: 2.473, Accuracy: 55.876%\n",
            "Epoch 000: Loss: 2.470, Accuracy: 55.903%\n",
            "Epoch 000: Loss: 2.470, Accuracy: 55.904%\n",
            "Epoch 000: Loss: 2.469, Accuracy: 55.918%\n",
            "Epoch 000: Loss: 2.468, Accuracy: 55.944%\n",
            "Epoch 000: Loss: 2.466, Accuracy: 55.969%\n",
            "Epoch 000: Loss: 2.464, Accuracy: 55.995%\n",
            "Epoch 000: Loss: 2.462, Accuracy: 56.032%\n",
            "Epoch 000: Loss: 2.460, Accuracy: 56.045%\n",
            "Epoch 000: Loss: 2.460, Accuracy: 56.070%\n",
            "Epoch 000: Loss: 2.458, Accuracy: 56.106%\n",
            "Epoch 000: Loss: 2.456, Accuracy: 56.143%\n",
            "Epoch 000: Loss: 2.456, Accuracy: 56.131%\n",
            "Epoch 000: Loss: 2.455, Accuracy: 56.120%\n",
            "Epoch 000: Loss: 2.453, Accuracy: 56.156%\n",
            "Epoch 000: Loss: 2.452, Accuracy: 56.203%\n",
            "Epoch 000: Loss: 2.451, Accuracy: 56.215%\n",
            "Epoch 000: Loss: 2.448, Accuracy: 56.262%\n",
            "Epoch 000: Loss: 2.448, Accuracy: 56.273%\n",
            "Epoch 000: Loss: 2.449, Accuracy: 56.238%\n",
            "Epoch 000: Loss: 2.446, Accuracy: 56.319%\n",
            "Epoch 000: Loss: 2.444, Accuracy: 56.353%\n",
            "Epoch 000: Loss: 2.443, Accuracy: 56.387%\n",
            "Epoch 000: Loss: 2.442, Accuracy: 56.421%\n",
            "Epoch 000: Loss: 2.440, Accuracy: 56.432%\n",
            "Epoch 000: Loss: 2.437, Accuracy: 56.476%\n",
            "Epoch 000: Loss: 2.437, Accuracy: 56.476%\n",
            "Epoch 000: Loss: 2.436, Accuracy: 56.475%\n",
            "Epoch 000: Loss: 2.435, Accuracy: 56.519%\n",
            "Epoch 000: Loss: 2.433, Accuracy: 56.540%\n",
            "Epoch 000: Loss: 2.431, Accuracy: 56.580%\n",
            "Epoch 001: Loss: 1.428, Accuracy: 75.000%\n",
            "Epoch 001: Loss: 1.426, Accuracy: 76.562%\n",
            "Epoch 001: Loss: 1.628, Accuracy: 69.792%\n",
            "Epoch 001: Loss: 1.610, Accuracy: 71.094%\n",
            "Epoch 001: Loss: 1.602, Accuracy: 73.125%\n",
            "Epoch 001: Loss: 1.642, Accuracy: 71.354%\n",
            "Epoch 001: Loss: 1.768, Accuracy: 67.857%\n",
            "Epoch 001: Loss: 1.702, Accuracy: 69.141%\n",
            "Epoch 001: Loss: 1.849, Accuracy: 65.972%\n",
            "Epoch 001: Loss: 1.840, Accuracy: 66.250%\n",
            "Epoch 001: Loss: 1.809, Accuracy: 67.330%\n",
            "Epoch 001: Loss: 1.813, Accuracy: 66.927%\n",
            "Epoch 001: Loss: 1.832, Accuracy: 66.346%\n",
            "Epoch 001: Loss: 1.829, Accuracy: 66.071%\n",
            "Epoch 001: Loss: 1.838, Accuracy: 66.042%\n",
            "Epoch 001: Loss: 1.877, Accuracy: 65.430%\n",
            "Epoch 001: Loss: 1.900, Accuracy: 64.706%\n",
            "Epoch 001: Loss: 1.894, Accuracy: 65.104%\n",
            "Epoch 001: Loss: 1.887, Accuracy: 64.474%\n",
            "Epoch 001: Loss: 1.889, Accuracy: 64.688%\n",
            "Epoch 001: Loss: 1.897, Accuracy: 64.435%\n",
            "Epoch 001: Loss: 1.889, Accuracy: 64.773%\n",
            "Epoch 001: Loss: 1.923, Accuracy: 64.402%\n",
            "Epoch 001: Loss: 1.925, Accuracy: 64.583%\n",
            "Epoch 001: Loss: 1.911, Accuracy: 64.750%\n",
            "Epoch 001: Loss: 1.924, Accuracy: 64.303%\n",
            "Epoch 001: Loss: 1.948, Accuracy: 63.426%\n",
            "Epoch 001: Loss: 1.967, Accuracy: 62.946%\n",
            "Epoch 001: Loss: 1.971, Accuracy: 63.147%\n",
            "Epoch 001: Loss: 1.969, Accuracy: 63.333%\n",
            "Epoch 001: Loss: 1.949, Accuracy: 63.710%\n",
            "Epoch 001: Loss: 1.945, Accuracy: 63.770%\n",
            "Epoch 001: Loss: 1.959, Accuracy: 63.636%\n",
            "Epoch 001: Loss: 1.977, Accuracy: 63.327%\n",
            "Epoch 001: Loss: 1.990, Accuracy: 63.125%\n",
            "Epoch 001: Loss: 1.991, Accuracy: 63.368%\n",
            "Epoch 001: Loss: 1.992, Accuracy: 63.429%\n",
            "Epoch 001: Loss: 1.992, Accuracy: 63.569%\n",
            "Epoch 001: Loss: 1.993, Accuracy: 63.462%\n",
            "Epoch 001: Loss: 1.985, Accuracy: 63.906%\n",
            "Epoch 001: Loss: 1.988, Accuracy: 63.872%\n",
            "Epoch 001: Loss: 1.989, Accuracy: 63.839%\n",
            "Epoch 001: Loss: 1.983, Accuracy: 64.099%\n",
            "Epoch 001: Loss: 1.997, Accuracy: 63.636%\n",
            "Epoch 001: Loss: 1.994, Accuracy: 63.681%\n",
            "Epoch 001: Loss: 2.004, Accuracy: 63.519%\n",
            "Epoch 001: Loss: 1.993, Accuracy: 63.763%\n",
            "Epoch 001: Loss: 1.992, Accuracy: 63.867%\n",
            "Epoch 001: Loss: 1.986, Accuracy: 63.903%\n",
            "Epoch 001: Loss: 1.991, Accuracy: 63.937%\n",
            "Epoch 001: Loss: 1.994, Accuracy: 63.971%\n",
            "Epoch 001: Loss: 1.991, Accuracy: 64.062%\n",
            "Epoch 001: Loss: 1.998, Accuracy: 63.974%\n",
            "Epoch 001: Loss: 2.003, Accuracy: 63.889%\n",
            "Epoch 001: Loss: 1.998, Accuracy: 63.977%\n",
            "Epoch 001: Loss: 1.997, Accuracy: 63.839%\n",
            "Epoch 001: Loss: 1.998, Accuracy: 63.816%\n",
            "Epoch 001: Loss: 2.009, Accuracy: 63.685%\n",
            "Epoch 001: Loss: 2.018, Accuracy: 63.453%\n",
            "Epoch 001: Loss: 2.011, Accuracy: 63.594%\n",
            "Epoch 001: Loss: 2.007, Accuracy: 63.627%\n",
            "Epoch 001: Loss: 2.009, Accuracy: 63.710%\n",
            "Epoch 001: Loss: 2.014, Accuracy: 63.641%\n",
            "Epoch 001: Loss: 2.018, Accuracy: 63.525%\n",
            "Epoch 001: Loss: 2.026, Accuracy: 63.365%\n",
            "Epoch 001: Loss: 2.024, Accuracy: 63.542%\n",
            "Epoch 001: Loss: 2.015, Accuracy: 63.759%\n",
            "Epoch 001: Loss: 2.013, Accuracy: 63.787%\n",
            "Epoch 001: Loss: 2.017, Accuracy: 63.678%\n",
            "Epoch 001: Loss: 2.018, Accuracy: 63.616%\n",
            "Epoch 001: Loss: 2.027, Accuracy: 63.512%\n",
            "Epoch 001: Loss: 2.026, Accuracy: 63.585%\n",
            "Epoch 001: Loss: 2.021, Accuracy: 63.613%\n",
            "Epoch 001: Loss: 2.027, Accuracy: 63.429%\n",
            "Epoch 001: Loss: 2.018, Accuracy: 63.750%\n",
            "Epoch 001: Loss: 2.016, Accuracy: 63.734%\n",
            "Epoch 001: Loss: 2.015, Accuracy: 63.758%\n",
            "Epoch 001: Loss: 2.016, Accuracy: 63.662%\n",
            "Epoch 001: Loss: 2.014, Accuracy: 63.647%\n",
            "Epoch 001: Loss: 2.015, Accuracy: 63.555%\n",
            "Epoch 001: Loss: 2.012, Accuracy: 63.542%\n",
            "Epoch 001: Loss: 2.007, Accuracy: 63.681%\n",
            "Epoch 001: Loss: 2.001, Accuracy: 63.818%\n",
            "Epoch 001: Loss: 2.003, Accuracy: 63.765%\n",
            "Epoch 001: Loss: 2.007, Accuracy: 63.640%\n",
            "Epoch 001: Loss: 2.003, Accuracy: 63.772%\n",
            "Epoch 001: Loss: 1.997, Accuracy: 63.829%\n",
            "Epoch 001: Loss: 1.993, Accuracy: 63.885%\n",
            "Epoch 001: Loss: 1.996, Accuracy: 63.834%\n",
            "Epoch 001: Loss: 1.994, Accuracy: 63.854%\n",
            "Epoch 001: Loss: 1.995, Accuracy: 63.839%\n",
            "Epoch 001: Loss: 1.997, Accuracy: 63.757%\n",
            "Epoch 001: Loss: 1.992, Accuracy: 63.878%\n",
            "Epoch 001: Loss: 2.002, Accuracy: 63.797%\n",
            "Epoch 001: Loss: 1.999, Accuracy: 63.882%\n",
            "Epoch 001: Loss: 2.002, Accuracy: 63.802%\n",
            "Epoch 001: Loss: 2.004, Accuracy: 63.692%\n",
            "Epoch 001: Loss: 2.002, Accuracy: 63.680%\n",
            "Epoch 001: Loss: 1.998, Accuracy: 63.794%\n",
            "Epoch 001: Loss: 2.001, Accuracy: 63.656%\n",
            "Epoch 001: Loss: 1.999, Accuracy: 63.707%\n",
            "Epoch 001: Loss: 2.002, Accuracy: 63.725%\n",
            "Epoch 001: Loss: 2.002, Accuracy: 63.714%\n",
            "Epoch 001: Loss: 2.002, Accuracy: 63.702%\n",
            "Epoch 001: Loss: 2.001, Accuracy: 63.780%\n",
            "Epoch 001: Loss: 2.002, Accuracy: 63.797%\n",
            "Epoch 001: Loss: 2.001, Accuracy: 63.843%\n",
            "Epoch 001: Loss: 2.001, Accuracy: 63.744%\n",
            "Epoch 001: Loss: 2.000, Accuracy: 63.790%\n",
            "Epoch 001: Loss: 1.998, Accuracy: 63.778%\n",
            "Epoch 001: Loss: 1.994, Accuracy: 63.880%\n",
            "Epoch 001: Loss: 1.992, Accuracy: 63.951%\n",
            "Epoch 001: Loss: 1.991, Accuracy: 63.883%\n",
            "Epoch 001: Loss: 1.995, Accuracy: 63.816%\n",
            "Epoch 001: Loss: 1.997, Accuracy: 63.859%\n",
            "Epoch 001: Loss: 1.996, Accuracy: 63.847%\n",
            "Epoch 001: Loss: 1.998, Accuracy: 63.809%\n",
            "Epoch 001: Loss: 1.997, Accuracy: 63.824%\n",
            "Epoch 001: Loss: 2.002, Accuracy: 63.734%\n",
            "Epoch 001: Loss: 1.999, Accuracy: 63.802%\n",
            "Epoch 001: Loss: 1.997, Accuracy: 63.791%\n",
            "Epoch 001: Loss: 1.996, Accuracy: 63.781%\n",
            "Epoch 001: Loss: 2.000, Accuracy: 63.618%\n",
            "Epoch 001: Loss: 1.998, Accuracy: 63.634%\n",
            "Epoch 001: Loss: 2.006, Accuracy: 63.550%\n",
            "Epoch 001: Loss: 2.005, Accuracy: 63.566%\n",
            "Epoch 001: Loss: 2.006, Accuracy: 63.533%\n",
            "Epoch 001: Loss: 2.007, Accuracy: 63.501%\n",
            "Epoch 001: Loss: 2.003, Accuracy: 63.566%\n",
            "Epoch 001: Loss: 2.000, Accuracy: 63.606%\n",
            "Epoch 001: Loss: 2.004, Accuracy: 63.526%\n",
            "Epoch 001: Loss: 2.001, Accuracy: 63.660%\n",
            "Epoch 001: Loss: 1.999, Accuracy: 63.745%\n",
            "Epoch 001: Loss: 1.998, Accuracy: 63.759%\n",
            "Epoch 001: Loss: 2.001, Accuracy: 63.750%\n",
            "Epoch 001: Loss: 2.000, Accuracy: 63.833%\n",
            "Epoch 001: Loss: 1.998, Accuracy: 63.914%\n",
            "Epoch 001: Loss: 1.993, Accuracy: 64.062%\n",
            "Epoch 001: Loss: 1.991, Accuracy: 64.119%\n",
            "Epoch 001: Loss: 1.990, Accuracy: 64.152%\n",
            "Epoch 001: Loss: 1.990, Accuracy: 64.140%\n",
            "Epoch 001: Loss: 1.986, Accuracy: 64.305%\n",
            "Epoch 001: Loss: 1.983, Accuracy: 64.445%\n",
            "Epoch 001: Loss: 1.982, Accuracy: 64.431%\n",
            "Epoch 001: Loss: 1.982, Accuracy: 64.397%\n",
            "Epoch 001: Loss: 1.981, Accuracy: 64.426%\n",
            "Epoch 001: Loss: 1.980, Accuracy: 64.456%\n",
            "Epoch 001: Loss: 1.982, Accuracy: 64.400%\n",
            "Epoch 001: Loss: 1.983, Accuracy: 64.430%\n",
            "Epoch 001: Loss: 1.980, Accuracy: 64.542%\n",
            "Epoch 001: Loss: 1.978, Accuracy: 64.590%\n",
            "Epoch 001: Loss: 1.977, Accuracy: 64.618%\n",
            "Epoch 001: Loss: 1.976, Accuracy: 64.624%\n",
            "Epoch 001: Loss: 1.974, Accuracy: 64.712%\n",
            "Epoch 001: Loss: 1.972, Accuracy: 64.738%\n",
            "Epoch 001: Loss: 1.974, Accuracy: 64.724%\n",
            "Epoch 001: Loss: 1.972, Accuracy: 64.769%\n",
            "Epoch 001: Loss: 1.976, Accuracy: 64.715%\n",
            "Epoch 001: Loss: 1.976, Accuracy: 64.701%\n",
            "Epoch 001: Loss: 1.977, Accuracy: 64.609%\n",
            "Epoch 001: Loss: 1.979, Accuracy: 64.635%\n",
            "Epoch 001: Loss: 1.978, Accuracy: 64.660%\n",
            "Epoch 001: Loss: 1.975, Accuracy: 64.686%\n",
            "Epoch 001: Loss: 1.976, Accuracy: 64.653%\n",
            "Epoch 001: Loss: 1.976, Accuracy: 64.678%\n",
            "Epoch 001: Loss: 1.975, Accuracy: 64.627%\n",
            "Epoch 001: Loss: 1.978, Accuracy: 64.540%\n",
            "Epoch 001: Loss: 1.980, Accuracy: 64.472%\n",
            "Epoch 001: Loss: 1.981, Accuracy: 64.460%\n",
            "Epoch 001: Loss: 1.980, Accuracy: 64.485%\n",
            "Epoch 001: Loss: 1.980, Accuracy: 64.492%\n",
            "Epoch 001: Loss: 1.976, Accuracy: 64.589%\n",
            "Epoch 001: Loss: 1.971, Accuracy: 64.704%\n",
            "Epoch 001: Loss: 1.969, Accuracy: 64.781%\n",
            "Epoch 001: Loss: 1.965, Accuracy: 64.857%\n",
            "Epoch 001: Loss: 1.964, Accuracy: 64.879%\n",
            "Epoch 001: Loss: 1.962, Accuracy: 64.848%\n",
            "Epoch 001: Loss: 1.961, Accuracy: 64.888%\n",
            "Epoch 001: Loss: 1.963, Accuracy: 64.892%\n",
            "Epoch 001: Loss: 1.963, Accuracy: 64.913%\n",
            "Epoch 001: Loss: 1.964, Accuracy: 64.848%\n",
            "Epoch 001: Loss: 1.961, Accuracy: 64.955%\n",
            "Epoch 001: Loss: 1.959, Accuracy: 65.027%\n",
            "Epoch 001: Loss: 1.960, Accuracy: 64.980%\n",
            "Epoch 001: Loss: 1.962, Accuracy: 64.966%\n",
            "Epoch 001: Loss: 1.963, Accuracy: 64.919%\n",
            "Epoch 001: Loss: 1.964, Accuracy: 64.890%\n",
            "Epoch 001: Loss: 1.960, Accuracy: 64.993%\n",
            "Epoch 001: Loss: 1.960, Accuracy: 64.997%\n",
            "Epoch 001: Loss: 1.960, Accuracy: 65.033%\n",
            "Epoch 001: Loss: 1.959, Accuracy: 65.036%\n",
            "Epoch 001: Loss: 1.960, Accuracy: 65.023%\n",
            "Epoch 001: Loss: 1.959, Accuracy: 64.994%\n",
            "Epoch 001: Loss: 1.960, Accuracy: 65.029%\n",
            "Epoch 001: Loss: 1.963, Accuracy: 64.984%\n",
            "Epoch 001: Loss: 1.962, Accuracy: 65.035%\n",
            "Epoch 001: Loss: 1.961, Accuracy: 65.054%\n",
            "Epoch 001: Loss: 1.960, Accuracy: 65.073%\n",
            "Epoch 001: Loss: 1.958, Accuracy: 65.107%\n",
            "Epoch 001: Loss: 1.957, Accuracy: 65.109%\n",
            "Epoch 001: Loss: 1.956, Accuracy: 65.143%\n",
            "Epoch 001: Loss: 1.959, Accuracy: 65.053%\n",
            "Epoch 001: Loss: 1.957, Accuracy: 65.071%\n",
            "Epoch 001: Loss: 1.957, Accuracy: 65.028%\n",
            "Epoch 001: Loss: 1.958, Accuracy: 64.985%\n",
            "Epoch 001: Loss: 1.959, Accuracy: 65.003%\n",
            "Epoch 001: Loss: 1.956, Accuracy: 65.021%\n",
            "Epoch 001: Loss: 1.956, Accuracy: 65.009%\n",
            "Epoch 001: Loss: 1.955, Accuracy: 65.057%\n",
            "Epoch 001: Loss: 1.954, Accuracy: 65.060%\n",
            "Epoch 001: Loss: 1.954, Accuracy: 65.018%\n",
            "Epoch 001: Loss: 1.953, Accuracy: 65.035%\n",
            "Epoch 001: Loss: 1.953, Accuracy: 65.023%\n",
            "Epoch 001: Loss: 1.956, Accuracy: 64.997%\n",
            "Epoch 001: Loss: 1.956, Accuracy: 64.985%\n",
            "Epoch 001: Loss: 1.956, Accuracy: 64.974%\n",
            "Epoch 001: Loss: 1.956, Accuracy: 64.977%\n",
            "Epoch 001: Loss: 1.954, Accuracy: 64.994%\n",
            "Epoch 001: Loss: 1.955, Accuracy: 64.983%\n",
            "Epoch 001: Loss: 1.953, Accuracy: 65.028%\n",
            "Epoch 001: Loss: 1.951, Accuracy: 65.017%\n",
            "Epoch 001: Loss: 1.951, Accuracy: 65.020%\n",
            "Epoch 001: Loss: 1.951, Accuracy: 65.078%\n",
            "Epoch 001: Loss: 1.951, Accuracy: 65.053%\n",
            "Epoch 001: Loss: 1.948, Accuracy: 65.125%\n",
            "Epoch 001: Loss: 1.947, Accuracy: 65.127%\n",
            "Epoch 001: Loss: 1.946, Accuracy: 65.129%\n",
            "Epoch 001: Loss: 1.944, Accuracy: 65.159%\n",
            "Epoch 001: Loss: 1.944, Accuracy: 65.161%\n",
            "Epoch 001: Loss: 1.945, Accuracy: 65.149%\n",
            "Epoch 001: Loss: 1.944, Accuracy: 65.152%\n",
            "Epoch 001: Loss: 1.942, Accuracy: 65.221%\n",
            "Epoch 001: Loss: 1.941, Accuracy: 65.236%\n",
            "Epoch 001: Loss: 1.940, Accuracy: 65.251%\n",
            "Epoch 001: Loss: 1.941, Accuracy: 65.226%\n",
            "Epoch 001: Loss: 1.939, Accuracy: 65.281%\n",
            "Epoch 001: Loss: 1.937, Accuracy: 65.335%\n",
            "Epoch 001: Loss: 1.938, Accuracy: 65.297%\n",
            "Epoch 001: Loss: 1.939, Accuracy: 65.259%\n",
            "Epoch 001: Loss: 1.939, Accuracy: 65.273%\n",
            "Epoch 001: Loss: 1.939, Accuracy: 65.262%\n",
            "Epoch 001: Loss: 1.938, Accuracy: 65.276%\n",
            "Epoch 001: Loss: 1.939, Accuracy: 65.303%\n",
            "Epoch 001: Loss: 1.940, Accuracy: 65.292%\n",
            "Epoch 001: Loss: 1.941, Accuracy: 65.293%\n",
            "Epoch 001: Loss: 1.939, Accuracy: 65.346%\n",
            "Epoch 001: Loss: 1.937, Accuracy: 65.372%\n",
            "Epoch 001: Loss: 1.935, Accuracy: 65.386%\n",
            "Epoch 001: Loss: 1.936, Accuracy: 65.361%\n",
            "Epoch 001: Loss: 1.936, Accuracy: 65.412%\n",
            "Epoch 001: Loss: 1.935, Accuracy: 65.413%\n",
            "Epoch 001: Loss: 1.934, Accuracy: 65.439%\n",
            "Epoch 001: Loss: 1.934, Accuracy: 65.403%\n",
            "Epoch 001: Loss: 1.935, Accuracy: 65.367%\n",
            "Epoch 001: Loss: 1.935, Accuracy: 65.380%\n",
            "Epoch 001: Loss: 1.935, Accuracy: 65.381%\n",
            "Epoch 001: Loss: 1.935, Accuracy: 65.382%\n",
            "Epoch 001: Loss: 1.934, Accuracy: 65.395%\n",
            "Epoch 001: Loss: 1.934, Accuracy: 65.408%\n",
            "Epoch 001: Loss: 1.933, Accuracy: 65.421%\n",
            "Epoch 001: Loss: 1.933, Accuracy: 65.445%\n",
            "Epoch 001: Loss: 1.932, Accuracy: 65.458%\n",
            "Epoch 001: Loss: 1.933, Accuracy: 65.459%\n",
            "Epoch 001: Loss: 1.933, Accuracy: 65.495%\n",
            "Epoch 001: Loss: 1.933, Accuracy: 65.495%\n",
            "Epoch 001: Loss: 1.933, Accuracy: 65.519%\n",
            "Epoch 001: Loss: 1.933, Accuracy: 65.508%\n",
            "Epoch 001: Loss: 1.932, Accuracy: 65.532%\n",
            "Epoch 001: Loss: 1.932, Accuracy: 65.520%\n",
            "Epoch 001: Loss: 1.934, Accuracy: 65.463%\n",
            "Epoch 001: Loss: 1.932, Accuracy: 65.521%\n",
            "Epoch 001: Loss: 1.932, Accuracy: 65.533%\n",
            "Epoch 001: Loss: 1.932, Accuracy: 65.556%\n",
            "Epoch 001: Loss: 1.931, Accuracy: 65.591%\n",
            "Epoch 001: Loss: 1.930, Accuracy: 65.591%\n",
            "Epoch 001: Loss: 1.929, Accuracy: 65.648%\n",
            "Epoch 001: Loss: 1.931, Accuracy: 65.591%\n",
            "Epoch 001: Loss: 1.931, Accuracy: 65.580%\n",
            "Epoch 001: Loss: 1.931, Accuracy: 65.625%\n",
            "Epoch 001: Loss: 1.932, Accuracy: 65.636%\n",
            "Epoch 001: Loss: 1.931, Accuracy: 65.654%\n",
            "Epoch 002: Loss: 1.266, Accuracy: 81.250%\n",
            "Epoch 002: Loss: 1.299, Accuracy: 81.250%\n",
            "Epoch 002: Loss: 1.497, Accuracy: 73.958%\n",
            "Epoch 002: Loss: 1.529, Accuracy: 73.438%\n",
            "Epoch 002: Loss: 1.486, Accuracy: 75.000%\n",
            "Epoch 002: Loss: 1.527, Accuracy: 73.958%\n",
            "Epoch 002: Loss: 1.659, Accuracy: 71.875%\n",
            "Epoch 002: Loss: 1.618, Accuracy: 73.438%\n",
            "Epoch 002: Loss: 1.749, Accuracy: 70.486%\n",
            "Epoch 002: Loss: 1.736, Accuracy: 70.625%\n",
            "Epoch 002: Loss: 1.704, Accuracy: 71.307%\n",
            "Epoch 002: Loss: 1.715, Accuracy: 71.354%\n",
            "Epoch 002: Loss: 1.734, Accuracy: 71.154%\n",
            "Epoch 002: Loss: 1.747, Accuracy: 70.759%\n",
            "Epoch 002: Loss: 1.767, Accuracy: 70.833%\n",
            "Epoch 002: Loss: 1.800, Accuracy: 70.117%\n",
            "Epoch 002: Loss: 1.817, Accuracy: 70.037%\n",
            "Epoch 002: Loss: 1.816, Accuracy: 70.312%\n",
            "Epoch 002: Loss: 1.814, Accuracy: 70.395%\n",
            "Epoch 002: Loss: 1.808, Accuracy: 70.469%\n",
            "Epoch 002: Loss: 1.808, Accuracy: 70.536%\n",
            "Epoch 002: Loss: 1.806, Accuracy: 70.881%\n",
            "Epoch 002: Loss: 1.833, Accuracy: 70.245%\n",
            "Epoch 002: Loss: 1.833, Accuracy: 70.312%\n",
            "Epoch 002: Loss: 1.816, Accuracy: 70.625%\n",
            "Epoch 002: Loss: 1.825, Accuracy: 70.192%\n",
            "Epoch 002: Loss: 1.849, Accuracy: 69.213%\n",
            "Epoch 002: Loss: 1.856, Accuracy: 68.638%\n",
            "Epoch 002: Loss: 1.854, Accuracy: 68.750%\n",
            "Epoch 002: Loss: 1.854, Accuracy: 68.958%\n",
            "Epoch 002: Loss: 1.840, Accuracy: 68.952%\n",
            "Epoch 002: Loss: 1.845, Accuracy: 68.750%\n",
            "Epoch 002: Loss: 1.858, Accuracy: 68.466%\n",
            "Epoch 002: Loss: 1.868, Accuracy: 68.199%\n",
            "Epoch 002: Loss: 1.881, Accuracy: 67.679%\n",
            "Epoch 002: Loss: 1.879, Accuracy: 67.795%\n",
            "Epoch 002: Loss: 1.882, Accuracy: 67.821%\n",
            "Epoch 002: Loss: 1.877, Accuracy: 67.845%\n",
            "Epoch 002: Loss: 1.882, Accuracy: 67.788%\n",
            "Epoch 002: Loss: 1.869, Accuracy: 68.047%\n",
            "Epoch 002: Loss: 1.873, Accuracy: 68.064%\n",
            "Epoch 002: Loss: 1.871, Accuracy: 68.006%\n",
            "Epoch 002: Loss: 1.866, Accuracy: 68.314%\n",
            "Epoch 002: Loss: 1.880, Accuracy: 67.898%\n",
            "Epoch 002: Loss: 1.880, Accuracy: 67.778%\n",
            "Epoch 002: Loss: 1.890, Accuracy: 67.459%\n",
            "Epoch 002: Loss: 1.883, Accuracy: 67.753%\n",
            "Epoch 002: Loss: 1.883, Accuracy: 67.839%\n",
            "Epoch 002: Loss: 1.880, Accuracy: 67.921%\n",
            "Epoch 002: Loss: 1.881, Accuracy: 67.813%\n",
            "Epoch 002: Loss: 1.887, Accuracy: 67.647%\n",
            "Epoch 002: Loss: 1.887, Accuracy: 67.728%\n",
            "Epoch 002: Loss: 1.896, Accuracy: 67.630%\n",
            "Epoch 002: Loss: 1.901, Accuracy: 67.593%\n",
            "Epoch 002: Loss: 1.898, Accuracy: 67.727%\n",
            "Epoch 002: Loss: 1.896, Accuracy: 67.746%\n",
            "Epoch 002: Loss: 1.894, Accuracy: 67.708%\n",
            "Epoch 002: Loss: 1.897, Accuracy: 67.672%\n",
            "Epoch 002: Loss: 1.906, Accuracy: 67.532%\n",
            "Epoch 002: Loss: 1.901, Accuracy: 67.708%\n",
            "Epoch 002: Loss: 1.895, Accuracy: 67.674%\n",
            "Epoch 002: Loss: 1.892, Accuracy: 67.692%\n",
            "Epoch 002: Loss: 1.896, Accuracy: 67.560%\n",
            "Epoch 002: Loss: 1.898, Accuracy: 67.432%\n",
            "Epoch 002: Loss: 1.907, Accuracy: 67.260%\n",
            "Epoch 002: Loss: 1.905, Accuracy: 67.282%\n",
            "Epoch 002: Loss: 1.897, Accuracy: 67.444%\n",
            "Epoch 002: Loss: 1.895, Accuracy: 67.509%\n",
            "Epoch 002: Loss: 1.895, Accuracy: 67.346%\n",
            "Epoch 002: Loss: 1.893, Accuracy: 67.411%\n",
            "Epoch 002: Loss: 1.902, Accuracy: 67.342%\n",
            "Epoch 002: Loss: 1.901, Accuracy: 67.361%\n",
            "Epoch 002: Loss: 1.898, Accuracy: 67.380%\n",
            "Epoch 002: Loss: 1.908, Accuracy: 67.230%\n",
            "Epoch 002: Loss: 1.898, Accuracy: 67.458%\n",
            "Epoch 002: Loss: 1.897, Accuracy: 67.352%\n",
            "Epoch 002: Loss: 1.899, Accuracy: 67.208%\n",
            "Epoch 002: Loss: 1.900, Accuracy: 67.107%\n",
            "Epoch 002: Loss: 1.899, Accuracy: 67.049%\n",
            "Epoch 002: Loss: 1.902, Accuracy: 66.953%\n",
            "Epoch 002: Loss: 1.900, Accuracy: 67.052%\n",
            "Epoch 002: Loss: 1.894, Accuracy: 67.111%\n",
            "Epoch 002: Loss: 1.890, Accuracy: 67.169%\n",
            "Epoch 002: Loss: 1.891, Accuracy: 67.113%\n",
            "Epoch 002: Loss: 1.897, Accuracy: 67.059%\n",
            "Epoch 002: Loss: 1.892, Accuracy: 67.078%\n",
            "Epoch 002: Loss: 1.885, Accuracy: 67.205%\n",
            "Epoch 002: Loss: 1.882, Accuracy: 67.294%\n",
            "Epoch 002: Loss: 1.883, Accuracy: 67.170%\n",
            "Epoch 002: Loss: 1.884, Accuracy: 67.153%\n",
            "Epoch 002: Loss: 1.883, Accuracy: 67.170%\n",
            "Epoch 002: Loss: 1.885, Accuracy: 67.120%\n",
            "Epoch 002: Loss: 1.882, Accuracy: 67.204%\n",
            "Epoch 002: Loss: 1.891, Accuracy: 67.088%\n",
            "Epoch 002: Loss: 1.888, Accuracy: 67.138%\n",
            "Epoch 002: Loss: 1.891, Accuracy: 67.025%\n",
            "Epoch 002: Loss: 1.894, Accuracy: 66.978%\n",
            "Epoch 002: Loss: 1.893, Accuracy: 66.996%\n",
            "Epoch 002: Loss: 1.888, Accuracy: 67.077%\n",
            "Epoch 002: Loss: 1.893, Accuracy: 66.906%\n",
            "Epoch 002: Loss: 1.890, Accuracy: 66.894%\n",
            "Epoch 002: Loss: 1.890, Accuracy: 66.942%\n",
            "Epoch 002: Loss: 1.893, Accuracy: 66.899%\n",
            "Epoch 002: Loss: 1.892, Accuracy: 66.947%\n",
            "Epoch 002: Loss: 1.890, Accuracy: 67.024%\n",
            "Epoch 002: Loss: 1.891, Accuracy: 67.011%\n",
            "Epoch 002: Loss: 1.889, Accuracy: 66.998%\n",
            "Epoch 002: Loss: 1.890, Accuracy: 66.956%\n",
            "Epoch 002: Loss: 1.890, Accuracy: 66.915%\n",
            "Epoch 002: Loss: 1.888, Accuracy: 66.960%\n",
            "Epoch 002: Loss: 1.882, Accuracy: 67.117%\n",
            "Epoch 002: Loss: 1.882, Accuracy: 67.215%\n",
            "Epoch 002: Loss: 1.882, Accuracy: 67.201%\n",
            "Epoch 002: Loss: 1.885, Accuracy: 67.133%\n",
            "Epoch 002: Loss: 1.887, Accuracy: 67.147%\n",
            "Epoch 002: Loss: 1.885, Accuracy: 67.080%\n",
            "Epoch 002: Loss: 1.890, Accuracy: 67.041%\n",
            "Epoch 002: Loss: 1.889, Accuracy: 67.055%\n",
            "Epoch 002: Loss: 1.894, Accuracy: 66.886%\n",
            "Epoch 002: Loss: 1.892, Accuracy: 66.953%\n",
            "Epoch 002: Loss: 1.890, Accuracy: 66.942%\n",
            "Epoch 002: Loss: 1.890, Accuracy: 66.906%\n",
            "Epoch 002: Loss: 1.892, Accuracy: 66.870%\n",
            "Epoch 002: Loss: 1.892, Accuracy: 66.885%\n",
            "Epoch 002: Loss: 1.898, Accuracy: 66.775%\n",
            "Epoch 002: Loss: 1.897, Accuracy: 66.741%\n",
            "Epoch 002: Loss: 1.898, Accuracy: 66.732%\n",
            "Epoch 002: Loss: 1.900, Accuracy: 66.675%\n",
            "Epoch 002: Loss: 1.897, Accuracy: 66.788%\n",
            "Epoch 002: Loss: 1.893, Accuracy: 66.899%\n",
            "Epoch 002: Loss: 1.897, Accuracy: 66.818%\n",
            "Epoch 002: Loss: 1.894, Accuracy: 66.903%\n",
            "Epoch 002: Loss: 1.891, Accuracy: 66.941%\n",
            "Epoch 002: Loss: 1.892, Accuracy: 66.931%\n",
            "Epoch 002: Loss: 1.895, Accuracy: 66.898%\n",
            "Epoch 002: Loss: 1.893, Accuracy: 66.958%\n",
            "Epoch 002: Loss: 1.891, Accuracy: 66.994%\n",
            "Epoch 002: Loss: 1.887, Accuracy: 67.142%\n",
            "Epoch 002: Loss: 1.887, Accuracy: 67.154%\n",
            "Epoch 002: Loss: 1.887, Accuracy: 67.165%\n",
            "Epoch 002: Loss: 1.889, Accuracy: 67.088%\n",
            "Epoch 002: Loss: 1.884, Accuracy: 67.232%\n",
            "Epoch 002: Loss: 1.880, Accuracy: 67.308%\n",
            "Epoch 002: Loss: 1.879, Accuracy: 67.339%\n",
            "Epoch 002: Loss: 1.880, Accuracy: 67.328%\n",
            "Epoch 002: Loss: 1.879, Accuracy: 67.359%\n",
            "Epoch 002: Loss: 1.878, Accuracy: 67.347%\n",
            "Epoch 002: Loss: 1.880, Accuracy: 67.314%\n",
            "Epoch 002: Loss: 1.880, Accuracy: 67.282%\n",
            "Epoch 002: Loss: 1.877, Accuracy: 67.354%\n",
            "Epoch 002: Loss: 1.876, Accuracy: 67.384%\n",
            "Epoch 002: Loss: 1.876, Accuracy: 67.373%\n",
            "Epoch 002: Loss: 1.876, Accuracy: 67.382%\n",
            "Epoch 002: Loss: 1.874, Accuracy: 67.431%\n",
            "Epoch 002: Loss: 1.873, Accuracy: 67.460%\n",
            "Epoch 002: Loss: 1.875, Accuracy: 67.468%\n",
            "Epoch 002: Loss: 1.873, Accuracy: 67.536%\n",
            "Epoch 002: Loss: 1.875, Accuracy: 67.445%\n",
            "Epoch 002: Loss: 1.875, Accuracy: 67.472%\n",
            "Epoch 002: Loss: 1.877, Accuracy: 67.383%\n",
            "Epoch 002: Loss: 1.879, Accuracy: 67.411%\n",
            "Epoch 002: Loss: 1.878, Accuracy: 67.438%\n",
            "Epoch 002: Loss: 1.874, Accuracy: 67.446%\n",
            "Epoch 002: Loss: 1.876, Accuracy: 67.416%\n",
            "Epoch 002: Loss: 1.877, Accuracy: 67.424%\n",
            "Epoch 002: Loss: 1.876, Accuracy: 67.432%\n",
            "Epoch 002: Loss: 1.879, Accuracy: 67.365%\n",
            "Epoch 002: Loss: 1.880, Accuracy: 67.281%\n",
            "Epoch 002: Loss: 1.881, Accuracy: 67.252%\n",
            "Epoch 002: Loss: 1.880, Accuracy: 67.243%\n",
            "Epoch 002: Loss: 1.878, Accuracy: 67.270%\n",
            "Epoch 002: Loss: 1.875, Accuracy: 67.387%\n",
            "Epoch 002: Loss: 1.871, Accuracy: 67.522%\n",
            "Epoch 002: Loss: 1.869, Accuracy: 67.583%\n",
            "Epoch 002: Loss: 1.865, Accuracy: 67.679%\n",
            "Epoch 002: Loss: 1.865, Accuracy: 67.667%\n",
            "Epoch 002: Loss: 1.863, Accuracy: 67.655%\n",
            "Epoch 002: Loss: 1.863, Accuracy: 67.662%\n",
            "Epoch 002: Loss: 1.864, Accuracy: 67.650%\n",
            "Epoch 002: Loss: 1.863, Accuracy: 67.656%\n",
            "Epoch 002: Loss: 1.864, Accuracy: 67.576%\n",
            "Epoch 002: Loss: 1.860, Accuracy: 67.651%\n",
            "Epoch 002: Loss: 1.860, Accuracy: 67.708%\n",
            "Epoch 002: Loss: 1.862, Accuracy: 67.646%\n",
            "Epoch 002: Loss: 1.864, Accuracy: 67.635%\n",
            "Epoch 002: Loss: 1.866, Accuracy: 67.574%\n",
            "Epoch 002: Loss: 1.867, Accuracy: 67.547%\n",
            "Epoch 002: Loss: 1.863, Accuracy: 67.636%\n",
            "Epoch 002: Loss: 1.863, Accuracy: 67.642%\n",
            "Epoch 002: Loss: 1.861, Accuracy: 67.664%\n",
            "Epoch 002: Loss: 1.861, Accuracy: 67.670%\n",
            "Epoch 002: Loss: 1.861, Accuracy: 67.643%\n",
            "Epoch 002: Loss: 1.862, Accuracy: 67.600%\n",
            "Epoch 002: Loss: 1.862, Accuracy: 67.590%\n",
            "Epoch 002: Loss: 1.864, Accuracy: 67.516%\n",
            "Epoch 002: Loss: 1.864, Accuracy: 67.554%\n",
            "Epoch 002: Loss: 1.863, Accuracy: 67.576%\n",
            "Epoch 002: Loss: 1.862, Accuracy: 67.629%\n",
            "Epoch 002: Loss: 1.860, Accuracy: 67.666%\n",
            "Epoch 002: Loss: 1.859, Accuracy: 67.703%\n",
            "Epoch 002: Loss: 1.858, Accuracy: 67.739%\n",
            "Epoch 002: Loss: 1.860, Accuracy: 67.652%\n",
            "Epoch 002: Loss: 1.859, Accuracy: 67.657%\n",
            "Epoch 002: Loss: 1.859, Accuracy: 67.678%\n",
            "Epoch 002: Loss: 1.860, Accuracy: 67.652%\n",
            "Epoch 002: Loss: 1.860, Accuracy: 67.673%\n",
            "Epoch 002: Loss: 1.857, Accuracy: 67.739%\n",
            "Epoch 002: Loss: 1.857, Accuracy: 67.743%\n",
            "Epoch 002: Loss: 1.856, Accuracy: 67.763%\n",
            "Epoch 002: Loss: 1.856, Accuracy: 67.768%\n",
            "Epoch 002: Loss: 1.857, Accuracy: 67.773%\n",
            "Epoch 002: Loss: 1.855, Accuracy: 67.777%\n",
            "Epoch 002: Loss: 1.857, Accuracy: 67.752%\n",
            "Epoch 002: Loss: 1.859, Accuracy: 67.684%\n",
            "Epoch 002: Loss: 1.860, Accuracy: 67.645%\n",
            "Epoch 002: Loss: 1.860, Accuracy: 67.650%\n",
            "Epoch 002: Loss: 1.860, Accuracy: 67.656%\n",
            "Epoch 002: Loss: 1.859, Accuracy: 67.661%\n",
            "Epoch 002: Loss: 1.859, Accuracy: 67.651%\n",
            "Epoch 002: Loss: 1.857, Accuracy: 67.727%\n",
            "Epoch 002: Loss: 1.855, Accuracy: 67.718%\n",
            "Epoch 002: Loss: 1.855, Accuracy: 67.722%\n",
            "Epoch 002: Loss: 1.855, Accuracy: 67.741%\n",
            "Epoch 002: Loss: 1.855, Accuracy: 67.718%\n",
            "Epoch 002: Loss: 1.853, Accuracy: 67.792%\n",
            "Epoch 002: Loss: 1.851, Accuracy: 67.824%\n",
            "Epoch 002: Loss: 1.851, Accuracy: 67.841%\n",
            "Epoch 002: Loss: 1.850, Accuracy: 67.845%\n",
            "Epoch 002: Loss: 1.851, Accuracy: 67.822%\n",
            "Epoch 002: Loss: 1.852, Accuracy: 67.813%\n",
            "Epoch 002: Loss: 1.851, Accuracy: 67.817%\n",
            "Epoch 002: Loss: 1.849, Accuracy: 67.861%\n",
            "Epoch 002: Loss: 1.849, Accuracy: 67.851%\n",
            "Epoch 002: Loss: 1.847, Accuracy: 67.855%\n",
            "Epoch 002: Loss: 1.848, Accuracy: 67.832%\n",
            "Epoch 002: Loss: 1.847, Accuracy: 67.863%\n",
            "Epoch 002: Loss: 1.844, Accuracy: 67.893%\n",
            "Epoch 002: Loss: 1.847, Accuracy: 67.897%\n",
            "Epoch 002: Loss: 1.848, Accuracy: 67.848%\n",
            "Epoch 002: Loss: 1.847, Accuracy: 67.852%\n",
            "Epoch 002: Loss: 1.848, Accuracy: 67.829%\n",
            "Epoch 002: Loss: 1.847, Accuracy: 67.833%\n",
            "Epoch 002: Loss: 1.846, Accuracy: 67.837%\n",
            "Epoch 002: Loss: 1.847, Accuracy: 67.815%\n",
            "Epoch 002: Loss: 1.847, Accuracy: 67.806%\n",
            "Epoch 002: Loss: 1.846, Accuracy: 67.835%\n",
            "Epoch 002: Loss: 1.844, Accuracy: 67.852%\n",
            "Epoch 002: Loss: 1.843, Accuracy: 67.881%\n",
            "Epoch 002: Loss: 1.844, Accuracy: 67.859%\n",
            "Epoch 002: Loss: 1.843, Accuracy: 67.900%\n",
            "Epoch 002: Loss: 1.842, Accuracy: 67.953%\n",
            "Epoch 002: Loss: 1.842, Accuracy: 68.006%\n",
            "Epoch 002: Loss: 1.842, Accuracy: 67.972%\n",
            "Epoch 002: Loss: 1.843, Accuracy: 67.950%\n",
            "Epoch 002: Loss: 1.843, Accuracy: 67.966%\n",
            "Epoch 002: Loss: 1.843, Accuracy: 67.944%\n",
            "Epoch 002: Loss: 1.842, Accuracy: 67.960%\n",
            "Epoch 002: Loss: 1.842, Accuracy: 67.975%\n",
            "Epoch 002: Loss: 1.842, Accuracy: 67.990%\n",
            "Epoch 002: Loss: 1.841, Accuracy: 67.993%\n",
            "Epoch 002: Loss: 1.840, Accuracy: 68.020%\n",
            "Epoch 002: Loss: 1.840, Accuracy: 68.010%\n",
            "Epoch 002: Loss: 1.842, Accuracy: 67.990%\n",
            "Epoch 002: Loss: 1.841, Accuracy: 68.004%\n",
            "Epoch 002: Loss: 1.841, Accuracy: 68.007%\n",
            "Epoch 002: Loss: 1.841, Accuracy: 68.010%\n",
            "Epoch 002: Loss: 1.841, Accuracy: 68.001%\n",
            "Epoch 002: Loss: 1.841, Accuracy: 68.015%\n",
            "Epoch 002: Loss: 1.842, Accuracy: 67.983%\n",
            "Epoch 002: Loss: 1.844, Accuracy: 67.905%\n",
            "Epoch 002: Loss: 1.842, Accuracy: 67.931%\n",
            "Epoch 002: Loss: 1.842, Accuracy: 67.946%\n",
            "Epoch 002: Loss: 1.842, Accuracy: 67.972%\n",
            "Epoch 002: Loss: 1.841, Accuracy: 67.997%\n",
            "Epoch 002: Loss: 1.840, Accuracy: 67.989%\n",
            "Epoch 002: Loss: 1.839, Accuracy: 68.037%\n",
            "Epoch 002: Loss: 1.839, Accuracy: 67.983%\n",
            "Epoch 002: Loss: 1.840, Accuracy: 67.986%\n",
            "Epoch 002: Loss: 1.839, Accuracy: 68.011%\n",
            "Epoch 002: Loss: 1.839, Accuracy: 68.002%\n",
            "Epoch 002: Loss: 1.839, Accuracy: 68.003%\n",
            "Epoch 003: Loss: 1.046, Accuracy: 84.375%\n",
            "Epoch 003: Loss: 1.126, Accuracy: 82.812%\n",
            "Epoch 003: Loss: 1.356, Accuracy: 76.042%\n",
            "Epoch 003: Loss: 1.413, Accuracy: 76.562%\n",
            "Epoch 003: Loss: 1.401, Accuracy: 76.875%\n",
            "Epoch 003: Loss: 1.446, Accuracy: 76.562%\n",
            "Epoch 003: Loss: 1.598, Accuracy: 74.554%\n",
            "Epoch 003: Loss: 1.552, Accuracy: 76.172%\n",
            "Epoch 003: Loss: 1.683, Accuracy: 72.917%\n",
            "Epoch 003: Loss: 1.669, Accuracy: 73.125%\n",
            "Epoch 003: Loss: 1.646, Accuracy: 73.580%\n",
            "Epoch 003: Loss: 1.659, Accuracy: 73.438%\n",
            "Epoch 003: Loss: 1.675, Accuracy: 73.077%\n",
            "Epoch 003: Loss: 1.693, Accuracy: 72.545%\n",
            "Epoch 003: Loss: 1.703, Accuracy: 72.500%\n",
            "Epoch 003: Loss: 1.735, Accuracy: 71.680%\n",
            "Epoch 003: Loss: 1.747, Accuracy: 71.140%\n",
            "Epoch 003: Loss: 1.743, Accuracy: 71.181%\n",
            "Epoch 003: Loss: 1.740, Accuracy: 70.888%\n",
            "Epoch 003: Loss: 1.745, Accuracy: 71.094%\n",
            "Epoch 003: Loss: 1.758, Accuracy: 70.685%\n",
            "Epoch 003: Loss: 1.757, Accuracy: 70.881%\n",
            "Epoch 003: Loss: 1.786, Accuracy: 70.109%\n",
            "Epoch 003: Loss: 1.790, Accuracy: 70.052%\n",
            "Epoch 003: Loss: 1.769, Accuracy: 70.250%\n",
            "Epoch 003: Loss: 1.780, Accuracy: 69.952%\n",
            "Epoch 003: Loss: 1.813, Accuracy: 69.213%\n",
            "Epoch 003: Loss: 1.825, Accuracy: 68.862%\n",
            "Epoch 003: Loss: 1.817, Accuracy: 68.966%\n",
            "Epoch 003: Loss: 1.811, Accuracy: 69.167%\n",
            "Epoch 003: Loss: 1.799, Accuracy: 69.456%\n",
            "Epoch 003: Loss: 1.801, Accuracy: 69.336%\n",
            "Epoch 003: Loss: 1.814, Accuracy: 69.223%\n",
            "Epoch 003: Loss: 1.822, Accuracy: 69.026%\n",
            "Epoch 003: Loss: 1.834, Accuracy: 68.750%\n",
            "Epoch 003: Loss: 1.830, Accuracy: 68.663%\n",
            "Epoch 003: Loss: 1.826, Accuracy: 68.834%\n",
            "Epoch 003: Loss: 1.822, Accuracy: 68.832%\n",
            "Epoch 003: Loss: 1.825, Accuracy: 68.830%\n",
            "Epoch 003: Loss: 1.811, Accuracy: 69.219%\n",
            "Epoch 003: Loss: 1.813, Accuracy: 69.207%\n",
            "Epoch 003: Loss: 1.815, Accuracy: 69.122%\n",
            "Epoch 003: Loss: 1.809, Accuracy: 69.331%\n",
            "Epoch 003: Loss: 1.827, Accuracy: 68.821%\n",
            "Epoch 003: Loss: 1.824, Accuracy: 68.819%\n",
            "Epoch 003: Loss: 1.833, Accuracy: 68.478%\n",
            "Epoch 003: Loss: 1.830, Accuracy: 68.750%\n",
            "Epoch 003: Loss: 1.832, Accuracy: 68.685%\n",
            "Epoch 003: Loss: 1.830, Accuracy: 68.814%\n",
            "Epoch 003: Loss: 1.833, Accuracy: 68.750%\n",
            "Epoch 003: Loss: 1.834, Accuracy: 68.811%\n",
            "Epoch 003: Loss: 1.831, Accuracy: 68.870%\n",
            "Epoch 003: Loss: 1.837, Accuracy: 68.750%\n",
            "Epoch 003: Loss: 1.841, Accuracy: 68.750%\n",
            "Epoch 003: Loss: 1.836, Accuracy: 68.693%\n",
            "Epoch 003: Loss: 1.834, Accuracy: 68.638%\n",
            "Epoch 003: Loss: 1.834, Accuracy: 68.695%\n",
            "Epoch 003: Loss: 1.839, Accuracy: 68.642%\n",
            "Epoch 003: Loss: 1.848, Accuracy: 68.485%\n",
            "Epoch 003: Loss: 1.843, Accuracy: 68.750%\n",
            "Epoch 003: Loss: 1.837, Accuracy: 68.955%\n",
            "Epoch 003: Loss: 1.834, Accuracy: 68.901%\n",
            "Epoch 003: Loss: 1.842, Accuracy: 68.750%\n",
            "Epoch 003: Loss: 1.842, Accuracy: 68.701%\n",
            "Epoch 003: Loss: 1.850, Accuracy: 68.510%\n",
            "Epoch 003: Loss: 1.849, Accuracy: 68.561%\n",
            "Epoch 003: Loss: 1.842, Accuracy: 68.703%\n",
            "Epoch 003: Loss: 1.839, Accuracy: 68.658%\n",
            "Epoch 003: Loss: 1.839, Accuracy: 68.478%\n",
            "Epoch 003: Loss: 1.837, Accuracy: 68.437%\n",
            "Epoch 003: Loss: 1.842, Accuracy: 68.178%\n",
            "Epoch 003: Loss: 1.842, Accuracy: 68.316%\n",
            "Epoch 003: Loss: 1.838, Accuracy: 68.236%\n",
            "Epoch 003: Loss: 1.845, Accuracy: 68.159%\n",
            "Epoch 003: Loss: 1.836, Accuracy: 68.375%\n",
            "Epoch 003: Loss: 1.833, Accuracy: 68.339%\n",
            "Epoch 003: Loss: 1.834, Accuracy: 68.304%\n",
            "Epoch 003: Loss: 1.834, Accuracy: 68.149%\n",
            "Epoch 003: Loss: 1.835, Accuracy: 68.157%\n",
            "Epoch 003: Loss: 1.838, Accuracy: 68.008%\n",
            "Epoch 003: Loss: 1.837, Accuracy: 68.094%\n",
            "Epoch 003: Loss: 1.832, Accuracy: 68.140%\n",
            "Epoch 003: Loss: 1.829, Accuracy: 68.261%\n",
            "Epoch 003: Loss: 1.830, Accuracy: 68.192%\n",
            "Epoch 003: Loss: 1.837, Accuracy: 68.088%\n",
            "Epoch 003: Loss: 1.832, Accuracy: 68.241%\n",
            "Epoch 003: Loss: 1.827, Accuracy: 68.355%\n",
            "Epoch 003: Loss: 1.825, Accuracy: 68.395%\n",
            "Epoch 003: Loss: 1.827, Accuracy: 68.258%\n",
            "Epoch 003: Loss: 1.827, Accuracy: 68.264%\n",
            "Epoch 003: Loss: 1.826, Accuracy: 68.269%\n",
            "Epoch 003: Loss: 1.827, Accuracy: 68.207%\n",
            "Epoch 003: Loss: 1.823, Accuracy: 68.246%\n",
            "Epoch 003: Loss: 1.831, Accuracy: 68.085%\n",
            "Epoch 003: Loss: 1.829, Accuracy: 68.158%\n",
            "Epoch 003: Loss: 1.835, Accuracy: 68.001%\n",
            "Epoch 003: Loss: 1.837, Accuracy: 67.977%\n",
            "Epoch 003: Loss: 1.836, Accuracy: 67.985%\n",
            "Epoch 003: Loss: 1.830, Accuracy: 68.056%\n",
            "Epoch 003: Loss: 1.835, Accuracy: 68.000%\n",
            "Epoch 003: Loss: 1.832, Accuracy: 68.069%\n",
            "Epoch 003: Loss: 1.834, Accuracy: 68.015%\n",
            "Epoch 003: Loss: 1.838, Accuracy: 67.931%\n",
            "Epoch 003: Loss: 1.837, Accuracy: 67.999%\n",
            "Epoch 003: Loss: 1.837, Accuracy: 68.065%\n",
            "Epoch 003: Loss: 1.837, Accuracy: 68.072%\n",
            "Epoch 003: Loss: 1.835, Accuracy: 68.078%\n",
            "Epoch 003: Loss: 1.836, Accuracy: 68.027%\n",
            "Epoch 003: Loss: 1.835, Accuracy: 68.062%\n",
            "Epoch 003: Loss: 1.833, Accuracy: 68.097%\n",
            "Epoch 003: Loss: 1.829, Accuracy: 68.243%\n",
            "Epoch 003: Loss: 1.826, Accuracy: 68.192%\n",
            "Epoch 003: Loss: 1.825, Accuracy: 68.142%\n",
            "Epoch 003: Loss: 1.828, Accuracy: 68.037%\n",
            "Epoch 003: Loss: 1.829, Accuracy: 67.989%\n",
            "Epoch 003: Loss: 1.829, Accuracy: 68.050%\n",
            "Epoch 003: Loss: 1.832, Accuracy: 67.922%\n",
            "Epoch 003: Loss: 1.831, Accuracy: 67.982%\n",
            "Epoch 003: Loss: 1.836, Accuracy: 67.857%\n",
            "Epoch 003: Loss: 1.834, Accuracy: 67.865%\n",
            "Epoch 003: Loss: 1.831, Accuracy: 67.872%\n",
            "Epoch 003: Loss: 1.832, Accuracy: 67.879%\n",
            "Epoch 003: Loss: 1.833, Accuracy: 67.810%\n",
            "Epoch 003: Loss: 1.833, Accuracy: 67.843%\n",
            "Epoch 003: Loss: 1.839, Accuracy: 67.750%\n",
            "Epoch 003: Loss: 1.838, Accuracy: 67.733%\n",
            "Epoch 003: Loss: 1.839, Accuracy: 67.692%\n",
            "Epoch 003: Loss: 1.841, Accuracy: 67.627%\n",
            "Epoch 003: Loss: 1.837, Accuracy: 67.733%\n",
            "Epoch 003: Loss: 1.834, Accuracy: 67.788%\n",
            "Epoch 003: Loss: 1.835, Accuracy: 67.724%\n",
            "Epoch 003: Loss: 1.832, Accuracy: 67.827%\n",
            "Epoch 003: Loss: 1.830, Accuracy: 67.881%\n",
            "Epoch 003: Loss: 1.830, Accuracy: 67.887%\n",
            "Epoch 003: Loss: 1.834, Accuracy: 67.847%\n",
            "Epoch 003: Loss: 1.833, Accuracy: 67.877%\n",
            "Epoch 003: Loss: 1.830, Accuracy: 67.929%\n",
            "Epoch 003: Loss: 1.826, Accuracy: 68.071%\n",
            "Epoch 003: Loss: 1.825, Accuracy: 68.053%\n",
            "Epoch 003: Loss: 1.825, Accuracy: 68.036%\n",
            "Epoch 003: Loss: 1.825, Accuracy: 67.996%\n",
            "Epoch 003: Loss: 1.821, Accuracy: 68.090%\n",
            "Epoch 003: Loss: 1.818, Accuracy: 68.116%\n",
            "Epoch 003: Loss: 1.817, Accuracy: 68.121%\n",
            "Epoch 003: Loss: 1.818, Accuracy: 68.082%\n",
            "Epoch 003: Loss: 1.818, Accuracy: 68.129%\n",
            "Epoch 003: Loss: 1.819, Accuracy: 68.134%\n",
            "Epoch 003: Loss: 1.820, Accuracy: 68.117%\n",
            "Epoch 003: Loss: 1.821, Accuracy: 68.121%\n",
            "Epoch 003: Loss: 1.819, Accuracy: 68.208%\n",
            "Epoch 003: Loss: 1.818, Accuracy: 68.212%\n",
            "Epoch 003: Loss: 1.818, Accuracy: 68.174%\n",
            "Epoch 003: Loss: 1.817, Accuracy: 68.137%\n",
            "Epoch 003: Loss: 1.815, Accuracy: 68.202%\n",
            "Epoch 003: Loss: 1.814, Accuracy: 68.246%\n",
            "Epoch 003: Loss: 1.816, Accuracy: 68.209%\n",
            "Epoch 003: Loss: 1.814, Accuracy: 68.252%\n",
            "Epoch 003: Loss: 1.817, Accuracy: 68.216%\n",
            "Epoch 003: Loss: 1.817, Accuracy: 68.200%\n",
            "Epoch 003: Loss: 1.818, Accuracy: 68.125%\n",
            "Epoch 003: Loss: 1.820, Accuracy: 68.129%\n",
            "Epoch 003: Loss: 1.818, Accuracy: 68.133%\n",
            "Epoch 003: Loss: 1.816, Accuracy: 68.175%\n",
            "Epoch 003: Loss: 1.817, Accuracy: 68.159%\n",
            "Epoch 003: Loss: 1.817, Accuracy: 68.201%\n",
            "Epoch 003: Loss: 1.817, Accuracy: 68.223%\n",
            "Epoch 003: Loss: 1.820, Accuracy: 68.170%\n",
            "Epoch 003: Loss: 1.822, Accuracy: 68.099%\n",
            "Epoch 003: Loss: 1.823, Accuracy: 68.103%\n",
            "Epoch 003: Loss: 1.823, Accuracy: 68.125%\n",
            "Epoch 003: Loss: 1.822, Accuracy: 68.147%\n",
            "Epoch 003: Loss: 1.819, Accuracy: 68.241%\n",
            "Epoch 003: Loss: 1.815, Accuracy: 68.371%\n",
            "Epoch 003: Loss: 1.812, Accuracy: 68.427%\n",
            "Epoch 003: Loss: 1.808, Accuracy: 68.500%\n",
            "Epoch 003: Loss: 1.808, Accuracy: 68.519%\n",
            "Epoch 003: Loss: 1.806, Accuracy: 68.538%\n",
            "Epoch 003: Loss: 1.806, Accuracy: 68.539%\n",
            "Epoch 003: Loss: 1.809, Accuracy: 68.523%\n",
            "Epoch 003: Loss: 1.808, Accuracy: 68.507%\n",
            "Epoch 003: Loss: 1.809, Accuracy: 68.456%\n",
            "Epoch 003: Loss: 1.805, Accuracy: 68.544%\n",
            "Epoch 003: Loss: 1.804, Accuracy: 68.579%\n",
            "Epoch 003: Loss: 1.805, Accuracy: 68.546%\n",
            "Epoch 003: Loss: 1.806, Accuracy: 68.514%\n",
            "Epoch 003: Loss: 1.809, Accuracy: 68.431%\n",
            "Epoch 003: Loss: 1.809, Accuracy: 68.382%\n",
            "Epoch 003: Loss: 1.806, Accuracy: 68.484%\n",
            "Epoch 003: Loss: 1.807, Accuracy: 68.452%\n",
            "Epoch 003: Loss: 1.806, Accuracy: 68.503%\n",
            "Epoch 003: Loss: 1.805, Accuracy: 68.505%\n",
            "Epoch 003: Loss: 1.806, Accuracy: 68.473%\n",
            "Epoch 003: Loss: 1.807, Accuracy: 68.442%\n",
            "Epoch 003: Loss: 1.807, Accuracy: 68.460%\n",
            "Epoch 003: Loss: 1.810, Accuracy: 68.413%\n",
            "Epoch 003: Loss: 1.809, Accuracy: 68.431%\n",
            "Epoch 003: Loss: 1.809, Accuracy: 68.449%\n",
            "Epoch 003: Loss: 1.808, Accuracy: 68.482%\n",
            "Epoch 003: Loss: 1.807, Accuracy: 68.514%\n",
            "Epoch 003: Loss: 1.806, Accuracy: 68.547%\n",
            "Epoch 003: Loss: 1.805, Accuracy: 68.595%\n",
            "Epoch 003: Loss: 1.807, Accuracy: 68.518%\n",
            "Epoch 003: Loss: 1.806, Accuracy: 68.534%\n",
            "Epoch 003: Loss: 1.805, Accuracy: 68.581%\n",
            "Epoch 003: Loss: 1.806, Accuracy: 68.552%\n",
            "Epoch 003: Loss: 1.806, Accuracy: 68.583%\n",
            "Epoch 003: Loss: 1.803, Accuracy: 68.629%\n",
            "Epoch 003: Loss: 1.803, Accuracy: 68.615%\n",
            "Epoch 003: Loss: 1.802, Accuracy: 68.615%\n",
            "Epoch 003: Loss: 1.802, Accuracy: 68.631%\n",
            "Epoch 003: Loss: 1.802, Accuracy: 68.632%\n",
            "Epoch 003: Loss: 1.801, Accuracy: 68.647%\n",
            "Epoch 003: Loss: 1.801, Accuracy: 68.603%\n",
            "Epoch 003: Loss: 1.804, Accuracy: 68.560%\n",
            "Epoch 003: Loss: 1.805, Accuracy: 68.547%\n",
            "Epoch 003: Loss: 1.804, Accuracy: 68.533%\n",
            "Epoch 003: Loss: 1.804, Accuracy: 68.534%\n",
            "Epoch 003: Loss: 1.803, Accuracy: 68.535%\n",
            "Epoch 003: Loss: 1.803, Accuracy: 68.550%\n",
            "Epoch 003: Loss: 1.801, Accuracy: 68.580%\n",
            "Epoch 003: Loss: 1.800, Accuracy: 68.566%\n",
            "Epoch 003: Loss: 1.800, Accuracy: 68.567%\n",
            "Epoch 003: Loss: 1.799, Accuracy: 68.610%\n",
            "Epoch 003: Loss: 1.799, Accuracy: 68.610%\n",
            "Epoch 003: Loss: 1.796, Accuracy: 68.681%\n",
            "Epoch 003: Loss: 1.796, Accuracy: 68.681%\n",
            "Epoch 003: Loss: 1.794, Accuracy: 68.695%\n",
            "Epoch 003: Loss: 1.794, Accuracy: 68.723%\n",
            "Epoch 003: Loss: 1.794, Accuracy: 68.695%\n",
            "Epoch 003: Loss: 1.795, Accuracy: 68.682%\n",
            "Epoch 003: Loss: 1.795, Accuracy: 68.682%\n",
            "Epoch 003: Loss: 1.793, Accuracy: 68.737%\n",
            "Epoch 003: Loss: 1.792, Accuracy: 68.750%\n",
            "Epoch 003: Loss: 1.791, Accuracy: 68.763%\n",
            "Epoch 003: Loss: 1.792, Accuracy: 68.750%\n",
            "Epoch 003: Loss: 1.791, Accuracy: 68.790%\n",
            "Epoch 003: Loss: 1.789, Accuracy: 68.816%\n",
            "Epoch 003: Loss: 1.790, Accuracy: 68.803%\n",
            "Epoch 003: Loss: 1.790, Accuracy: 68.763%\n",
            "Epoch 003: Loss: 1.790, Accuracy: 68.763%\n",
            "Epoch 003: Loss: 1.790, Accuracy: 68.724%\n",
            "Epoch 003: Loss: 1.791, Accuracy: 68.737%\n",
            "Epoch 003: Loss: 1.791, Accuracy: 68.737%\n",
            "Epoch 003: Loss: 1.791, Accuracy: 68.724%\n",
            "Epoch 003: Loss: 1.792, Accuracy: 68.724%\n",
            "Epoch 003: Loss: 1.791, Accuracy: 68.763%\n",
            "Epoch 003: Loss: 1.789, Accuracy: 68.763%\n",
            "Epoch 003: Loss: 1.788, Accuracy: 68.775%\n",
            "Epoch 003: Loss: 1.789, Accuracy: 68.750%\n",
            "Epoch 003: Loss: 1.787, Accuracy: 68.750%\n",
            "Epoch 003: Loss: 1.786, Accuracy: 68.775%\n",
            "Epoch 003: Loss: 1.786, Accuracy: 68.800%\n",
            "Epoch 003: Loss: 1.787, Accuracy: 68.762%\n",
            "Epoch 003: Loss: 1.788, Accuracy: 68.725%\n",
            "Epoch 003: Loss: 1.788, Accuracy: 68.750%\n",
            "Epoch 003: Loss: 1.788, Accuracy: 68.750%\n",
            "Epoch 003: Loss: 1.788, Accuracy: 68.750%\n",
            "Epoch 003: Loss: 1.788, Accuracy: 68.762%\n",
            "Epoch 003: Loss: 1.788, Accuracy: 68.750%\n",
            "Epoch 003: Loss: 1.788, Accuracy: 68.750%\n",
            "Epoch 003: Loss: 1.787, Accuracy: 68.762%\n",
            "Epoch 003: Loss: 1.786, Accuracy: 68.726%\n",
            "Epoch 003: Loss: 1.788, Accuracy: 68.714%\n",
            "Epoch 003: Loss: 1.788, Accuracy: 68.726%\n",
            "Epoch 003: Loss: 1.787, Accuracy: 68.750%\n",
            "Epoch 003: Loss: 1.787, Accuracy: 68.773%\n",
            "Epoch 003: Loss: 1.787, Accuracy: 68.762%\n",
            "Epoch 003: Loss: 1.787, Accuracy: 68.797%\n",
            "Epoch 003: Loss: 1.788, Accuracy: 68.762%\n",
            "Epoch 003: Loss: 1.790, Accuracy: 68.715%\n",
            "Epoch 003: Loss: 1.788, Accuracy: 68.762%\n",
            "Epoch 003: Loss: 1.788, Accuracy: 68.761%\n",
            "Epoch 003: Loss: 1.787, Accuracy: 68.750%\n",
            "Epoch 003: Loss: 1.787, Accuracy: 68.773%\n",
            "Epoch 003: Loss: 1.786, Accuracy: 68.773%\n",
            "Epoch 003: Loss: 1.785, Accuracy: 68.818%\n",
            "Epoch 003: Loss: 1.787, Accuracy: 68.739%\n",
            "Epoch 003: Loss: 1.787, Accuracy: 68.728%\n",
            "Epoch 003: Loss: 1.787, Accuracy: 68.750%\n",
            "Epoch 003: Loss: 1.787, Accuracy: 68.750%\n",
            "Epoch 003: Loss: 1.787, Accuracy: 68.771%\n",
            "Epoch 004: Loss: 1.301, Accuracy: 84.375%\n",
            "Epoch 004: Loss: 1.266, Accuracy: 82.812%\n",
            "Epoch 004: Loss: 1.547, Accuracy: 75.000%\n",
            "Epoch 004: Loss: 1.568, Accuracy: 75.781%\n",
            "Epoch 004: Loss: 1.527, Accuracy: 76.875%\n",
            "Epoch 004: Loss: 1.525, Accuracy: 75.521%\n",
            "Epoch 004: Loss: 1.629, Accuracy: 73.214%\n",
            "Epoch 004: Loss: 1.579, Accuracy: 74.609%\n",
            "Epoch 004: Loss: 1.686, Accuracy: 71.528%\n",
            "Epoch 004: Loss: 1.667, Accuracy: 71.562%\n",
            "Epoch 004: Loss: 1.638, Accuracy: 72.159%\n",
            "Epoch 004: Loss: 1.638, Accuracy: 71.875%\n",
            "Epoch 004: Loss: 1.643, Accuracy: 71.394%\n",
            "Epoch 004: Loss: 1.641, Accuracy: 71.205%\n",
            "Epoch 004: Loss: 1.638, Accuracy: 71.250%\n",
            "Epoch 004: Loss: 1.670, Accuracy: 70.508%\n",
            "Epoch 004: Loss: 1.687, Accuracy: 70.221%\n",
            "Epoch 004: Loss: 1.677, Accuracy: 70.486%\n",
            "Epoch 004: Loss: 1.679, Accuracy: 70.395%\n",
            "Epoch 004: Loss: 1.678, Accuracy: 70.625%\n",
            "Epoch 004: Loss: 1.681, Accuracy: 70.238%\n",
            "Epoch 004: Loss: 1.674, Accuracy: 70.455%\n",
            "Epoch 004: Loss: 1.703, Accuracy: 69.837%\n",
            "Epoch 004: Loss: 1.707, Accuracy: 70.052%\n",
            "Epoch 004: Loss: 1.690, Accuracy: 70.125%\n",
            "Epoch 004: Loss: 1.709, Accuracy: 69.952%\n",
            "Epoch 004: Loss: 1.732, Accuracy: 69.097%\n",
            "Epoch 004: Loss: 1.740, Accuracy: 68.750%\n",
            "Epoch 004: Loss: 1.739, Accuracy: 68.966%\n",
            "Epoch 004: Loss: 1.738, Accuracy: 69.167%\n",
            "Epoch 004: Loss: 1.722, Accuracy: 69.456%\n",
            "Epoch 004: Loss: 1.723, Accuracy: 69.531%\n",
            "Epoch 004: Loss: 1.739, Accuracy: 69.413%\n",
            "Epoch 004: Loss: 1.752, Accuracy: 69.301%\n",
            "Epoch 004: Loss: 1.769, Accuracy: 69.018%\n",
            "Epoch 004: Loss: 1.770, Accuracy: 69.097%\n",
            "Epoch 004: Loss: 1.769, Accuracy: 69.088%\n",
            "Epoch 004: Loss: 1.767, Accuracy: 69.243%\n",
            "Epoch 004: Loss: 1.769, Accuracy: 69.311%\n",
            "Epoch 004: Loss: 1.756, Accuracy: 69.531%\n",
            "Epoch 004: Loss: 1.756, Accuracy: 69.512%\n",
            "Epoch 004: Loss: 1.761, Accuracy: 69.494%\n",
            "Epoch 004: Loss: 1.760, Accuracy: 69.767%\n",
            "Epoch 004: Loss: 1.777, Accuracy: 69.176%\n",
            "Epoch 004: Loss: 1.775, Accuracy: 69.167%\n",
            "Epoch 004: Loss: 1.784, Accuracy: 68.886%\n",
            "Epoch 004: Loss: 1.775, Accuracy: 69.149%\n",
            "Epoch 004: Loss: 1.775, Accuracy: 69.141%\n",
            "Epoch 004: Loss: 1.771, Accuracy: 69.133%\n",
            "Epoch 004: Loss: 1.773, Accuracy: 69.063%\n",
            "Epoch 004: Loss: 1.776, Accuracy: 69.179%\n",
            "Epoch 004: Loss: 1.779, Accuracy: 69.351%\n",
            "Epoch 004: Loss: 1.786, Accuracy: 69.163%\n",
            "Epoch 004: Loss: 1.788, Accuracy: 69.097%\n",
            "Epoch 004: Loss: 1.784, Accuracy: 69.148%\n",
            "Epoch 004: Loss: 1.782, Accuracy: 69.085%\n",
            "Epoch 004: Loss: 1.783, Accuracy: 69.079%\n",
            "Epoch 004: Loss: 1.782, Accuracy: 68.966%\n",
            "Epoch 004: Loss: 1.795, Accuracy: 68.750%\n",
            "Epoch 004: Loss: 1.786, Accuracy: 68.958%\n",
            "Epoch 004: Loss: 1.780, Accuracy: 69.211%\n",
            "Epoch 004: Loss: 1.778, Accuracy: 69.103%\n",
            "Epoch 004: Loss: 1.782, Accuracy: 68.948%\n",
            "Epoch 004: Loss: 1.786, Accuracy: 68.945%\n",
            "Epoch 004: Loss: 1.795, Accuracy: 68.654%\n",
            "Epoch 004: Loss: 1.794, Accuracy: 68.797%\n",
            "Epoch 004: Loss: 1.786, Accuracy: 68.890%\n",
            "Epoch 004: Loss: 1.785, Accuracy: 68.934%\n",
            "Epoch 004: Loss: 1.788, Accuracy: 68.886%\n",
            "Epoch 004: Loss: 1.788, Accuracy: 68.884%\n",
            "Epoch 004: Loss: 1.799, Accuracy: 68.750%\n",
            "Epoch 004: Loss: 1.797, Accuracy: 68.750%\n",
            "Epoch 004: Loss: 1.795, Accuracy: 68.750%\n",
            "Epoch 004: Loss: 1.803, Accuracy: 68.666%\n",
            "Epoch 004: Loss: 1.794, Accuracy: 68.958%\n",
            "Epoch 004: Loss: 1.795, Accuracy: 68.956%\n",
            "Epoch 004: Loss: 1.797, Accuracy: 68.953%\n",
            "Epoch 004: Loss: 1.799, Accuracy: 68.790%\n",
            "Epoch 004: Loss: 1.799, Accuracy: 68.710%\n",
            "Epoch 004: Loss: 1.802, Accuracy: 68.594%\n",
            "Epoch 004: Loss: 1.798, Accuracy: 68.634%\n",
            "Epoch 004: Loss: 1.794, Accuracy: 68.712%\n",
            "Epoch 004: Loss: 1.790, Accuracy: 68.788%\n",
            "Epoch 004: Loss: 1.793, Accuracy: 68.787%\n",
            "Epoch 004: Loss: 1.799, Accuracy: 68.640%\n",
            "Epoch 004: Loss: 1.795, Accuracy: 68.750%\n",
            "Epoch 004: Loss: 1.791, Accuracy: 68.858%\n",
            "Epoch 004: Loss: 1.786, Accuracy: 68.928%\n",
            "Epoch 004: Loss: 1.787, Accuracy: 68.820%\n",
            "Epoch 004: Loss: 1.788, Accuracy: 68.785%\n",
            "Epoch 004: Loss: 1.788, Accuracy: 68.853%\n",
            "Epoch 004: Loss: 1.792, Accuracy: 68.750%\n",
            "Epoch 004: Loss: 1.787, Accuracy: 68.817%\n",
            "Epoch 004: Loss: 1.795, Accuracy: 68.684%\n",
            "Epoch 004: Loss: 1.795, Accuracy: 68.750%\n",
            "Epoch 004: Loss: 1.799, Accuracy: 68.620%\n",
            "Epoch 004: Loss: 1.802, Accuracy: 68.557%\n",
            "Epoch 004: Loss: 1.803, Accuracy: 68.559%\n",
            "Epoch 004: Loss: 1.798, Accuracy: 68.624%\n",
            "Epoch 004: Loss: 1.803, Accuracy: 68.531%\n",
            "Epoch 004: Loss: 1.801, Accuracy: 68.533%\n",
            "Epoch 004: Loss: 1.802, Accuracy: 68.444%\n",
            "Epoch 004: Loss: 1.806, Accuracy: 68.356%\n",
            "Epoch 004: Loss: 1.805, Accuracy: 68.389%\n",
            "Epoch 004: Loss: 1.802, Accuracy: 68.423%\n",
            "Epoch 004: Loss: 1.804, Accuracy: 68.396%\n",
            "Epoch 004: Loss: 1.803, Accuracy: 68.429%\n",
            "Epoch 004: Loss: 1.802, Accuracy: 68.519%\n",
            "Epoch 004: Loss: 1.802, Accuracy: 68.549%\n",
            "Epoch 004: Loss: 1.801, Accuracy: 68.580%\n",
            "Epoch 004: Loss: 1.795, Accuracy: 68.666%\n",
            "Epoch 004: Loss: 1.791, Accuracy: 68.750%\n",
            "Epoch 004: Loss: 1.791, Accuracy: 68.695%\n",
            "Epoch 004: Loss: 1.795, Accuracy: 68.613%\n",
            "Epoch 004: Loss: 1.796, Accuracy: 68.587%\n",
            "Epoch 004: Loss: 1.797, Accuracy: 68.642%\n",
            "Epoch 004: Loss: 1.802, Accuracy: 68.563%\n",
            "Epoch 004: Loss: 1.800, Accuracy: 68.565%\n",
            "Epoch 004: Loss: 1.804, Accuracy: 68.435%\n",
            "Epoch 004: Loss: 1.802, Accuracy: 68.464%\n",
            "Epoch 004: Loss: 1.799, Accuracy: 68.492%\n",
            "Epoch 004: Loss: 1.799, Accuracy: 68.494%\n",
            "Epoch 004: Loss: 1.801, Accuracy: 68.445%\n",
            "Epoch 004: Loss: 1.800, Accuracy: 68.473%\n",
            "Epoch 004: Loss: 1.805, Accuracy: 68.350%\n",
            "Epoch 004: Loss: 1.804, Accuracy: 68.279%\n",
            "Epoch 004: Loss: 1.805, Accuracy: 68.258%\n",
            "Epoch 004: Loss: 1.805, Accuracy: 68.237%\n",
            "Epoch 004: Loss: 1.802, Accuracy: 68.338%\n",
            "Epoch 004: Loss: 1.799, Accuracy: 68.437%\n",
            "Epoch 004: Loss: 1.801, Accuracy: 68.392%\n",
            "Epoch 004: Loss: 1.798, Accuracy: 68.466%\n",
            "Epoch 004: Loss: 1.797, Accuracy: 68.492%\n",
            "Epoch 004: Loss: 1.797, Accuracy: 68.517%\n",
            "Epoch 004: Loss: 1.801, Accuracy: 68.519%\n",
            "Epoch 004: Loss: 1.798, Accuracy: 68.566%\n",
            "Epoch 004: Loss: 1.796, Accuracy: 68.636%\n",
            "Epoch 004: Loss: 1.792, Accuracy: 68.750%\n",
            "Epoch 004: Loss: 1.790, Accuracy: 68.817%\n",
            "Epoch 004: Loss: 1.790, Accuracy: 68.795%\n",
            "Epoch 004: Loss: 1.791, Accuracy: 68.728%\n",
            "Epoch 004: Loss: 1.787, Accuracy: 68.838%\n",
            "Epoch 004: Loss: 1.785, Accuracy: 68.925%\n",
            "Epoch 004: Loss: 1.784, Accuracy: 68.967%\n",
            "Epoch 004: Loss: 1.784, Accuracy: 68.944%\n",
            "Epoch 004: Loss: 1.783, Accuracy: 68.985%\n",
            "Epoch 004: Loss: 1.785, Accuracy: 69.005%\n",
            "Epoch 004: Loss: 1.787, Accuracy: 68.898%\n",
            "Epoch 004: Loss: 1.789, Accuracy: 68.897%\n",
            "Epoch 004: Loss: 1.786, Accuracy: 69.000%\n",
            "Epoch 004: Loss: 1.785, Accuracy: 69.019%\n",
            "Epoch 004: Loss: 1.786, Accuracy: 69.017%\n",
            "Epoch 004: Loss: 1.786, Accuracy: 68.995%\n",
            "Epoch 004: Loss: 1.785, Accuracy: 69.054%\n",
            "Epoch 004: Loss: 1.784, Accuracy: 69.093%\n",
            "Epoch 004: Loss: 1.785, Accuracy: 69.071%\n",
            "Epoch 004: Loss: 1.784, Accuracy: 69.128%\n",
            "Epoch 004: Loss: 1.787, Accuracy: 69.047%\n",
            "Epoch 004: Loss: 1.787, Accuracy: 69.045%\n",
            "Epoch 004: Loss: 1.789, Accuracy: 68.965%\n",
            "Epoch 004: Loss: 1.791, Accuracy: 68.964%\n",
            "Epoch 004: Loss: 1.790, Accuracy: 68.962%\n",
            "Epoch 004: Loss: 1.787, Accuracy: 68.961%\n",
            "Epoch 004: Loss: 1.789, Accuracy: 68.941%\n",
            "Epoch 004: Loss: 1.790, Accuracy: 68.939%\n",
            "Epoch 004: Loss: 1.790, Accuracy: 68.938%\n",
            "Epoch 004: Loss: 1.793, Accuracy: 68.881%\n",
            "Epoch 004: Loss: 1.795, Accuracy: 68.787%\n",
            "Epoch 004: Loss: 1.797, Accuracy: 68.805%\n",
            "Epoch 004: Loss: 1.796, Accuracy: 68.824%\n",
            "Epoch 004: Loss: 1.795, Accuracy: 68.823%\n",
            "Epoch 004: Loss: 1.791, Accuracy: 68.914%\n",
            "Epoch 004: Loss: 1.788, Accuracy: 69.039%\n",
            "Epoch 004: Loss: 1.785, Accuracy: 69.109%\n",
            "Epoch 004: Loss: 1.782, Accuracy: 69.161%\n",
            "Epoch 004: Loss: 1.781, Accuracy: 69.194%\n",
            "Epoch 004: Loss: 1.781, Accuracy: 69.209%\n",
            "Epoch 004: Loss: 1.781, Accuracy: 69.206%\n",
            "Epoch 004: Loss: 1.782, Accuracy: 69.204%\n",
            "Epoch 004: Loss: 1.782, Accuracy: 69.201%\n",
            "Epoch 004: Loss: 1.784, Accuracy: 69.147%\n",
            "Epoch 004: Loss: 1.780, Accuracy: 69.265%\n",
            "Epoch 004: Loss: 1.779, Accuracy: 69.314%\n",
            "Epoch 004: Loss: 1.780, Accuracy: 69.276%\n",
            "Epoch 004: Loss: 1.782, Accuracy: 69.206%\n",
            "Epoch 004: Loss: 1.783, Accuracy: 69.153%\n",
            "Epoch 004: Loss: 1.784, Accuracy: 69.118%\n",
            "Epoch 004: Loss: 1.780, Accuracy: 69.215%\n",
            "Epoch 004: Loss: 1.780, Accuracy: 69.213%\n",
            "Epoch 004: Loss: 1.778, Accuracy: 69.211%\n",
            "Epoch 004: Loss: 1.778, Accuracy: 69.208%\n",
            "Epoch 004: Loss: 1.779, Accuracy: 69.173%\n",
            "Epoch 004: Loss: 1.780, Accuracy: 69.155%\n",
            "Epoch 004: Loss: 1.781, Accuracy: 69.137%\n",
            "Epoch 004: Loss: 1.783, Accuracy: 69.087%\n",
            "Epoch 004: Loss: 1.781, Accuracy: 69.117%\n",
            "Epoch 004: Loss: 1.782, Accuracy: 69.131%\n",
            "Epoch 004: Loss: 1.781, Accuracy: 69.160%\n",
            "Epoch 004: Loss: 1.780, Accuracy: 69.174%\n",
            "Epoch 004: Loss: 1.779, Accuracy: 69.219%\n",
            "Epoch 004: Loss: 1.778, Accuracy: 69.232%\n",
            "Epoch 004: Loss: 1.780, Accuracy: 69.137%\n",
            "Epoch 004: Loss: 1.779, Accuracy: 69.119%\n",
            "Epoch 004: Loss: 1.778, Accuracy: 69.118%\n",
            "Epoch 004: Loss: 1.780, Accuracy: 69.085%\n",
            "Epoch 004: Loss: 1.781, Accuracy: 69.084%\n",
            "Epoch 004: Loss: 1.778, Accuracy: 69.143%\n",
            "Epoch 004: Loss: 1.779, Accuracy: 69.126%\n",
            "Epoch 004: Loss: 1.778, Accuracy: 69.124%\n",
            "Epoch 004: Loss: 1.778, Accuracy: 69.122%\n",
            "Epoch 004: Loss: 1.778, Accuracy: 69.120%\n",
            "Epoch 004: Loss: 1.778, Accuracy: 69.133%\n",
            "Epoch 004: Loss: 1.778, Accuracy: 69.131%\n",
            "Epoch 004: Loss: 1.781, Accuracy: 69.071%\n",
            "Epoch 004: Loss: 1.782, Accuracy: 69.041%\n",
            "Epoch 004: Loss: 1.781, Accuracy: 69.025%\n",
            "Epoch 004: Loss: 1.780, Accuracy: 69.024%\n",
            "Epoch 004: Loss: 1.779, Accuracy: 69.022%\n",
            "Epoch 004: Loss: 1.779, Accuracy: 69.021%\n",
            "Epoch 004: Loss: 1.777, Accuracy: 69.077%\n",
            "Epoch 004: Loss: 1.776, Accuracy: 69.047%\n",
            "Epoch 004: Loss: 1.776, Accuracy: 69.046%\n",
            "Epoch 004: Loss: 1.775, Accuracy: 69.072%\n",
            "Epoch 004: Loss: 1.776, Accuracy: 69.057%\n",
            "Epoch 004: Loss: 1.773, Accuracy: 69.111%\n",
            "Epoch 004: Loss: 1.773, Accuracy: 69.110%\n",
            "Epoch 004: Loss: 1.772, Accuracy: 69.122%\n",
            "Epoch 004: Loss: 1.772, Accuracy: 69.175%\n",
            "Epoch 004: Loss: 1.773, Accuracy: 69.132%\n",
            "Epoch 004: Loss: 1.773, Accuracy: 69.117%\n",
            "Epoch 004: Loss: 1.773, Accuracy: 69.115%\n",
            "Epoch 004: Loss: 1.771, Accuracy: 69.195%\n",
            "Epoch 004: Loss: 1.770, Accuracy: 69.139%\n",
            "Epoch 004: Loss: 1.769, Accuracy: 69.151%\n",
            "Epoch 004: Loss: 1.770, Accuracy: 69.109%\n",
            "Epoch 004: Loss: 1.768, Accuracy: 69.134%\n",
            "Epoch 004: Loss: 1.767, Accuracy: 69.172%\n",
            "Epoch 004: Loss: 1.768, Accuracy: 69.144%\n",
            "Epoch 004: Loss: 1.769, Accuracy: 69.116%\n",
            "Epoch 004: Loss: 1.768, Accuracy: 69.102%\n",
            "Epoch 004: Loss: 1.769, Accuracy: 69.061%\n",
            "Epoch 004: Loss: 1.769, Accuracy: 69.073%\n",
            "Epoch 004: Loss: 1.770, Accuracy: 69.110%\n",
            "Epoch 004: Loss: 1.770, Accuracy: 69.096%\n",
            "Epoch 004: Loss: 1.770, Accuracy: 69.082%\n",
            "Epoch 004: Loss: 1.770, Accuracy: 69.106%\n",
            "Epoch 004: Loss: 1.768, Accuracy: 69.142%\n",
            "Epoch 004: Loss: 1.766, Accuracy: 69.191%\n",
            "Epoch 004: Loss: 1.768, Accuracy: 69.177%\n",
            "Epoch 004: Loss: 1.766, Accuracy: 69.213%\n",
            "Epoch 004: Loss: 1.765, Accuracy: 69.248%\n",
            "Epoch 004: Loss: 1.764, Accuracy: 69.271%\n",
            "Epoch 004: Loss: 1.765, Accuracy: 69.219%\n",
            "Epoch 004: Loss: 1.766, Accuracy: 69.193%\n",
            "Epoch 004: Loss: 1.766, Accuracy: 69.216%\n",
            "Epoch 004: Loss: 1.766, Accuracy: 69.189%\n",
            "Epoch 004: Loss: 1.766, Accuracy: 69.188%\n",
            "Epoch 004: Loss: 1.766, Accuracy: 69.210%\n",
            "Epoch 004: Loss: 1.766, Accuracy: 69.208%\n",
            "Epoch 004: Loss: 1.766, Accuracy: 69.207%\n",
            "Epoch 004: Loss: 1.765, Accuracy: 69.217%\n",
            "Epoch 004: Loss: 1.765, Accuracy: 69.227%\n",
            "Epoch 004: Loss: 1.766, Accuracy: 69.213%\n",
            "Epoch 004: Loss: 1.766, Accuracy: 69.212%\n",
            "Epoch 004: Loss: 1.766, Accuracy: 69.210%\n",
            "Epoch 004: Loss: 1.766, Accuracy: 69.208%\n",
            "Epoch 004: Loss: 1.767, Accuracy: 69.195%\n",
            "Epoch 004: Loss: 1.766, Accuracy: 69.216%\n",
            "Epoch 004: Loss: 1.767, Accuracy: 69.215%\n",
            "Epoch 004: Loss: 1.769, Accuracy: 69.155%\n",
            "Epoch 004: Loss: 1.768, Accuracy: 69.200%\n",
            "Epoch 004: Loss: 1.767, Accuracy: 69.198%\n",
            "Epoch 004: Loss: 1.767, Accuracy: 69.196%\n",
            "Epoch 004: Loss: 1.766, Accuracy: 69.218%\n",
            "Epoch 004: Loss: 1.765, Accuracy: 69.216%\n",
            "Epoch 004: Loss: 1.764, Accuracy: 69.271%\n",
            "Epoch 004: Loss: 1.765, Accuracy: 69.235%\n",
            "Epoch 004: Loss: 1.766, Accuracy: 69.256%\n",
            "Epoch 004: Loss: 1.765, Accuracy: 69.288%\n",
            "Epoch 004: Loss: 1.765, Accuracy: 69.275%\n",
            "Epoch 004: Loss: 1.765, Accuracy: 69.283%\n",
            "Epoch 005: Loss: 1.262, Accuracy: 87.500%\n",
            "Epoch 005: Loss: 1.282, Accuracy: 81.250%\n",
            "Epoch 005: Loss: 1.438, Accuracy: 76.042%\n",
            "Epoch 005: Loss: 1.449, Accuracy: 77.344%\n",
            "Epoch 005: Loss: 1.413, Accuracy: 78.125%\n",
            "Epoch 005: Loss: 1.415, Accuracy: 77.604%\n",
            "Epoch 005: Loss: 1.566, Accuracy: 75.000%\n",
            "Epoch 005: Loss: 1.511, Accuracy: 76.562%\n",
            "Epoch 005: Loss: 1.649, Accuracy: 72.917%\n",
            "Epoch 005: Loss: 1.629, Accuracy: 72.812%\n",
            "Epoch 005: Loss: 1.596, Accuracy: 73.295%\n",
            "Epoch 005: Loss: 1.613, Accuracy: 73.438%\n",
            "Epoch 005: Loss: 1.632, Accuracy: 72.837%\n",
            "Epoch 005: Loss: 1.631, Accuracy: 72.545%\n",
            "Epoch 005: Loss: 1.655, Accuracy: 72.500%\n",
            "Epoch 005: Loss: 1.676, Accuracy: 71.680%\n",
            "Epoch 005: Loss: 1.687, Accuracy: 70.956%\n",
            "Epoch 005: Loss: 1.680, Accuracy: 71.007%\n",
            "Epoch 005: Loss: 1.686, Accuracy: 70.888%\n",
            "Epoch 005: Loss: 1.683, Accuracy: 71.094%\n",
            "Epoch 005: Loss: 1.682, Accuracy: 70.982%\n",
            "Epoch 005: Loss: 1.678, Accuracy: 71.023%\n",
            "Epoch 005: Loss: 1.705, Accuracy: 70.109%\n",
            "Epoch 005: Loss: 1.708, Accuracy: 70.052%\n",
            "Epoch 005: Loss: 1.695, Accuracy: 70.375%\n",
            "Epoch 005: Loss: 1.713, Accuracy: 70.192%\n",
            "Epoch 005: Loss: 1.733, Accuracy: 69.444%\n",
            "Epoch 005: Loss: 1.738, Accuracy: 69.085%\n",
            "Epoch 005: Loss: 1.730, Accuracy: 69.289%\n",
            "Epoch 005: Loss: 1.726, Accuracy: 69.687%\n",
            "Epoch 005: Loss: 1.717, Accuracy: 70.060%\n",
            "Epoch 005: Loss: 1.719, Accuracy: 70.020%\n",
            "Epoch 005: Loss: 1.732, Accuracy: 69.792%\n",
            "Epoch 005: Loss: 1.740, Accuracy: 69.577%\n",
            "Epoch 005: Loss: 1.756, Accuracy: 69.286%\n",
            "Epoch 005: Loss: 1.751, Accuracy: 69.358%\n",
            "Epoch 005: Loss: 1.745, Accuracy: 69.426%\n",
            "Epoch 005: Loss: 1.746, Accuracy: 69.572%\n",
            "Epoch 005: Loss: 1.747, Accuracy: 69.551%\n",
            "Epoch 005: Loss: 1.735, Accuracy: 69.687%\n",
            "Epoch 005: Loss: 1.734, Accuracy: 69.665%\n",
            "Epoch 005: Loss: 1.739, Accuracy: 69.643%\n",
            "Epoch 005: Loss: 1.734, Accuracy: 69.913%\n",
            "Epoch 005: Loss: 1.749, Accuracy: 69.389%\n",
            "Epoch 005: Loss: 1.748, Accuracy: 69.375%\n",
            "Epoch 005: Loss: 1.759, Accuracy: 69.090%\n",
            "Epoch 005: Loss: 1.753, Accuracy: 69.348%\n",
            "Epoch 005: Loss: 1.752, Accuracy: 69.401%\n",
            "Epoch 005: Loss: 1.749, Accuracy: 69.388%\n",
            "Epoch 005: Loss: 1.749, Accuracy: 69.375%\n",
            "Epoch 005: Loss: 1.752, Accuracy: 69.240%\n",
            "Epoch 005: Loss: 1.755, Accuracy: 69.411%\n",
            "Epoch 005: Loss: 1.762, Accuracy: 69.163%\n",
            "Epoch 005: Loss: 1.764, Accuracy: 69.155%\n",
            "Epoch 005: Loss: 1.759, Accuracy: 69.148%\n",
            "Epoch 005: Loss: 1.757, Accuracy: 69.085%\n",
            "Epoch 005: Loss: 1.757, Accuracy: 69.024%\n",
            "Epoch 005: Loss: 1.757, Accuracy: 68.912%\n",
            "Epoch 005: Loss: 1.769, Accuracy: 68.591%\n",
            "Epoch 005: Loss: 1.762, Accuracy: 68.906%\n",
            "Epoch 005: Loss: 1.755, Accuracy: 69.057%\n",
            "Epoch 005: Loss: 1.752, Accuracy: 69.052%\n",
            "Epoch 005: Loss: 1.758, Accuracy: 68.899%\n",
            "Epoch 005: Loss: 1.760, Accuracy: 68.799%\n",
            "Epoch 005: Loss: 1.768, Accuracy: 68.606%\n",
            "Epoch 005: Loss: 1.766, Accuracy: 68.655%\n",
            "Epoch 005: Loss: 1.760, Accuracy: 68.750%\n",
            "Epoch 005: Loss: 1.756, Accuracy: 68.842%\n",
            "Epoch 005: Loss: 1.760, Accuracy: 68.841%\n",
            "Epoch 005: Loss: 1.760, Accuracy: 68.929%\n",
            "Epoch 005: Loss: 1.767, Accuracy: 68.838%\n",
            "Epoch 005: Loss: 1.770, Accuracy: 68.837%\n",
            "Epoch 005: Loss: 1.770, Accuracy: 68.836%\n",
            "Epoch 005: Loss: 1.779, Accuracy: 68.750%\n",
            "Epoch 005: Loss: 1.770, Accuracy: 69.042%\n",
            "Epoch 005: Loss: 1.768, Accuracy: 68.956%\n",
            "Epoch 005: Loss: 1.767, Accuracy: 68.872%\n",
            "Epoch 005: Loss: 1.768, Accuracy: 68.710%\n",
            "Epoch 005: Loss: 1.768, Accuracy: 68.710%\n",
            "Epoch 005: Loss: 1.771, Accuracy: 68.672%\n",
            "Epoch 005: Loss: 1.772, Accuracy: 68.750%\n",
            "Epoch 005: Loss: 1.767, Accuracy: 68.826%\n",
            "Epoch 005: Loss: 1.763, Accuracy: 68.901%\n",
            "Epoch 005: Loss: 1.765, Accuracy: 68.899%\n",
            "Epoch 005: Loss: 1.771, Accuracy: 68.713%\n",
            "Epoch 005: Loss: 1.766, Accuracy: 68.859%\n",
            "Epoch 005: Loss: 1.762, Accuracy: 68.858%\n",
            "Epoch 005: Loss: 1.758, Accuracy: 68.963%\n",
            "Epoch 005: Loss: 1.760, Accuracy: 68.855%\n",
            "Epoch 005: Loss: 1.761, Accuracy: 68.854%\n",
            "Epoch 005: Loss: 1.760, Accuracy: 68.922%\n",
            "Epoch 005: Loss: 1.763, Accuracy: 68.818%\n",
            "Epoch 005: Loss: 1.758, Accuracy: 68.851%\n",
            "Epoch 005: Loss: 1.766, Accuracy: 68.783%\n",
            "Epoch 005: Loss: 1.764, Accuracy: 68.849%\n",
            "Epoch 005: Loss: 1.769, Accuracy: 68.750%\n",
            "Epoch 005: Loss: 1.773, Accuracy: 68.718%\n",
            "Epoch 005: Loss: 1.773, Accuracy: 68.750%\n",
            "Epoch 005: Loss: 1.769, Accuracy: 68.845%\n",
            "Epoch 005: Loss: 1.773, Accuracy: 68.781%\n",
            "Epoch 005: Loss: 1.771, Accuracy: 68.843%\n",
            "Epoch 005: Loss: 1.772, Accuracy: 68.781%\n",
            "Epoch 005: Loss: 1.775, Accuracy: 68.750%\n",
            "Epoch 005: Loss: 1.775, Accuracy: 68.780%\n",
            "Epoch 005: Loss: 1.773, Accuracy: 68.869%\n",
            "Epoch 005: Loss: 1.774, Accuracy: 68.868%\n",
            "Epoch 005: Loss: 1.774, Accuracy: 68.867%\n",
            "Epoch 005: Loss: 1.772, Accuracy: 68.808%\n",
            "Epoch 005: Loss: 1.771, Accuracy: 68.893%\n",
            "Epoch 005: Loss: 1.769, Accuracy: 68.864%\n",
            "Epoch 005: Loss: 1.764, Accuracy: 69.032%\n",
            "Epoch 005: Loss: 1.762, Accuracy: 69.113%\n",
            "Epoch 005: Loss: 1.761, Accuracy: 69.137%\n",
            "Epoch 005: Loss: 1.763, Accuracy: 69.079%\n",
            "Epoch 005: Loss: 1.764, Accuracy: 69.022%\n",
            "Epoch 005: Loss: 1.762, Accuracy: 69.073%\n",
            "Epoch 005: Loss: 1.765, Accuracy: 68.964%\n",
            "Epoch 005: Loss: 1.763, Accuracy: 69.015%\n",
            "Epoch 005: Loss: 1.768, Accuracy: 68.881%\n",
            "Epoch 005: Loss: 1.766, Accuracy: 68.932%\n",
            "Epoch 005: Loss: 1.765, Accuracy: 68.957%\n",
            "Epoch 005: Loss: 1.766, Accuracy: 68.955%\n",
            "Epoch 005: Loss: 1.769, Accuracy: 68.852%\n",
            "Epoch 005: Loss: 1.767, Accuracy: 68.901%\n",
            "Epoch 005: Loss: 1.771, Accuracy: 68.775%\n",
            "Epoch 005: Loss: 1.770, Accuracy: 68.725%\n",
            "Epoch 005: Loss: 1.772, Accuracy: 68.652%\n",
            "Epoch 005: Loss: 1.774, Accuracy: 68.628%\n",
            "Epoch 005: Loss: 1.771, Accuracy: 68.726%\n",
            "Epoch 005: Loss: 1.768, Accuracy: 68.822%\n",
            "Epoch 005: Loss: 1.771, Accuracy: 68.726%\n",
            "Epoch 005: Loss: 1.767, Accuracy: 68.821%\n",
            "Epoch 005: Loss: 1.766, Accuracy: 68.867%\n",
            "Epoch 005: Loss: 1.765, Accuracy: 68.890%\n",
            "Epoch 005: Loss: 1.768, Accuracy: 68.843%\n",
            "Epoch 005: Loss: 1.766, Accuracy: 68.865%\n",
            "Epoch 005: Loss: 1.764, Accuracy: 68.910%\n",
            "Epoch 005: Loss: 1.760, Accuracy: 69.044%\n",
            "Epoch 005: Loss: 1.760, Accuracy: 69.020%\n",
            "Epoch 005: Loss: 1.760, Accuracy: 69.018%\n",
            "Epoch 005: Loss: 1.760, Accuracy: 68.949%\n",
            "Epoch 005: Loss: 1.757, Accuracy: 69.102%\n",
            "Epoch 005: Loss: 1.754, Accuracy: 69.165%\n",
            "Epoch 005: Loss: 1.753, Accuracy: 69.184%\n",
            "Epoch 005: Loss: 1.754, Accuracy: 69.181%\n",
            "Epoch 005: Loss: 1.754, Accuracy: 69.199%\n",
            "Epoch 005: Loss: 1.754, Accuracy: 69.175%\n",
            "Epoch 005: Loss: 1.755, Accuracy: 69.151%\n",
            "Epoch 005: Loss: 1.756, Accuracy: 69.148%\n",
            "Epoch 005: Loss: 1.754, Accuracy: 69.229%\n",
            "Epoch 005: Loss: 1.753, Accuracy: 69.247%\n",
            "Epoch 005: Loss: 1.753, Accuracy: 69.243%\n",
            "Epoch 005: Loss: 1.754, Accuracy: 69.240%\n",
            "Epoch 005: Loss: 1.752, Accuracy: 69.278%\n",
            "Epoch 005: Loss: 1.750, Accuracy: 69.274%\n",
            "Epoch 005: Loss: 1.752, Accuracy: 69.271%\n",
            "Epoch 005: Loss: 1.750, Accuracy: 69.327%\n",
            "Epoch 005: Loss: 1.753, Accuracy: 69.244%\n",
            "Epoch 005: Loss: 1.752, Accuracy: 69.241%\n",
            "Epoch 005: Loss: 1.754, Accuracy: 69.160%\n",
            "Epoch 005: Loss: 1.756, Accuracy: 69.158%\n",
            "Epoch 005: Loss: 1.754, Accuracy: 69.194%\n",
            "Epoch 005: Loss: 1.751, Accuracy: 69.248%\n",
            "Epoch 005: Loss: 1.753, Accuracy: 69.169%\n",
            "Epoch 005: Loss: 1.753, Accuracy: 69.205%\n",
            "Epoch 005: Loss: 1.754, Accuracy: 69.221%\n",
            "Epoch 005: Loss: 1.756, Accuracy: 69.162%\n",
            "Epoch 005: Loss: 1.758, Accuracy: 69.085%\n",
            "Epoch 005: Loss: 1.758, Accuracy: 69.064%\n",
            "Epoch 005: Loss: 1.757, Accuracy: 69.099%\n",
            "Epoch 005: Loss: 1.758, Accuracy: 69.115%\n",
            "Epoch 005: Loss: 1.754, Accuracy: 69.168%\n",
            "Epoch 005: Loss: 1.750, Accuracy: 69.256%\n",
            "Epoch 005: Loss: 1.748, Accuracy: 69.325%\n",
            "Epoch 005: Loss: 1.745, Accuracy: 69.393%\n",
            "Epoch 005: Loss: 1.745, Accuracy: 69.425%\n",
            "Epoch 005: Loss: 1.745, Accuracy: 69.456%\n",
            "Epoch 005: Loss: 1.744, Accuracy: 69.487%\n",
            "Epoch 005: Loss: 1.746, Accuracy: 69.466%\n",
            "Epoch 005: Loss: 1.745, Accuracy: 69.462%\n",
            "Epoch 005: Loss: 1.746, Accuracy: 69.354%\n",
            "Epoch 005: Loss: 1.743, Accuracy: 69.454%\n",
            "Epoch 005: Loss: 1.742, Accuracy: 69.501%\n",
            "Epoch 005: Loss: 1.742, Accuracy: 69.480%\n",
            "Epoch 005: Loss: 1.744, Accuracy: 69.443%\n",
            "Epoch 005: Loss: 1.746, Accuracy: 69.388%\n",
            "Epoch 005: Loss: 1.748, Accuracy: 69.335%\n",
            "Epoch 005: Loss: 1.743, Accuracy: 69.398%\n",
            "Epoch 005: Loss: 1.743, Accuracy: 69.378%\n",
            "Epoch 005: Loss: 1.742, Accuracy: 69.375%\n",
            "Epoch 005: Loss: 1.742, Accuracy: 69.388%\n",
            "Epoch 005: Loss: 1.743, Accuracy: 69.352%\n",
            "Epoch 005: Loss: 1.743, Accuracy: 69.301%\n",
            "Epoch 005: Loss: 1.743, Accuracy: 69.282%\n",
            "Epoch 005: Loss: 1.745, Accuracy: 69.231%\n",
            "Epoch 005: Loss: 1.744, Accuracy: 69.260%\n",
            "Epoch 005: Loss: 1.744, Accuracy: 69.258%\n",
            "Epoch 005: Loss: 1.743, Accuracy: 69.302%\n",
            "Epoch 005: Loss: 1.741, Accuracy: 69.315%\n",
            "Epoch 005: Loss: 1.741, Accuracy: 69.359%\n",
            "Epoch 005: Loss: 1.739, Accuracy: 69.434%\n",
            "Epoch 005: Loss: 1.742, Accuracy: 69.338%\n",
            "Epoch 005: Loss: 1.741, Accuracy: 69.304%\n",
            "Epoch 005: Loss: 1.740, Accuracy: 69.332%\n",
            "Epoch 005: Loss: 1.741, Accuracy: 69.253%\n",
            "Epoch 005: Loss: 1.741, Accuracy: 69.281%\n",
            "Epoch 005: Loss: 1.738, Accuracy: 69.324%\n",
            "Epoch 005: Loss: 1.739, Accuracy: 69.336%\n",
            "Epoch 005: Loss: 1.738, Accuracy: 69.378%\n",
            "Epoch 005: Loss: 1.738, Accuracy: 69.375%\n",
            "Epoch 005: Loss: 1.738, Accuracy: 69.357%\n",
            "Epoch 005: Loss: 1.737, Accuracy: 69.369%\n",
            "Epoch 005: Loss: 1.738, Accuracy: 69.352%\n",
            "Epoch 005: Loss: 1.741, Accuracy: 69.290%\n",
            "Epoch 005: Loss: 1.741, Accuracy: 69.244%\n",
            "Epoch 005: Loss: 1.741, Accuracy: 69.256%\n",
            "Epoch 005: Loss: 1.740, Accuracy: 69.240%\n",
            "Epoch 005: Loss: 1.739, Accuracy: 69.266%\n",
            "Epoch 005: Loss: 1.739, Accuracy: 69.249%\n",
            "Epoch 005: Loss: 1.736, Accuracy: 69.276%\n",
            "Epoch 005: Loss: 1.735, Accuracy: 69.273%\n",
            "Epoch 005: Loss: 1.736, Accuracy: 69.271%\n",
            "Epoch 005: Loss: 1.735, Accuracy: 69.311%\n",
            "Epoch 005: Loss: 1.735, Accuracy: 69.308%\n",
            "Epoch 005: Loss: 1.733, Accuracy: 69.347%\n",
            "Epoch 005: Loss: 1.732, Accuracy: 69.372%\n",
            "Epoch 005: Loss: 1.732, Accuracy: 69.369%\n",
            "Epoch 005: Loss: 1.731, Accuracy: 69.408%\n",
            "Epoch 005: Loss: 1.731, Accuracy: 69.391%\n",
            "Epoch 005: Loss: 1.732, Accuracy: 69.375%\n",
            "Epoch 005: Loss: 1.732, Accuracy: 69.372%\n",
            "Epoch 005: Loss: 1.730, Accuracy: 69.437%\n",
            "Epoch 005: Loss: 1.730, Accuracy: 69.474%\n",
            "Epoch 005: Loss: 1.729, Accuracy: 69.485%\n",
            "Epoch 005: Loss: 1.731, Accuracy: 69.441%\n",
            "Epoch 005: Loss: 1.729, Accuracy: 69.492%\n",
            "Epoch 005: Loss: 1.728, Accuracy: 69.528%\n",
            "Epoch 005: Loss: 1.728, Accuracy: 69.472%\n",
            "Epoch 005: Loss: 1.729, Accuracy: 69.430%\n",
            "Epoch 005: Loss: 1.729, Accuracy: 69.427%\n",
            "Epoch 005: Loss: 1.729, Accuracy: 69.411%\n",
            "Epoch 005: Loss: 1.730, Accuracy: 69.421%\n",
            "Epoch 005: Loss: 1.729, Accuracy: 69.444%\n",
            "Epoch 005: Loss: 1.729, Accuracy: 69.442%\n",
            "Epoch 005: Loss: 1.730, Accuracy: 69.426%\n",
            "Epoch 005: Loss: 1.729, Accuracy: 69.461%\n",
            "Epoch 005: Loss: 1.727, Accuracy: 69.496%\n",
            "Epoch 005: Loss: 1.726, Accuracy: 69.531%\n",
            "Epoch 005: Loss: 1.727, Accuracy: 69.516%\n",
            "Epoch 005: Loss: 1.726, Accuracy: 69.538%\n",
            "Epoch 005: Loss: 1.725, Accuracy: 69.572%\n",
            "Epoch 005: Loss: 1.725, Accuracy: 69.581%\n",
            "Epoch 005: Loss: 1.726, Accuracy: 69.541%\n",
            "Epoch 005: Loss: 1.727, Accuracy: 69.488%\n",
            "Epoch 005: Loss: 1.727, Accuracy: 69.498%\n",
            "Epoch 005: Loss: 1.727, Accuracy: 69.519%\n",
            "Epoch 005: Loss: 1.727, Accuracy: 69.516%\n",
            "Epoch 005: Loss: 1.726, Accuracy: 69.513%\n",
            "Epoch 005: Loss: 1.727, Accuracy: 69.522%\n",
            "Epoch 005: Loss: 1.726, Accuracy: 69.531%\n",
            "Epoch 005: Loss: 1.726, Accuracy: 69.540%\n",
            "Epoch 005: Loss: 1.726, Accuracy: 69.525%\n",
            "Epoch 005: Loss: 1.727, Accuracy: 69.522%\n",
            "Epoch 005: Loss: 1.727, Accuracy: 69.531%\n",
            "Epoch 005: Loss: 1.727, Accuracy: 69.540%\n",
            "Epoch 005: Loss: 1.726, Accuracy: 69.525%\n",
            "Epoch 005: Loss: 1.728, Accuracy: 69.522%\n",
            "Epoch 005: Loss: 1.727, Accuracy: 69.555%\n",
            "Epoch 005: Loss: 1.728, Accuracy: 69.540%\n",
            "Epoch 005: Loss: 1.730, Accuracy: 69.479%\n",
            "Epoch 005: Loss: 1.729, Accuracy: 69.488%\n",
            "Epoch 005: Loss: 1.729, Accuracy: 69.474%\n",
            "Epoch 005: Loss: 1.728, Accuracy: 69.494%\n",
            "Epoch 005: Loss: 1.727, Accuracy: 69.514%\n",
            "Epoch 005: Loss: 1.727, Accuracy: 69.545%\n",
            "Epoch 005: Loss: 1.726, Accuracy: 69.599%\n",
            "Epoch 005: Loss: 1.727, Accuracy: 69.551%\n",
            "Epoch 005: Loss: 1.727, Accuracy: 69.571%\n",
            "Epoch 005: Loss: 1.727, Accuracy: 69.568%\n",
            "Epoch 005: Loss: 1.728, Accuracy: 69.576%\n",
            "Epoch 005: Loss: 1.728, Accuracy: 69.595%\n",
            "Epoch 006: Loss: 1.198, Accuracy: 87.500%\n",
            "Epoch 006: Loss: 1.226, Accuracy: 82.812%\n",
            "Epoch 006: Loss: 1.429, Accuracy: 78.125%\n",
            "Epoch 006: Loss: 1.461, Accuracy: 78.125%\n",
            "Epoch 006: Loss: 1.445, Accuracy: 78.750%\n",
            "Epoch 006: Loss: 1.442, Accuracy: 77.604%\n",
            "Epoch 006: Loss: 1.565, Accuracy: 75.893%\n",
            "Epoch 006: Loss: 1.511, Accuracy: 77.344%\n",
            "Epoch 006: Loss: 1.596, Accuracy: 73.611%\n",
            "Epoch 006: Loss: 1.585, Accuracy: 74.063%\n",
            "Epoch 006: Loss: 1.563, Accuracy: 74.148%\n",
            "Epoch 006: Loss: 1.575, Accuracy: 73.698%\n",
            "Epoch 006: Loss: 1.586, Accuracy: 72.837%\n",
            "Epoch 006: Loss: 1.606, Accuracy: 72.321%\n",
            "Epoch 006: Loss: 1.618, Accuracy: 72.083%\n",
            "Epoch 006: Loss: 1.640, Accuracy: 71.094%\n",
            "Epoch 006: Loss: 1.651, Accuracy: 70.956%\n",
            "Epoch 006: Loss: 1.648, Accuracy: 71.007%\n",
            "Epoch 006: Loss: 1.658, Accuracy: 71.217%\n",
            "Epoch 006: Loss: 1.651, Accuracy: 71.250%\n",
            "Epoch 006: Loss: 1.647, Accuracy: 71.280%\n",
            "Epoch 006: Loss: 1.639, Accuracy: 71.733%\n",
            "Epoch 006: Loss: 1.666, Accuracy: 70.924%\n",
            "Epoch 006: Loss: 1.669, Accuracy: 70.833%\n",
            "Epoch 006: Loss: 1.653, Accuracy: 71.125%\n",
            "Epoch 006: Loss: 1.665, Accuracy: 70.793%\n",
            "Epoch 006: Loss: 1.694, Accuracy: 70.255%\n",
            "Epoch 006: Loss: 1.702, Accuracy: 69.866%\n",
            "Epoch 006: Loss: 1.695, Accuracy: 69.828%\n",
            "Epoch 006: Loss: 1.693, Accuracy: 70.104%\n",
            "Epoch 006: Loss: 1.684, Accuracy: 70.363%\n",
            "Epoch 006: Loss: 1.683, Accuracy: 70.215%\n",
            "Epoch 006: Loss: 1.698, Accuracy: 69.981%\n",
            "Epoch 006: Loss: 1.705, Accuracy: 69.761%\n",
            "Epoch 006: Loss: 1.725, Accuracy: 69.464%\n",
            "Epoch 006: Loss: 1.722, Accuracy: 69.618%\n",
            "Epoch 006: Loss: 1.722, Accuracy: 69.764%\n",
            "Epoch 006: Loss: 1.718, Accuracy: 69.819%\n",
            "Epoch 006: Loss: 1.721, Accuracy: 69.872%\n",
            "Epoch 006: Loss: 1.707, Accuracy: 70.234%\n",
            "Epoch 006: Loss: 1.712, Accuracy: 70.198%\n",
            "Epoch 006: Loss: 1.714, Accuracy: 70.164%\n",
            "Epoch 006: Loss: 1.708, Accuracy: 70.422%\n",
            "Epoch 006: Loss: 1.721, Accuracy: 69.957%\n",
            "Epoch 006: Loss: 1.719, Accuracy: 70.000%\n",
            "Epoch 006: Loss: 1.730, Accuracy: 69.633%\n",
            "Epoch 006: Loss: 1.726, Accuracy: 69.814%\n",
            "Epoch 006: Loss: 1.726, Accuracy: 69.857%\n",
            "Epoch 006: Loss: 1.723, Accuracy: 70.026%\n",
            "Epoch 006: Loss: 1.723, Accuracy: 70.063%\n",
            "Epoch 006: Loss: 1.727, Accuracy: 69.853%\n",
            "Epoch 006: Loss: 1.724, Accuracy: 70.072%\n",
            "Epoch 006: Loss: 1.729, Accuracy: 69.988%\n",
            "Epoch 006: Loss: 1.731, Accuracy: 69.907%\n",
            "Epoch 006: Loss: 1.727, Accuracy: 70.057%\n",
            "Epoch 006: Loss: 1.728, Accuracy: 70.033%\n",
            "Epoch 006: Loss: 1.729, Accuracy: 70.066%\n",
            "Epoch 006: Loss: 1.734, Accuracy: 70.097%\n",
            "Epoch 006: Loss: 1.743, Accuracy: 69.915%\n",
            "Epoch 006: Loss: 1.734, Accuracy: 70.104%\n",
            "Epoch 006: Loss: 1.729, Accuracy: 70.287%\n",
            "Epoch 006: Loss: 1.727, Accuracy: 70.312%\n",
            "Epoch 006: Loss: 1.733, Accuracy: 70.188%\n",
            "Epoch 006: Loss: 1.735, Accuracy: 70.020%\n",
            "Epoch 006: Loss: 1.741, Accuracy: 69.808%\n",
            "Epoch 006: Loss: 1.740, Accuracy: 69.934%\n",
            "Epoch 006: Loss: 1.732, Accuracy: 70.103%\n",
            "Epoch 006: Loss: 1.729, Accuracy: 70.083%\n",
            "Epoch 006: Loss: 1.731, Accuracy: 70.018%\n",
            "Epoch 006: Loss: 1.731, Accuracy: 69.955%\n",
            "Epoch 006: Loss: 1.743, Accuracy: 69.806%\n",
            "Epoch 006: Loss: 1.747, Accuracy: 69.965%\n",
            "Epoch 006: Loss: 1.746, Accuracy: 69.949%\n",
            "Epoch 006: Loss: 1.756, Accuracy: 69.848%\n",
            "Epoch 006: Loss: 1.748, Accuracy: 70.083%\n",
            "Epoch 006: Loss: 1.747, Accuracy: 70.025%\n",
            "Epoch 006: Loss: 1.747, Accuracy: 69.968%\n",
            "Epoch 006: Loss: 1.748, Accuracy: 69.872%\n",
            "Epoch 006: Loss: 1.749, Accuracy: 69.778%\n",
            "Epoch 006: Loss: 1.751, Accuracy: 69.727%\n",
            "Epoch 006: Loss: 1.749, Accuracy: 69.715%\n",
            "Epoch 006: Loss: 1.745, Accuracy: 69.779%\n",
            "Epoch 006: Loss: 1.741, Accuracy: 69.804%\n",
            "Epoch 006: Loss: 1.742, Accuracy: 69.717%\n",
            "Epoch 006: Loss: 1.749, Accuracy: 69.596%\n",
            "Epoch 006: Loss: 1.744, Accuracy: 69.767%\n",
            "Epoch 006: Loss: 1.739, Accuracy: 69.792%\n",
            "Epoch 006: Loss: 1.737, Accuracy: 69.922%\n",
            "Epoch 006: Loss: 1.738, Accuracy: 69.768%\n",
            "Epoch 006: Loss: 1.737, Accuracy: 69.757%\n",
            "Epoch 006: Loss: 1.738, Accuracy: 69.815%\n",
            "Epoch 006: Loss: 1.740, Accuracy: 69.735%\n",
            "Epoch 006: Loss: 1.736, Accuracy: 69.792%\n",
            "Epoch 006: Loss: 1.744, Accuracy: 69.648%\n",
            "Epoch 006: Loss: 1.741, Accuracy: 69.704%\n",
            "Epoch 006: Loss: 1.745, Accuracy: 69.564%\n",
            "Epoch 006: Loss: 1.748, Accuracy: 69.523%\n",
            "Epoch 006: Loss: 1.748, Accuracy: 69.515%\n",
            "Epoch 006: Loss: 1.742, Accuracy: 69.665%\n",
            "Epoch 006: Loss: 1.747, Accuracy: 69.563%\n",
            "Epoch 006: Loss: 1.744, Accuracy: 69.647%\n",
            "Epoch 006: Loss: 1.745, Accuracy: 69.577%\n",
            "Epoch 006: Loss: 1.749, Accuracy: 69.508%\n",
            "Epoch 006: Loss: 1.749, Accuracy: 69.531%\n",
            "Epoch 006: Loss: 1.746, Accuracy: 69.583%\n",
            "Epoch 006: Loss: 1.746, Accuracy: 69.546%\n",
            "Epoch 006: Loss: 1.744, Accuracy: 69.568%\n",
            "Epoch 006: Loss: 1.744, Accuracy: 69.531%\n",
            "Epoch 006: Loss: 1.744, Accuracy: 69.467%\n",
            "Epoch 006: Loss: 1.744, Accuracy: 69.489%\n",
            "Epoch 006: Loss: 1.739, Accuracy: 69.651%\n",
            "Epoch 006: Loss: 1.736, Accuracy: 69.699%\n",
            "Epoch 006: Loss: 1.736, Accuracy: 69.690%\n",
            "Epoch 006: Loss: 1.739, Accuracy: 69.627%\n",
            "Epoch 006: Loss: 1.741, Accuracy: 69.620%\n",
            "Epoch 006: Loss: 1.740, Accuracy: 69.639%\n",
            "Epoch 006: Loss: 1.743, Accuracy: 69.605%\n",
            "Epoch 006: Loss: 1.743, Accuracy: 69.624%\n",
            "Epoch 006: Loss: 1.748, Accuracy: 69.485%\n",
            "Epoch 006: Loss: 1.746, Accuracy: 69.453%\n",
            "Epoch 006: Loss: 1.745, Accuracy: 69.473%\n",
            "Epoch 006: Loss: 1.744, Accuracy: 69.365%\n",
            "Epoch 006: Loss: 1.747, Accuracy: 69.284%\n",
            "Epoch 006: Loss: 1.746, Accuracy: 69.279%\n",
            "Epoch 006: Loss: 1.752, Accuracy: 69.125%\n",
            "Epoch 006: Loss: 1.751, Accuracy: 68.998%\n",
            "Epoch 006: Loss: 1.752, Accuracy: 68.996%\n",
            "Epoch 006: Loss: 1.754, Accuracy: 68.921%\n",
            "Epoch 006: Loss: 1.752, Accuracy: 68.992%\n",
            "Epoch 006: Loss: 1.749, Accuracy: 69.087%\n",
            "Epoch 006: Loss: 1.753, Accuracy: 68.989%\n",
            "Epoch 006: Loss: 1.750, Accuracy: 69.081%\n",
            "Epoch 006: Loss: 1.749, Accuracy: 69.102%\n",
            "Epoch 006: Loss: 1.748, Accuracy: 69.123%\n",
            "Epoch 006: Loss: 1.751, Accuracy: 69.074%\n",
            "Epoch 006: Loss: 1.749, Accuracy: 69.118%\n",
            "Epoch 006: Loss: 1.747, Accuracy: 69.183%\n",
            "Epoch 006: Loss: 1.743, Accuracy: 69.316%\n",
            "Epoch 006: Loss: 1.742, Accuracy: 69.335%\n",
            "Epoch 006: Loss: 1.742, Accuracy: 69.330%\n",
            "Epoch 006: Loss: 1.743, Accuracy: 69.304%\n",
            "Epoch 006: Loss: 1.738, Accuracy: 69.432%\n",
            "Epoch 006: Loss: 1.736, Accuracy: 69.493%\n",
            "Epoch 006: Loss: 1.735, Accuracy: 69.510%\n",
            "Epoch 006: Loss: 1.736, Accuracy: 69.504%\n",
            "Epoch 006: Loss: 1.737, Accuracy: 69.521%\n",
            "Epoch 006: Loss: 1.737, Accuracy: 69.515%\n",
            "Epoch 006: Loss: 1.738, Accuracy: 69.468%\n",
            "Epoch 006: Loss: 1.739, Accuracy: 69.463%\n",
            "Epoch 006: Loss: 1.737, Accuracy: 69.542%\n",
            "Epoch 006: Loss: 1.736, Accuracy: 69.557%\n",
            "Epoch 006: Loss: 1.736, Accuracy: 69.552%\n",
            "Epoch 006: Loss: 1.736, Accuracy: 69.547%\n",
            "Epoch 006: Loss: 1.735, Accuracy: 69.562%\n",
            "Epoch 006: Loss: 1.733, Accuracy: 69.556%\n",
            "Epoch 006: Loss: 1.735, Accuracy: 69.571%\n",
            "Epoch 006: Loss: 1.733, Accuracy: 69.686%\n",
            "Epoch 006: Loss: 1.737, Accuracy: 69.600%\n",
            "Epoch 006: Loss: 1.739, Accuracy: 69.595%\n",
            "Epoch 006: Loss: 1.740, Accuracy: 69.531%\n",
            "Epoch 006: Loss: 1.741, Accuracy: 69.526%\n",
            "Epoch 006: Loss: 1.740, Accuracy: 69.560%\n",
            "Epoch 006: Loss: 1.737, Accuracy: 69.632%\n",
            "Epoch 006: Loss: 1.738, Accuracy: 69.588%\n",
            "Epoch 006: Loss: 1.739, Accuracy: 69.659%\n",
            "Epoch 006: Loss: 1.740, Accuracy: 69.672%\n",
            "Epoch 006: Loss: 1.742, Accuracy: 69.611%\n",
            "Epoch 006: Loss: 1.744, Accuracy: 69.531%\n",
            "Epoch 006: Loss: 1.746, Accuracy: 69.545%\n",
            "Epoch 006: Loss: 1.746, Accuracy: 69.559%\n",
            "Epoch 006: Loss: 1.747, Accuracy: 69.554%\n",
            "Epoch 006: Loss: 1.743, Accuracy: 69.658%\n",
            "Epoch 006: Loss: 1.738, Accuracy: 69.780%\n",
            "Epoch 006: Loss: 1.735, Accuracy: 69.846%\n",
            "Epoch 006: Loss: 1.731, Accuracy: 69.893%\n",
            "Epoch 006: Loss: 1.731, Accuracy: 69.904%\n",
            "Epoch 006: Loss: 1.730, Accuracy: 69.951%\n",
            "Epoch 006: Loss: 1.730, Accuracy: 69.944%\n",
            "Epoch 006: Loss: 1.732, Accuracy: 69.920%\n",
            "Epoch 006: Loss: 1.731, Accuracy: 69.931%\n",
            "Epoch 006: Loss: 1.732, Accuracy: 69.855%\n",
            "Epoch 006: Loss: 1.729, Accuracy: 69.952%\n",
            "Epoch 006: Loss: 1.728, Accuracy: 69.997%\n",
            "Epoch 006: Loss: 1.729, Accuracy: 69.922%\n",
            "Epoch 006: Loss: 1.731, Accuracy: 69.865%\n",
            "Epoch 006: Loss: 1.733, Accuracy: 69.808%\n",
            "Epoch 006: Loss: 1.734, Accuracy: 69.786%\n",
            "Epoch 006: Loss: 1.730, Accuracy: 69.847%\n",
            "Epoch 006: Loss: 1.731, Accuracy: 69.825%\n",
            "Epoch 006: Loss: 1.730, Accuracy: 69.868%\n",
            "Epoch 006: Loss: 1.730, Accuracy: 69.830%\n",
            "Epoch 006: Loss: 1.731, Accuracy: 69.808%\n",
            "Epoch 006: Loss: 1.732, Accuracy: 69.770%\n",
            "Epoch 006: Loss: 1.732, Accuracy: 69.749%\n",
            "Epoch 006: Loss: 1.735, Accuracy: 69.696%\n",
            "Epoch 006: Loss: 1.736, Accuracy: 69.707%\n",
            "Epoch 006: Loss: 1.735, Accuracy: 69.718%\n",
            "Epoch 006: Loss: 1.733, Accuracy: 69.744%\n",
            "Epoch 006: Loss: 1.732, Accuracy: 69.771%\n",
            "Epoch 006: Loss: 1.732, Accuracy: 69.781%\n",
            "Epoch 006: Loss: 1.732, Accuracy: 69.807%\n",
            "Epoch 006: Loss: 1.734, Accuracy: 69.740%\n",
            "Epoch 006: Loss: 1.733, Accuracy: 69.735%\n",
            "Epoch 006: Loss: 1.732, Accuracy: 69.746%\n",
            "Epoch 006: Loss: 1.733, Accuracy: 69.680%\n",
            "Epoch 006: Loss: 1.733, Accuracy: 69.615%\n",
            "Epoch 006: Loss: 1.731, Accuracy: 69.671%\n",
            "Epoch 006: Loss: 1.731, Accuracy: 69.666%\n",
            "Epoch 006: Loss: 1.730, Accuracy: 69.692%\n",
            "Epoch 006: Loss: 1.730, Accuracy: 69.702%\n",
            "Epoch 006: Loss: 1.732, Accuracy: 69.698%\n",
            "Epoch 006: Loss: 1.730, Accuracy: 69.708%\n",
            "Epoch 006: Loss: 1.732, Accuracy: 69.689%\n",
            "Epoch 006: Loss: 1.734, Accuracy: 69.626%\n",
            "Epoch 006: Loss: 1.735, Accuracy: 69.593%\n",
            "Epoch 006: Loss: 1.735, Accuracy: 69.560%\n",
            "Epoch 006: Loss: 1.734, Accuracy: 69.542%\n",
            "Epoch 006: Loss: 1.733, Accuracy: 69.538%\n",
            "Epoch 006: Loss: 1.734, Accuracy: 69.549%\n",
            "Epoch 006: Loss: 1.731, Accuracy: 69.588%\n",
            "Epoch 006: Loss: 1.730, Accuracy: 69.584%\n",
            "Epoch 006: Loss: 1.729, Accuracy: 69.566%\n",
            "Epoch 006: Loss: 1.729, Accuracy: 69.605%\n",
            "Epoch 006: Loss: 1.728, Accuracy: 69.601%\n",
            "Epoch 006: Loss: 1.726, Accuracy: 69.667%\n",
            "Epoch 006: Loss: 1.725, Accuracy: 69.663%\n",
            "Epoch 006: Loss: 1.724, Accuracy: 69.659%\n",
            "Epoch 006: Loss: 1.723, Accuracy: 69.682%\n",
            "Epoch 006: Loss: 1.723, Accuracy: 69.664%\n",
            "Epoch 006: Loss: 1.724, Accuracy: 69.633%\n",
            "Epoch 006: Loss: 1.723, Accuracy: 69.629%\n",
            "Epoch 006: Loss: 1.721, Accuracy: 69.652%\n",
            "Epoch 006: Loss: 1.721, Accuracy: 69.662%\n",
            "Epoch 006: Loss: 1.719, Accuracy: 69.658%\n",
            "Epoch 006: Loss: 1.719, Accuracy: 69.641%\n",
            "Epoch 006: Loss: 1.718, Accuracy: 69.690%\n",
            "Epoch 006: Loss: 1.716, Accuracy: 69.726%\n",
            "Epoch 006: Loss: 1.718, Accuracy: 69.709%\n",
            "Epoch 006: Loss: 1.719, Accuracy: 69.665%\n",
            "Epoch 006: Loss: 1.720, Accuracy: 69.648%\n",
            "Epoch 006: Loss: 1.720, Accuracy: 69.645%\n",
            "Epoch 006: Loss: 1.720, Accuracy: 69.654%\n",
            "Epoch 006: Loss: 1.720, Accuracy: 69.650%\n",
            "Epoch 006: Loss: 1.721, Accuracy: 69.647%\n",
            "Epoch 006: Loss: 1.722, Accuracy: 69.617%\n",
            "Epoch 006: Loss: 1.720, Accuracy: 69.677%\n",
            "Epoch 006: Loss: 1.719, Accuracy: 69.712%\n",
            "Epoch 006: Loss: 1.718, Accuracy: 69.733%\n",
            "Epoch 006: Loss: 1.720, Accuracy: 69.704%\n",
            "Epoch 006: Loss: 1.719, Accuracy: 69.725%\n",
            "Epoch 006: Loss: 1.718, Accuracy: 69.771%\n",
            "Epoch 006: Loss: 1.717, Accuracy: 69.829%\n",
            "Epoch 006: Loss: 1.718, Accuracy: 69.788%\n",
            "Epoch 006: Loss: 1.719, Accuracy: 69.759%\n",
            "Epoch 006: Loss: 1.719, Accuracy: 69.767%\n",
            "Epoch 006: Loss: 1.719, Accuracy: 69.775%\n",
            "Epoch 006: Loss: 1.719, Accuracy: 69.808%\n",
            "Epoch 006: Loss: 1.719, Accuracy: 69.816%\n",
            "Epoch 006: Loss: 1.719, Accuracy: 69.836%\n",
            "Epoch 006: Loss: 1.719, Accuracy: 69.820%\n",
            "Epoch 006: Loss: 1.719, Accuracy: 69.852%\n",
            "Epoch 006: Loss: 1.720, Accuracy: 69.847%\n",
            "Epoch 006: Loss: 1.721, Accuracy: 69.819%\n",
            "Epoch 006: Loss: 1.721, Accuracy: 69.839%\n",
            "Epoch 006: Loss: 1.722, Accuracy: 69.847%\n",
            "Epoch 006: Loss: 1.722, Accuracy: 69.854%\n",
            "Epoch 006: Loss: 1.723, Accuracy: 69.838%\n",
            "Epoch 006: Loss: 1.723, Accuracy: 69.858%\n",
            "Epoch 006: Loss: 1.723, Accuracy: 69.854%\n",
            "Epoch 006: Loss: 1.725, Accuracy: 69.792%\n",
            "Epoch 006: Loss: 1.724, Accuracy: 69.845%\n",
            "Epoch 006: Loss: 1.724, Accuracy: 69.853%\n",
            "Epoch 006: Loss: 1.723, Accuracy: 69.872%\n",
            "Epoch 006: Loss: 1.723, Accuracy: 69.868%\n",
            "Epoch 006: Loss: 1.722, Accuracy: 69.875%\n",
            "Epoch 006: Loss: 1.721, Accuracy: 69.916%\n",
            "Epoch 006: Loss: 1.722, Accuracy: 69.867%\n",
            "Epoch 006: Loss: 1.722, Accuracy: 69.829%\n",
            "Epoch 006: Loss: 1.722, Accuracy: 69.859%\n",
            "Epoch 006: Loss: 1.722, Accuracy: 69.844%\n",
            "Epoch 006: Loss: 1.722, Accuracy: 69.862%\n",
            "Epoch 007: Loss: 0.975, Accuracy: 87.500%\n",
            "Epoch 007: Loss: 1.098, Accuracy: 84.375%\n",
            "Epoch 007: Loss: 1.340, Accuracy: 77.083%\n",
            "Epoch 007: Loss: 1.343, Accuracy: 76.562%\n",
            "Epoch 007: Loss: 1.345, Accuracy: 77.500%\n",
            "Epoch 007: Loss: 1.347, Accuracy: 77.604%\n",
            "Epoch 007: Loss: 1.455, Accuracy: 75.446%\n",
            "Epoch 007: Loss: 1.433, Accuracy: 77.344%\n",
            "Epoch 007: Loss: 1.582, Accuracy: 74.306%\n",
            "Epoch 007: Loss: 1.562, Accuracy: 74.375%\n",
            "Epoch 007: Loss: 1.535, Accuracy: 74.716%\n",
            "Epoch 007: Loss: 1.546, Accuracy: 74.219%\n",
            "Epoch 007: Loss: 1.555, Accuracy: 73.558%\n",
            "Epoch 007: Loss: 1.565, Accuracy: 73.214%\n",
            "Epoch 007: Loss: 1.584, Accuracy: 72.917%\n",
            "Epoch 007: Loss: 1.615, Accuracy: 72.070%\n",
            "Epoch 007: Loss: 1.627, Accuracy: 71.691%\n",
            "Epoch 007: Loss: 1.627, Accuracy: 71.701%\n",
            "Epoch 007: Loss: 1.625, Accuracy: 71.382%\n",
            "Epoch 007: Loss: 1.620, Accuracy: 71.562%\n",
            "Epoch 007: Loss: 1.633, Accuracy: 71.280%\n",
            "Epoch 007: Loss: 1.627, Accuracy: 71.449%\n",
            "Epoch 007: Loss: 1.659, Accuracy: 70.788%\n",
            "Epoch 007: Loss: 1.661, Accuracy: 70.833%\n",
            "Epoch 007: Loss: 1.640, Accuracy: 71.250%\n",
            "Epoch 007: Loss: 1.653, Accuracy: 71.034%\n",
            "Epoch 007: Loss: 1.677, Accuracy: 70.255%\n",
            "Epoch 007: Loss: 1.684, Accuracy: 69.978%\n",
            "Epoch 007: Loss: 1.683, Accuracy: 69.935%\n",
            "Epoch 007: Loss: 1.686, Accuracy: 70.000%\n",
            "Epoch 007: Loss: 1.673, Accuracy: 70.464%\n",
            "Epoch 007: Loss: 1.678, Accuracy: 70.215%\n",
            "Epoch 007: Loss: 1.692, Accuracy: 69.981%\n",
            "Epoch 007: Loss: 1.704, Accuracy: 69.577%\n",
            "Epoch 007: Loss: 1.718, Accuracy: 69.286%\n",
            "Epoch 007: Loss: 1.712, Accuracy: 69.358%\n",
            "Epoch 007: Loss: 1.709, Accuracy: 69.595%\n",
            "Epoch 007: Loss: 1.708, Accuracy: 69.490%\n",
            "Epoch 007: Loss: 1.708, Accuracy: 69.551%\n",
            "Epoch 007: Loss: 1.694, Accuracy: 69.922%\n",
            "Epoch 007: Loss: 1.697, Accuracy: 69.893%\n",
            "Epoch 007: Loss: 1.699, Accuracy: 69.792%\n",
            "Epoch 007: Loss: 1.697, Accuracy: 69.985%\n",
            "Epoch 007: Loss: 1.711, Accuracy: 69.602%\n",
            "Epoch 007: Loss: 1.710, Accuracy: 69.514%\n",
            "Epoch 007: Loss: 1.723, Accuracy: 69.226%\n",
            "Epoch 007: Loss: 1.716, Accuracy: 69.481%\n",
            "Epoch 007: Loss: 1.717, Accuracy: 69.531%\n",
            "Epoch 007: Loss: 1.716, Accuracy: 69.579%\n",
            "Epoch 007: Loss: 1.716, Accuracy: 69.563%\n",
            "Epoch 007: Loss: 1.716, Accuracy: 69.485%\n",
            "Epoch 007: Loss: 1.718, Accuracy: 69.651%\n",
            "Epoch 007: Loss: 1.724, Accuracy: 69.517%\n",
            "Epoch 007: Loss: 1.725, Accuracy: 69.502%\n",
            "Epoch 007: Loss: 1.721, Accuracy: 69.602%\n",
            "Epoch 007: Loss: 1.724, Accuracy: 69.587%\n",
            "Epoch 007: Loss: 1.725, Accuracy: 69.572%\n",
            "Epoch 007: Loss: 1.726, Accuracy: 69.666%\n",
            "Epoch 007: Loss: 1.735, Accuracy: 69.439%\n",
            "Epoch 007: Loss: 1.729, Accuracy: 69.740%\n",
            "Epoch 007: Loss: 1.724, Accuracy: 70.031%\n",
            "Epoch 007: Loss: 1.721, Accuracy: 70.010%\n",
            "Epoch 007: Loss: 1.726, Accuracy: 69.841%\n",
            "Epoch 007: Loss: 1.728, Accuracy: 69.629%\n",
            "Epoch 007: Loss: 1.735, Accuracy: 69.423%\n",
            "Epoch 007: Loss: 1.732, Accuracy: 69.460%\n",
            "Epoch 007: Loss: 1.728, Accuracy: 69.543%\n",
            "Epoch 007: Loss: 1.725, Accuracy: 69.485%\n",
            "Epoch 007: Loss: 1.727, Accuracy: 69.384%\n",
            "Epoch 007: Loss: 1.725, Accuracy: 69.330%\n",
            "Epoch 007: Loss: 1.737, Accuracy: 69.190%\n",
            "Epoch 007: Loss: 1.736, Accuracy: 69.227%\n",
            "Epoch 007: Loss: 1.735, Accuracy: 69.264%\n",
            "Epoch 007: Loss: 1.742, Accuracy: 69.046%\n",
            "Epoch 007: Loss: 1.734, Accuracy: 69.292%\n",
            "Epoch 007: Loss: 1.732, Accuracy: 69.285%\n",
            "Epoch 007: Loss: 1.733, Accuracy: 69.237%\n",
            "Epoch 007: Loss: 1.733, Accuracy: 69.071%\n",
            "Epoch 007: Loss: 1.734, Accuracy: 69.027%\n",
            "Epoch 007: Loss: 1.735, Accuracy: 68.945%\n",
            "Epoch 007: Loss: 1.733, Accuracy: 68.943%\n",
            "Epoch 007: Loss: 1.731, Accuracy: 68.979%\n",
            "Epoch 007: Loss: 1.728, Accuracy: 69.089%\n",
            "Epoch 007: Loss: 1.732, Accuracy: 69.085%\n",
            "Epoch 007: Loss: 1.738, Accuracy: 69.007%\n",
            "Epoch 007: Loss: 1.731, Accuracy: 69.186%\n",
            "Epoch 007: Loss: 1.727, Accuracy: 69.253%\n",
            "Epoch 007: Loss: 1.725, Accuracy: 69.283%\n",
            "Epoch 007: Loss: 1.727, Accuracy: 69.136%\n",
            "Epoch 007: Loss: 1.727, Accuracy: 69.132%\n",
            "Epoch 007: Loss: 1.727, Accuracy: 69.162%\n",
            "Epoch 007: Loss: 1.731, Accuracy: 69.124%\n",
            "Epoch 007: Loss: 1.727, Accuracy: 69.153%\n",
            "Epoch 007: Loss: 1.735, Accuracy: 69.082%\n",
            "Epoch 007: Loss: 1.733, Accuracy: 69.112%\n",
            "Epoch 007: Loss: 1.737, Accuracy: 69.010%\n",
            "Epoch 007: Loss: 1.740, Accuracy: 68.976%\n",
            "Epoch 007: Loss: 1.741, Accuracy: 68.973%\n",
            "Epoch 007: Loss: 1.736, Accuracy: 69.066%\n",
            "Epoch 007: Loss: 1.741, Accuracy: 68.969%\n",
            "Epoch 007: Loss: 1.739, Accuracy: 69.059%\n",
            "Epoch 007: Loss: 1.740, Accuracy: 69.087%\n",
            "Epoch 007: Loss: 1.743, Accuracy: 69.023%\n",
            "Epoch 007: Loss: 1.743, Accuracy: 69.111%\n",
            "Epoch 007: Loss: 1.742, Accuracy: 69.196%\n",
            "Epoch 007: Loss: 1.743, Accuracy: 69.163%\n",
            "Epoch 007: Loss: 1.743, Accuracy: 69.159%\n",
            "Epoch 007: Loss: 1.743, Accuracy: 69.155%\n",
            "Epoch 007: Loss: 1.742, Accuracy: 69.209%\n",
            "Epoch 007: Loss: 1.740, Accuracy: 69.205%\n",
            "Epoch 007: Loss: 1.735, Accuracy: 69.341%\n",
            "Epoch 007: Loss: 1.733, Accuracy: 69.448%\n",
            "Epoch 007: Loss: 1.732, Accuracy: 69.441%\n",
            "Epoch 007: Loss: 1.734, Accuracy: 69.353%\n",
            "Epoch 007: Loss: 1.735, Accuracy: 69.348%\n",
            "Epoch 007: Loss: 1.735, Accuracy: 69.397%\n",
            "Epoch 007: Loss: 1.738, Accuracy: 69.311%\n",
            "Epoch 007: Loss: 1.736, Accuracy: 69.306%\n",
            "Epoch 007: Loss: 1.742, Accuracy: 69.170%\n",
            "Epoch 007: Loss: 1.741, Accuracy: 69.219%\n",
            "Epoch 007: Loss: 1.740, Accuracy: 69.241%\n",
            "Epoch 007: Loss: 1.740, Accuracy: 69.237%\n",
            "Epoch 007: Loss: 1.742, Accuracy: 69.182%\n",
            "Epoch 007: Loss: 1.741, Accuracy: 69.178%\n",
            "Epoch 007: Loss: 1.746, Accuracy: 69.075%\n",
            "Epoch 007: Loss: 1.744, Accuracy: 69.048%\n",
            "Epoch 007: Loss: 1.745, Accuracy: 68.996%\n",
            "Epoch 007: Loss: 1.746, Accuracy: 68.945%\n",
            "Epoch 007: Loss: 1.742, Accuracy: 69.065%\n",
            "Epoch 007: Loss: 1.739, Accuracy: 69.135%\n",
            "Epoch 007: Loss: 1.743, Accuracy: 69.060%\n",
            "Epoch 007: Loss: 1.739, Accuracy: 69.152%\n",
            "Epoch 007: Loss: 1.737, Accuracy: 69.196%\n",
            "Epoch 007: Loss: 1.737, Accuracy: 69.193%\n",
            "Epoch 007: Loss: 1.741, Accuracy: 69.144%\n",
            "Epoch 007: Loss: 1.740, Accuracy: 69.187%\n",
            "Epoch 007: Loss: 1.738, Accuracy: 69.252%\n",
            "Epoch 007: Loss: 1.733, Accuracy: 69.361%\n",
            "Epoch 007: Loss: 1.734, Accuracy: 69.379%\n",
            "Epoch 007: Loss: 1.734, Accuracy: 69.375%\n",
            "Epoch 007: Loss: 1.734, Accuracy: 69.282%\n",
            "Epoch 007: Loss: 1.730, Accuracy: 69.410%\n",
            "Epoch 007: Loss: 1.727, Accuracy: 69.449%\n",
            "Epoch 007: Loss: 1.725, Accuracy: 69.510%\n",
            "Epoch 007: Loss: 1.727, Accuracy: 69.483%\n",
            "Epoch 007: Loss: 1.727, Accuracy: 69.478%\n",
            "Epoch 007: Loss: 1.728, Accuracy: 69.515%\n",
            "Epoch 007: Loss: 1.729, Accuracy: 69.468%\n",
            "Epoch 007: Loss: 1.731, Accuracy: 69.463%\n",
            "Epoch 007: Loss: 1.728, Accuracy: 69.563%\n",
            "Epoch 007: Loss: 1.728, Accuracy: 69.578%\n",
            "Epoch 007: Loss: 1.728, Accuracy: 69.572%\n",
            "Epoch 007: Loss: 1.728, Accuracy: 69.587%\n",
            "Epoch 007: Loss: 1.726, Accuracy: 69.663%\n",
            "Epoch 007: Loss: 1.725, Accuracy: 69.698%\n",
            "Epoch 007: Loss: 1.726, Accuracy: 69.671%\n",
            "Epoch 007: Loss: 1.725, Accuracy: 69.725%\n",
            "Epoch 007: Loss: 1.728, Accuracy: 69.640%\n",
            "Epoch 007: Loss: 1.728, Accuracy: 69.595%\n",
            "Epoch 007: Loss: 1.730, Accuracy: 69.531%\n",
            "Epoch 007: Loss: 1.732, Accuracy: 69.526%\n",
            "Epoch 007: Loss: 1.730, Accuracy: 69.560%\n",
            "Epoch 007: Loss: 1.727, Accuracy: 69.632%\n",
            "Epoch 007: Loss: 1.727, Accuracy: 69.588%\n",
            "Epoch 007: Loss: 1.726, Accuracy: 69.621%\n",
            "Epoch 007: Loss: 1.727, Accuracy: 69.597%\n",
            "Epoch 007: Loss: 1.728, Accuracy: 69.517%\n",
            "Epoch 007: Loss: 1.731, Accuracy: 69.420%\n",
            "Epoch 007: Loss: 1.732, Accuracy: 69.416%\n",
            "Epoch 007: Loss: 1.732, Accuracy: 69.430%\n",
            "Epoch 007: Loss: 1.731, Accuracy: 69.408%\n",
            "Epoch 007: Loss: 1.728, Accuracy: 69.531%\n",
            "Epoch 007: Loss: 1.724, Accuracy: 69.671%\n",
            "Epoch 007: Loss: 1.722, Accuracy: 69.738%\n",
            "Epoch 007: Loss: 1.719, Accuracy: 69.804%\n",
            "Epoch 007: Loss: 1.719, Accuracy: 69.833%\n",
            "Epoch 007: Loss: 1.718, Accuracy: 69.862%\n",
            "Epoch 007: Loss: 1.718, Accuracy: 69.891%\n",
            "Epoch 007: Loss: 1.720, Accuracy: 69.867%\n",
            "Epoch 007: Loss: 1.719, Accuracy: 69.861%\n",
            "Epoch 007: Loss: 1.720, Accuracy: 69.786%\n",
            "Epoch 007: Loss: 1.717, Accuracy: 69.883%\n",
            "Epoch 007: Loss: 1.716, Accuracy: 69.928%\n",
            "Epoch 007: Loss: 1.717, Accuracy: 69.905%\n",
            "Epoch 007: Loss: 1.718, Accuracy: 69.882%\n",
            "Epoch 007: Loss: 1.721, Accuracy: 69.825%\n",
            "Epoch 007: Loss: 1.721, Accuracy: 69.820%\n",
            "Epoch 007: Loss: 1.717, Accuracy: 69.914%\n",
            "Epoch 007: Loss: 1.717, Accuracy: 69.924%\n",
            "Epoch 007: Loss: 1.715, Accuracy: 69.918%\n",
            "Epoch 007: Loss: 1.714, Accuracy: 69.895%\n",
            "Epoch 007: Loss: 1.715, Accuracy: 69.873%\n",
            "Epoch 007: Loss: 1.717, Accuracy: 69.835%\n",
            "Epoch 007: Loss: 1.718, Accuracy: 69.829%\n",
            "Epoch 007: Loss: 1.720, Accuracy: 69.744%\n",
            "Epoch 007: Loss: 1.719, Accuracy: 69.770%\n",
            "Epoch 007: Loss: 1.719, Accuracy: 69.781%\n",
            "Epoch 007: Loss: 1.718, Accuracy: 69.807%\n",
            "Epoch 007: Loss: 1.717, Accuracy: 69.834%\n",
            "Epoch 007: Loss: 1.718, Accuracy: 69.859%\n",
            "Epoch 007: Loss: 1.717, Accuracy: 69.869%\n",
            "Epoch 007: Loss: 1.719, Accuracy: 69.771%\n",
            "Epoch 007: Loss: 1.718, Accuracy: 69.781%\n",
            "Epoch 007: Loss: 1.717, Accuracy: 69.776%\n",
            "Epoch 007: Loss: 1.718, Accuracy: 69.741%\n",
            "Epoch 007: Loss: 1.719, Accuracy: 69.736%\n",
            "Epoch 007: Loss: 1.716, Accuracy: 69.792%\n",
            "Epoch 007: Loss: 1.716, Accuracy: 69.817%\n",
            "Epoch 007: Loss: 1.715, Accuracy: 69.842%\n",
            "Epoch 007: Loss: 1.715, Accuracy: 69.836%\n",
            "Epoch 007: Loss: 1.715, Accuracy: 69.802%\n",
            "Epoch 007: Loss: 1.715, Accuracy: 69.811%\n",
            "Epoch 007: Loss: 1.716, Accuracy: 69.792%\n",
            "Epoch 007: Loss: 1.719, Accuracy: 69.728%\n",
            "Epoch 007: Loss: 1.720, Accuracy: 69.695%\n",
            "Epoch 007: Loss: 1.720, Accuracy: 69.690%\n",
            "Epoch 007: Loss: 1.719, Accuracy: 69.672%\n",
            "Epoch 007: Loss: 1.719, Accuracy: 69.696%\n",
            "Epoch 007: Loss: 1.720, Accuracy: 69.692%\n",
            "Epoch 007: Loss: 1.718, Accuracy: 69.730%\n",
            "Epoch 007: Loss: 1.717, Accuracy: 69.726%\n",
            "Epoch 007: Loss: 1.717, Accuracy: 69.721%\n",
            "Epoch 007: Loss: 1.717, Accuracy: 69.773%\n",
            "Epoch 007: Loss: 1.717, Accuracy: 69.782%\n",
            "Epoch 007: Loss: 1.715, Accuracy: 69.819%\n",
            "Epoch 007: Loss: 1.714, Accuracy: 69.801%\n",
            "Epoch 007: Loss: 1.713, Accuracy: 69.810%\n",
            "Epoch 007: Loss: 1.713, Accuracy: 69.846%\n",
            "Epoch 007: Loss: 1.713, Accuracy: 69.814%\n",
            "Epoch 007: Loss: 1.714, Accuracy: 69.810%\n",
            "Epoch 007: Loss: 1.714, Accuracy: 69.778%\n",
            "Epoch 007: Loss: 1.711, Accuracy: 69.841%\n",
            "Epoch 007: Loss: 1.712, Accuracy: 69.850%\n",
            "Epoch 007: Loss: 1.711, Accuracy: 69.858%\n",
            "Epoch 007: Loss: 1.712, Accuracy: 69.814%\n",
            "Epoch 007: Loss: 1.711, Accuracy: 69.836%\n",
            "Epoch 007: Loss: 1.709, Accuracy: 69.871%\n",
            "Epoch 007: Loss: 1.711, Accuracy: 69.800%\n",
            "Epoch 007: Loss: 1.712, Accuracy: 69.757%\n",
            "Epoch 007: Loss: 1.711, Accuracy: 69.753%\n",
            "Epoch 007: Loss: 1.711, Accuracy: 69.723%\n",
            "Epoch 007: Loss: 1.711, Accuracy: 69.718%\n",
            "Epoch 007: Loss: 1.712, Accuracy: 69.753%\n",
            "Epoch 007: Loss: 1.712, Accuracy: 69.736%\n",
            "Epoch 007: Loss: 1.712, Accuracy: 69.719%\n",
            "Epoch 007: Loss: 1.711, Accuracy: 69.754%\n",
            "Epoch 007: Loss: 1.710, Accuracy: 69.775%\n",
            "Epoch 007: Loss: 1.708, Accuracy: 69.771%\n",
            "Epoch 007: Loss: 1.711, Accuracy: 69.741%\n",
            "Epoch 007: Loss: 1.710, Accuracy: 69.750%\n",
            "Epoch 007: Loss: 1.709, Accuracy: 69.783%\n",
            "Epoch 007: Loss: 1.709, Accuracy: 69.816%\n",
            "Epoch 007: Loss: 1.710, Accuracy: 69.775%\n",
            "Epoch 007: Loss: 1.712, Accuracy: 69.734%\n",
            "Epoch 007: Loss: 1.711, Accuracy: 69.743%\n",
            "Epoch 007: Loss: 1.711, Accuracy: 69.739%\n",
            "Epoch 007: Loss: 1.712, Accuracy: 69.747%\n",
            "Epoch 007: Loss: 1.712, Accuracy: 69.755%\n",
            "Epoch 007: Loss: 1.713, Accuracy: 69.751%\n",
            "Epoch 007: Loss: 1.712, Accuracy: 69.748%\n",
            "Epoch 007: Loss: 1.711, Accuracy: 69.768%\n",
            "Epoch 007: Loss: 1.711, Accuracy: 69.776%\n",
            "Epoch 007: Loss: 1.712, Accuracy: 69.772%\n",
            "Epoch 007: Loss: 1.712, Accuracy: 69.780%\n",
            "Epoch 007: Loss: 1.713, Accuracy: 69.776%\n",
            "Epoch 007: Loss: 1.713, Accuracy: 69.784%\n",
            "Epoch 007: Loss: 1.714, Accuracy: 69.780%\n",
            "Epoch 007: Loss: 1.714, Accuracy: 69.799%\n",
            "Epoch 007: Loss: 1.715, Accuracy: 69.784%\n",
            "Epoch 007: Loss: 1.717, Accuracy: 69.722%\n",
            "Epoch 007: Loss: 1.715, Accuracy: 69.753%\n",
            "Epoch 007: Loss: 1.715, Accuracy: 69.750%\n",
            "Epoch 007: Loss: 1.715, Accuracy: 69.769%\n",
            "Epoch 007: Loss: 1.714, Accuracy: 69.765%\n",
            "Epoch 007: Loss: 1.714, Accuracy: 69.773%\n",
            "Epoch 007: Loss: 1.713, Accuracy: 69.826%\n",
            "Epoch 007: Loss: 1.714, Accuracy: 69.788%\n",
            "Epoch 007: Loss: 1.714, Accuracy: 69.807%\n",
            "Epoch 007: Loss: 1.714, Accuracy: 69.825%\n",
            "Epoch 007: Loss: 1.715, Accuracy: 69.821%\n",
            "Epoch 007: Loss: 1.715, Accuracy: 69.817%\n",
            "Epoch 008: Loss: 1.290, Accuracy: 78.125%\n",
            "Epoch 008: Loss: 1.299, Accuracy: 79.688%\n",
            "Epoch 008: Loss: 1.456, Accuracy: 75.000%\n",
            "Epoch 008: Loss: 1.437, Accuracy: 77.344%\n",
            "Epoch 008: Loss: 1.397, Accuracy: 77.500%\n",
            "Epoch 008: Loss: 1.398, Accuracy: 78.125%\n",
            "Epoch 008: Loss: 1.530, Accuracy: 75.893%\n",
            "Epoch 008: Loss: 1.483, Accuracy: 77.734%\n",
            "Epoch 008: Loss: 1.605, Accuracy: 73.958%\n",
            "Epoch 008: Loss: 1.588, Accuracy: 73.750%\n",
            "Epoch 008: Loss: 1.555, Accuracy: 74.148%\n",
            "Epoch 008: Loss: 1.557, Accuracy: 73.698%\n",
            "Epoch 008: Loss: 1.584, Accuracy: 73.317%\n",
            "Epoch 008: Loss: 1.595, Accuracy: 72.991%\n",
            "Epoch 008: Loss: 1.614, Accuracy: 72.917%\n",
            "Epoch 008: Loss: 1.636, Accuracy: 71.875%\n",
            "Epoch 008: Loss: 1.639, Accuracy: 71.691%\n",
            "Epoch 008: Loss: 1.644, Accuracy: 71.875%\n",
            "Epoch 008: Loss: 1.647, Accuracy: 71.382%\n",
            "Epoch 008: Loss: 1.644, Accuracy: 71.406%\n",
            "Epoch 008: Loss: 1.650, Accuracy: 71.131%\n",
            "Epoch 008: Loss: 1.650, Accuracy: 71.449%\n",
            "Epoch 008: Loss: 1.678, Accuracy: 70.788%\n",
            "Epoch 008: Loss: 1.680, Accuracy: 70.833%\n",
            "Epoch 008: Loss: 1.665, Accuracy: 71.000%\n",
            "Epoch 008: Loss: 1.675, Accuracy: 70.673%\n",
            "Epoch 008: Loss: 1.703, Accuracy: 70.023%\n",
            "Epoch 008: Loss: 1.706, Accuracy: 69.754%\n",
            "Epoch 008: Loss: 1.701, Accuracy: 69.828%\n",
            "Epoch 008: Loss: 1.696, Accuracy: 69.792%\n",
            "Epoch 008: Loss: 1.681, Accuracy: 70.060%\n",
            "Epoch 008: Loss: 1.676, Accuracy: 70.117%\n",
            "Epoch 008: Loss: 1.690, Accuracy: 69.981%\n",
            "Epoch 008: Loss: 1.704, Accuracy: 69.945%\n",
            "Epoch 008: Loss: 1.722, Accuracy: 69.554%\n",
            "Epoch 008: Loss: 1.721, Accuracy: 69.618%\n",
            "Epoch 008: Loss: 1.716, Accuracy: 69.679%\n",
            "Epoch 008: Loss: 1.717, Accuracy: 69.655%\n",
            "Epoch 008: Loss: 1.720, Accuracy: 69.712%\n",
            "Epoch 008: Loss: 1.709, Accuracy: 70.078%\n",
            "Epoch 008: Loss: 1.708, Accuracy: 70.046%\n",
            "Epoch 008: Loss: 1.710, Accuracy: 69.866%\n",
            "Epoch 008: Loss: 1.702, Accuracy: 70.131%\n",
            "Epoch 008: Loss: 1.719, Accuracy: 69.602%\n",
            "Epoch 008: Loss: 1.716, Accuracy: 69.583%\n",
            "Epoch 008: Loss: 1.728, Accuracy: 69.158%\n",
            "Epoch 008: Loss: 1.721, Accuracy: 69.282%\n",
            "Epoch 008: Loss: 1.721, Accuracy: 69.271%\n",
            "Epoch 008: Loss: 1.718, Accuracy: 69.388%\n",
            "Epoch 008: Loss: 1.716, Accuracy: 69.313%\n",
            "Epoch 008: Loss: 1.720, Accuracy: 69.240%\n",
            "Epoch 008: Loss: 1.722, Accuracy: 69.291%\n",
            "Epoch 008: Loss: 1.728, Accuracy: 68.927%\n",
            "Epoch 008: Loss: 1.729, Accuracy: 68.924%\n",
            "Epoch 008: Loss: 1.724, Accuracy: 69.034%\n",
            "Epoch 008: Loss: 1.724, Accuracy: 69.029%\n",
            "Epoch 008: Loss: 1.721, Accuracy: 68.969%\n",
            "Epoch 008: Loss: 1.728, Accuracy: 69.073%\n",
            "Epoch 008: Loss: 1.741, Accuracy: 68.909%\n",
            "Epoch 008: Loss: 1.737, Accuracy: 69.063%\n",
            "Epoch 008: Loss: 1.730, Accuracy: 69.211%\n",
            "Epoch 008: Loss: 1.729, Accuracy: 69.254%\n",
            "Epoch 008: Loss: 1.734, Accuracy: 69.048%\n",
            "Epoch 008: Loss: 1.738, Accuracy: 68.994%\n",
            "Epoch 008: Loss: 1.747, Accuracy: 68.798%\n",
            "Epoch 008: Loss: 1.744, Accuracy: 68.939%\n",
            "Epoch 008: Loss: 1.737, Accuracy: 69.123%\n",
            "Epoch 008: Loss: 1.735, Accuracy: 69.164%\n",
            "Epoch 008: Loss: 1.736, Accuracy: 69.112%\n",
            "Epoch 008: Loss: 1.735, Accuracy: 69.152%\n",
            "Epoch 008: Loss: 1.743, Accuracy: 69.014%\n",
            "Epoch 008: Loss: 1.742, Accuracy: 69.141%\n",
            "Epoch 008: Loss: 1.741, Accuracy: 69.135%\n",
            "Epoch 008: Loss: 1.747, Accuracy: 69.046%\n",
            "Epoch 008: Loss: 1.738, Accuracy: 69.292%\n",
            "Epoch 008: Loss: 1.737, Accuracy: 69.285%\n",
            "Epoch 008: Loss: 1.737, Accuracy: 69.278%\n",
            "Epoch 008: Loss: 1.738, Accuracy: 69.111%\n",
            "Epoch 008: Loss: 1.739, Accuracy: 68.987%\n",
            "Epoch 008: Loss: 1.743, Accuracy: 68.984%\n",
            "Epoch 008: Loss: 1.740, Accuracy: 69.020%\n",
            "Epoch 008: Loss: 1.736, Accuracy: 69.055%\n",
            "Epoch 008: Loss: 1.731, Accuracy: 69.164%\n",
            "Epoch 008: Loss: 1.734, Accuracy: 69.159%\n",
            "Epoch 008: Loss: 1.741, Accuracy: 69.044%\n",
            "Epoch 008: Loss: 1.737, Accuracy: 69.150%\n",
            "Epoch 008: Loss: 1.732, Accuracy: 69.253%\n",
            "Epoch 008: Loss: 1.732, Accuracy: 69.318%\n",
            "Epoch 008: Loss: 1.733, Accuracy: 69.277%\n",
            "Epoch 008: Loss: 1.732, Accuracy: 69.236%\n",
            "Epoch 008: Loss: 1.733, Accuracy: 69.231%\n",
            "Epoch 008: Loss: 1.734, Accuracy: 69.124%\n",
            "Epoch 008: Loss: 1.730, Accuracy: 69.187%\n",
            "Epoch 008: Loss: 1.737, Accuracy: 69.016%\n",
            "Epoch 008: Loss: 1.734, Accuracy: 69.079%\n",
            "Epoch 008: Loss: 1.738, Accuracy: 68.978%\n",
            "Epoch 008: Loss: 1.742, Accuracy: 68.911%\n",
            "Epoch 008: Loss: 1.741, Accuracy: 68.973%\n",
            "Epoch 008: Loss: 1.737, Accuracy: 69.003%\n",
            "Epoch 008: Loss: 1.741, Accuracy: 68.906%\n",
            "Epoch 008: Loss: 1.738, Accuracy: 68.967%\n",
            "Epoch 008: Loss: 1.739, Accuracy: 68.934%\n",
            "Epoch 008: Loss: 1.744, Accuracy: 68.871%\n",
            "Epoch 008: Loss: 1.744, Accuracy: 68.840%\n",
            "Epoch 008: Loss: 1.742, Accuracy: 68.899%\n",
            "Epoch 008: Loss: 1.743, Accuracy: 68.868%\n",
            "Epoch 008: Loss: 1.743, Accuracy: 68.867%\n",
            "Epoch 008: Loss: 1.743, Accuracy: 68.895%\n",
            "Epoch 008: Loss: 1.745, Accuracy: 68.893%\n",
            "Epoch 008: Loss: 1.744, Accuracy: 68.864%\n",
            "Epoch 008: Loss: 1.739, Accuracy: 69.003%\n",
            "Epoch 008: Loss: 1.735, Accuracy: 69.029%\n",
            "Epoch 008: Loss: 1.734, Accuracy: 68.999%\n",
            "Epoch 008: Loss: 1.736, Accuracy: 68.914%\n",
            "Epoch 008: Loss: 1.738, Accuracy: 68.832%\n",
            "Epoch 008: Loss: 1.739, Accuracy: 68.912%\n",
            "Epoch 008: Loss: 1.743, Accuracy: 68.857%\n",
            "Epoch 008: Loss: 1.743, Accuracy: 68.909%\n",
            "Epoch 008: Loss: 1.747, Accuracy: 68.750%\n",
            "Epoch 008: Loss: 1.746, Accuracy: 68.750%\n",
            "Epoch 008: Loss: 1.745, Accuracy: 68.750%\n",
            "Epoch 008: Loss: 1.745, Accuracy: 68.724%\n",
            "Epoch 008: Loss: 1.748, Accuracy: 68.699%\n",
            "Epoch 008: Loss: 1.746, Accuracy: 68.725%\n",
            "Epoch 008: Loss: 1.753, Accuracy: 68.600%\n",
            "Epoch 008: Loss: 1.752, Accuracy: 68.552%\n",
            "Epoch 008: Loss: 1.752, Accuracy: 68.578%\n",
            "Epoch 008: Loss: 1.754, Accuracy: 68.530%\n",
            "Epoch 008: Loss: 1.751, Accuracy: 68.629%\n",
            "Epoch 008: Loss: 1.748, Accuracy: 68.726%\n",
            "Epoch 008: Loss: 1.749, Accuracy: 68.655%\n",
            "Epoch 008: Loss: 1.745, Accuracy: 68.703%\n",
            "Epoch 008: Loss: 1.744, Accuracy: 68.750%\n",
            "Epoch 008: Loss: 1.743, Accuracy: 68.750%\n",
            "Epoch 008: Loss: 1.746, Accuracy: 68.727%\n",
            "Epoch 008: Loss: 1.746, Accuracy: 68.773%\n",
            "Epoch 008: Loss: 1.745, Accuracy: 68.841%\n",
            "Epoch 008: Loss: 1.741, Accuracy: 68.954%\n",
            "Epoch 008: Loss: 1.740, Accuracy: 68.952%\n",
            "Epoch 008: Loss: 1.740, Accuracy: 68.929%\n",
            "Epoch 008: Loss: 1.741, Accuracy: 68.883%\n",
            "Epoch 008: Loss: 1.737, Accuracy: 69.014%\n",
            "Epoch 008: Loss: 1.734, Accuracy: 69.078%\n",
            "Epoch 008: Loss: 1.733, Accuracy: 69.054%\n",
            "Epoch 008: Loss: 1.734, Accuracy: 68.966%\n",
            "Epoch 008: Loss: 1.734, Accuracy: 69.007%\n",
            "Epoch 008: Loss: 1.735, Accuracy: 69.048%\n",
            "Epoch 008: Loss: 1.737, Accuracy: 68.961%\n",
            "Epoch 008: Loss: 1.738, Accuracy: 68.939%\n",
            "Epoch 008: Loss: 1.734, Accuracy: 69.021%\n",
            "Epoch 008: Loss: 1.734, Accuracy: 69.040%\n",
            "Epoch 008: Loss: 1.735, Accuracy: 69.038%\n",
            "Epoch 008: Loss: 1.736, Accuracy: 69.016%\n",
            "Epoch 008: Loss: 1.735, Accuracy: 69.075%\n",
            "Epoch 008: Loss: 1.733, Accuracy: 69.113%\n",
            "Epoch 008: Loss: 1.735, Accuracy: 69.091%\n",
            "Epoch 008: Loss: 1.732, Accuracy: 69.148%\n",
            "Epoch 008: Loss: 1.735, Accuracy: 69.086%\n",
            "Epoch 008: Loss: 1.735, Accuracy: 69.064%\n",
            "Epoch 008: Loss: 1.736, Accuracy: 68.984%\n",
            "Epoch 008: Loss: 1.737, Accuracy: 68.983%\n",
            "Epoch 008: Loss: 1.736, Accuracy: 68.981%\n",
            "Epoch 008: Loss: 1.733, Accuracy: 69.018%\n",
            "Epoch 008: Loss: 1.734, Accuracy: 68.979%\n",
            "Epoch 008: Loss: 1.735, Accuracy: 69.034%\n",
            "Epoch 008: Loss: 1.735, Accuracy: 69.051%\n",
            "Epoch 008: Loss: 1.739, Accuracy: 68.993%\n",
            "Epoch 008: Loss: 1.741, Accuracy: 68.880%\n",
            "Epoch 008: Loss: 1.743, Accuracy: 68.861%\n",
            "Epoch 008: Loss: 1.742, Accuracy: 68.879%\n",
            "Epoch 008: Loss: 1.743, Accuracy: 68.896%\n",
            "Epoch 008: Loss: 1.740, Accuracy: 68.968%\n",
            "Epoch 008: Loss: 1.736, Accuracy: 69.093%\n",
            "Epoch 008: Loss: 1.733, Accuracy: 69.181%\n",
            "Epoch 008: Loss: 1.730, Accuracy: 69.250%\n",
            "Epoch 008: Loss: 1.730, Accuracy: 69.283%\n",
            "Epoch 008: Loss: 1.728, Accuracy: 69.297%\n",
            "Epoch 008: Loss: 1.728, Accuracy: 69.312%\n",
            "Epoch 008: Loss: 1.728, Accuracy: 69.291%\n",
            "Epoch 008: Loss: 1.728, Accuracy: 69.271%\n",
            "Epoch 008: Loss: 1.729, Accuracy: 69.199%\n",
            "Epoch 008: Loss: 1.726, Accuracy: 69.282%\n",
            "Epoch 008: Loss: 1.724, Accuracy: 69.348%\n",
            "Epoch 008: Loss: 1.725, Accuracy: 69.310%\n",
            "Epoch 008: Loss: 1.726, Accuracy: 69.274%\n",
            "Epoch 008: Loss: 1.729, Accuracy: 69.204%\n",
            "Epoch 008: Loss: 1.729, Accuracy: 69.184%\n",
            "Epoch 008: Loss: 1.724, Accuracy: 69.265%\n",
            "Epoch 008: Loss: 1.725, Accuracy: 69.246%\n",
            "Epoch 008: Loss: 1.724, Accuracy: 69.260%\n",
            "Epoch 008: Loss: 1.723, Accuracy: 69.241%\n",
            "Epoch 008: Loss: 1.725, Accuracy: 69.206%\n",
            "Epoch 008: Loss: 1.725, Accuracy: 69.187%\n",
            "Epoch 008: Loss: 1.726, Accuracy: 69.185%\n",
            "Epoch 008: Loss: 1.729, Accuracy: 69.119%\n",
            "Epoch 008: Loss: 1.729, Accuracy: 69.149%\n",
            "Epoch 008: Loss: 1.729, Accuracy: 69.162%\n",
            "Epoch 008: Loss: 1.728, Accuracy: 69.192%\n",
            "Epoch 008: Loss: 1.726, Accuracy: 69.205%\n",
            "Epoch 008: Loss: 1.727, Accuracy: 69.250%\n",
            "Epoch 008: Loss: 1.725, Accuracy: 69.279%\n",
            "Epoch 008: Loss: 1.728, Accuracy: 69.199%\n",
            "Epoch 008: Loss: 1.726, Accuracy: 69.227%\n",
            "Epoch 008: Loss: 1.725, Accuracy: 69.256%\n",
            "Epoch 008: Loss: 1.725, Accuracy: 69.223%\n",
            "Epoch 008: Loss: 1.725, Accuracy: 69.220%\n",
            "Epoch 008: Loss: 1.722, Accuracy: 69.263%\n",
            "Epoch 008: Loss: 1.723, Accuracy: 69.291%\n",
            "Epoch 008: Loss: 1.722, Accuracy: 69.333%\n",
            "Epoch 008: Loss: 1.721, Accuracy: 69.330%\n",
            "Epoch 008: Loss: 1.722, Accuracy: 69.313%\n",
            "Epoch 008: Loss: 1.721, Accuracy: 69.295%\n",
            "Epoch 008: Loss: 1.723, Accuracy: 69.293%\n",
            "Epoch 008: Loss: 1.725, Accuracy: 69.232%\n",
            "Epoch 008: Loss: 1.726, Accuracy: 69.215%\n",
            "Epoch 008: Loss: 1.726, Accuracy: 69.213%\n",
            "Epoch 008: Loss: 1.725, Accuracy: 69.211%\n",
            "Epoch 008: Loss: 1.725, Accuracy: 69.194%\n",
            "Epoch 008: Loss: 1.725, Accuracy: 69.207%\n",
            "Epoch 008: Loss: 1.723, Accuracy: 69.276%\n",
            "Epoch 008: Loss: 1.722, Accuracy: 69.287%\n",
            "Epoch 008: Loss: 1.720, Accuracy: 69.299%\n",
            "Epoch 008: Loss: 1.719, Accuracy: 69.339%\n",
            "Epoch 008: Loss: 1.720, Accuracy: 69.294%\n",
            "Epoch 008: Loss: 1.717, Accuracy: 69.347%\n",
            "Epoch 008: Loss: 1.716, Accuracy: 69.331%\n",
            "Epoch 008: Loss: 1.715, Accuracy: 69.328%\n",
            "Epoch 008: Loss: 1.714, Accuracy: 69.367%\n",
            "Epoch 008: Loss: 1.715, Accuracy: 69.323%\n",
            "Epoch 008: Loss: 1.715, Accuracy: 69.321%\n",
            "Epoch 008: Loss: 1.714, Accuracy: 69.291%\n",
            "Epoch 008: Loss: 1.713, Accuracy: 69.356%\n",
            "Epoch 008: Loss: 1.711, Accuracy: 69.380%\n",
            "Epoch 008: Loss: 1.711, Accuracy: 69.351%\n",
            "Epoch 008: Loss: 1.712, Accuracy: 69.309%\n",
            "Epoch 008: Loss: 1.710, Accuracy: 69.306%\n",
            "Epoch 008: Loss: 1.709, Accuracy: 69.343%\n",
            "Epoch 008: Loss: 1.709, Accuracy: 69.315%\n",
            "Epoch 008: Loss: 1.711, Accuracy: 69.273%\n",
            "Epoch 008: Loss: 1.710, Accuracy: 69.271%\n",
            "Epoch 008: Loss: 1.711, Accuracy: 69.243%\n",
            "Epoch 008: Loss: 1.711, Accuracy: 69.241%\n",
            "Epoch 008: Loss: 1.711, Accuracy: 69.277%\n",
            "Epoch 008: Loss: 1.712, Accuracy: 69.275%\n",
            "Epoch 008: Loss: 1.711, Accuracy: 69.273%\n",
            "Epoch 008: Loss: 1.710, Accuracy: 69.322%\n",
            "Epoch 008: Loss: 1.708, Accuracy: 69.357%\n",
            "Epoch 008: Loss: 1.707, Accuracy: 69.380%\n",
            "Epoch 008: Loss: 1.708, Accuracy: 69.340%\n",
            "Epoch 008: Loss: 1.707, Accuracy: 69.388%\n",
            "Epoch 008: Loss: 1.706, Accuracy: 69.422%\n",
            "Epoch 008: Loss: 1.707, Accuracy: 69.444%\n",
            "Epoch 008: Loss: 1.708, Accuracy: 69.392%\n",
            "Epoch 008: Loss: 1.709, Accuracy: 69.353%\n",
            "Epoch 008: Loss: 1.708, Accuracy: 69.363%\n",
            "Epoch 008: Loss: 1.709, Accuracy: 69.348%\n",
            "Epoch 008: Loss: 1.708, Accuracy: 69.370%\n",
            "Epoch 008: Loss: 1.708, Accuracy: 69.380%\n",
            "Epoch 008: Loss: 1.709, Accuracy: 69.377%\n",
            "Epoch 008: Loss: 1.708, Accuracy: 69.387%\n",
            "Epoch 008: Loss: 1.709, Accuracy: 69.385%\n",
            "Epoch 008: Loss: 1.708, Accuracy: 69.346%\n",
            "Epoch 008: Loss: 1.710, Accuracy: 69.332%\n",
            "Epoch 008: Loss: 1.710, Accuracy: 69.342%\n",
            "Epoch 008: Loss: 1.710, Accuracy: 69.351%\n",
            "Epoch 008: Loss: 1.710, Accuracy: 69.384%\n",
            "Epoch 008: Loss: 1.711, Accuracy: 69.359%\n",
            "Epoch 008: Loss: 1.710, Accuracy: 69.380%\n",
            "Epoch 008: Loss: 1.711, Accuracy: 69.377%\n",
            "Epoch 008: Loss: 1.713, Accuracy: 69.317%\n",
            "Epoch 008: Loss: 1.712, Accuracy: 69.350%\n",
            "Epoch 008: Loss: 1.711, Accuracy: 69.347%\n",
            "Epoch 008: Loss: 1.711, Accuracy: 69.345%\n",
            "Epoch 008: Loss: 1.710, Accuracy: 69.366%\n",
            "Epoch 008: Loss: 1.710, Accuracy: 69.386%\n",
            "Epoch 008: Loss: 1.708, Accuracy: 69.429%\n",
            "Epoch 008: Loss: 1.709, Accuracy: 69.404%\n",
            "Epoch 008: Loss: 1.710, Accuracy: 69.413%\n",
            "Epoch 008: Loss: 1.709, Accuracy: 69.433%\n",
            "Epoch 008: Loss: 1.710, Accuracy: 69.420%\n",
            "Epoch 008: Loss: 1.709, Accuracy: 69.428%\n",
            "Epoch 009: Loss: 1.016, Accuracy: 87.500%\n",
            "Epoch 009: Loss: 1.124, Accuracy: 81.250%\n",
            "Epoch 009: Loss: 1.310, Accuracy: 73.958%\n",
            "Epoch 009: Loss: 1.394, Accuracy: 75.000%\n",
            "Epoch 009: Loss: 1.373, Accuracy: 76.250%\n",
            "Epoch 009: Loss: 1.390, Accuracy: 75.521%\n",
            "Epoch 009: Loss: 1.508, Accuracy: 72.768%\n",
            "Epoch 009: Loss: 1.461, Accuracy: 74.609%\n",
            "Epoch 009: Loss: 1.597, Accuracy: 71.181%\n",
            "Epoch 009: Loss: 1.573, Accuracy: 71.250%\n",
            "Epoch 009: Loss: 1.543, Accuracy: 71.591%\n",
            "Epoch 009: Loss: 1.550, Accuracy: 71.354%\n",
            "Epoch 009: Loss: 1.570, Accuracy: 70.913%\n",
            "Epoch 009: Loss: 1.589, Accuracy: 70.759%\n",
            "Epoch 009: Loss: 1.607, Accuracy: 70.833%\n",
            "Epoch 009: Loss: 1.629, Accuracy: 69.922%\n",
            "Epoch 009: Loss: 1.640, Accuracy: 69.853%\n",
            "Epoch 009: Loss: 1.643, Accuracy: 69.792%\n",
            "Epoch 009: Loss: 1.641, Accuracy: 70.066%\n",
            "Epoch 009: Loss: 1.641, Accuracy: 70.312%\n",
            "Epoch 009: Loss: 1.651, Accuracy: 69.792%\n",
            "Epoch 009: Loss: 1.645, Accuracy: 70.170%\n",
            "Epoch 009: Loss: 1.670, Accuracy: 69.429%\n",
            "Epoch 009: Loss: 1.675, Accuracy: 69.531%\n",
            "Epoch 009: Loss: 1.656, Accuracy: 69.750%\n",
            "Epoch 009: Loss: 1.668, Accuracy: 69.591%\n",
            "Epoch 009: Loss: 1.693, Accuracy: 68.981%\n",
            "Epoch 009: Loss: 1.697, Accuracy: 68.750%\n",
            "Epoch 009: Loss: 1.693, Accuracy: 68.966%\n",
            "Epoch 009: Loss: 1.691, Accuracy: 69.063%\n",
            "Epoch 009: Loss: 1.679, Accuracy: 69.556%\n",
            "Epoch 009: Loss: 1.682, Accuracy: 69.531%\n",
            "Epoch 009: Loss: 1.694, Accuracy: 69.318%\n",
            "Epoch 009: Loss: 1.708, Accuracy: 69.210%\n",
            "Epoch 009: Loss: 1.722, Accuracy: 68.929%\n",
            "Epoch 009: Loss: 1.718, Accuracy: 68.837%\n",
            "Epoch 009: Loss: 1.721, Accuracy: 69.003%\n",
            "Epoch 009: Loss: 1.721, Accuracy: 69.161%\n",
            "Epoch 009: Loss: 1.722, Accuracy: 69.311%\n",
            "Epoch 009: Loss: 1.705, Accuracy: 69.687%\n",
            "Epoch 009: Loss: 1.708, Accuracy: 69.665%\n",
            "Epoch 009: Loss: 1.706, Accuracy: 69.643%\n",
            "Epoch 009: Loss: 1.704, Accuracy: 69.913%\n",
            "Epoch 009: Loss: 1.719, Accuracy: 69.460%\n",
            "Epoch 009: Loss: 1.715, Accuracy: 69.375%\n",
            "Epoch 009: Loss: 1.729, Accuracy: 69.090%\n",
            "Epoch 009: Loss: 1.724, Accuracy: 69.348%\n",
            "Epoch 009: Loss: 1.724, Accuracy: 69.336%\n",
            "Epoch 009: Loss: 1.718, Accuracy: 69.388%\n",
            "Epoch 009: Loss: 1.719, Accuracy: 69.500%\n",
            "Epoch 009: Loss: 1.722, Accuracy: 69.608%\n",
            "Epoch 009: Loss: 1.718, Accuracy: 69.772%\n",
            "Epoch 009: Loss: 1.728, Accuracy: 69.575%\n",
            "Epoch 009: Loss: 1.729, Accuracy: 69.560%\n",
            "Epoch 009: Loss: 1.725, Accuracy: 69.545%\n",
            "Epoch 009: Loss: 1.725, Accuracy: 69.475%\n",
            "Epoch 009: Loss: 1.723, Accuracy: 69.408%\n",
            "Epoch 009: Loss: 1.726, Accuracy: 69.450%\n",
            "Epoch 009: Loss: 1.736, Accuracy: 69.280%\n",
            "Epoch 009: Loss: 1.728, Accuracy: 69.583%\n",
            "Epoch 009: Loss: 1.722, Accuracy: 69.672%\n",
            "Epoch 009: Loss: 1.720, Accuracy: 69.556%\n",
            "Epoch 009: Loss: 1.725, Accuracy: 69.395%\n",
            "Epoch 009: Loss: 1.730, Accuracy: 69.385%\n",
            "Epoch 009: Loss: 1.738, Accuracy: 69.231%\n",
            "Epoch 009: Loss: 1.736, Accuracy: 69.318%\n",
            "Epoch 009: Loss: 1.728, Accuracy: 69.543%\n",
            "Epoch 009: Loss: 1.725, Accuracy: 69.623%\n",
            "Epoch 009: Loss: 1.725, Accuracy: 69.611%\n",
            "Epoch 009: Loss: 1.722, Accuracy: 69.643%\n",
            "Epoch 009: Loss: 1.731, Accuracy: 69.498%\n",
            "Epoch 009: Loss: 1.731, Accuracy: 69.531%\n",
            "Epoch 009: Loss: 1.729, Accuracy: 69.563%\n",
            "Epoch 009: Loss: 1.737, Accuracy: 69.383%\n",
            "Epoch 009: Loss: 1.729, Accuracy: 69.583%\n",
            "Epoch 009: Loss: 1.730, Accuracy: 69.613%\n",
            "Epoch 009: Loss: 1.731, Accuracy: 69.643%\n",
            "Epoch 009: Loss: 1.732, Accuracy: 69.511%\n",
            "Epoch 009: Loss: 1.735, Accuracy: 69.422%\n",
            "Epoch 009: Loss: 1.735, Accuracy: 69.375%\n",
            "Epoch 009: Loss: 1.734, Accuracy: 69.406%\n",
            "Epoch 009: Loss: 1.731, Accuracy: 69.474%\n",
            "Epoch 009: Loss: 1.726, Accuracy: 69.578%\n",
            "Epoch 009: Loss: 1.728, Accuracy: 69.494%\n",
            "Epoch 009: Loss: 1.734, Accuracy: 69.412%\n",
            "Epoch 009: Loss: 1.730, Accuracy: 69.549%\n",
            "Epoch 009: Loss: 1.725, Accuracy: 69.612%\n",
            "Epoch 009: Loss: 1.724, Accuracy: 69.709%\n",
            "Epoch 009: Loss: 1.726, Accuracy: 69.663%\n",
            "Epoch 009: Loss: 1.725, Accuracy: 69.583%\n",
            "Epoch 009: Loss: 1.727, Accuracy: 69.643%\n",
            "Epoch 009: Loss: 1.730, Accuracy: 69.599%\n",
            "Epoch 009: Loss: 1.726, Accuracy: 69.657%\n",
            "Epoch 009: Loss: 1.734, Accuracy: 69.548%\n",
            "Epoch 009: Loss: 1.732, Accuracy: 69.638%\n",
            "Epoch 009: Loss: 1.735, Accuracy: 69.564%\n",
            "Epoch 009: Loss: 1.737, Accuracy: 69.491%\n",
            "Epoch 009: Loss: 1.738, Accuracy: 69.515%\n",
            "Epoch 009: Loss: 1.733, Accuracy: 69.602%\n",
            "Epoch 009: Loss: 1.737, Accuracy: 69.500%\n",
            "Epoch 009: Loss: 1.735, Accuracy: 69.554%\n",
            "Epoch 009: Loss: 1.736, Accuracy: 69.516%\n",
            "Epoch 009: Loss: 1.740, Accuracy: 69.539%\n",
            "Epoch 009: Loss: 1.739, Accuracy: 69.561%\n",
            "Epoch 009: Loss: 1.739, Accuracy: 69.643%\n",
            "Epoch 009: Loss: 1.741, Accuracy: 69.634%\n",
            "Epoch 009: Loss: 1.739, Accuracy: 69.626%\n",
            "Epoch 009: Loss: 1.738, Accuracy: 69.589%\n",
            "Epoch 009: Loss: 1.738, Accuracy: 69.581%\n",
            "Epoch 009: Loss: 1.736, Accuracy: 69.602%\n",
            "Epoch 009: Loss: 1.731, Accuracy: 69.735%\n",
            "Epoch 009: Loss: 1.728, Accuracy: 69.810%\n",
            "Epoch 009: Loss: 1.727, Accuracy: 69.829%\n",
            "Epoch 009: Loss: 1.730, Accuracy: 69.792%\n",
            "Epoch 009: Loss: 1.731, Accuracy: 69.783%\n",
            "Epoch 009: Loss: 1.731, Accuracy: 69.881%\n",
            "Epoch 009: Loss: 1.734, Accuracy: 69.792%\n",
            "Epoch 009: Loss: 1.734, Accuracy: 69.809%\n",
            "Epoch 009: Loss: 1.738, Accuracy: 69.643%\n",
            "Epoch 009: Loss: 1.736, Accuracy: 69.661%\n",
            "Epoch 009: Loss: 1.734, Accuracy: 69.680%\n",
            "Epoch 009: Loss: 1.734, Accuracy: 69.698%\n",
            "Epoch 009: Loss: 1.735, Accuracy: 69.614%\n",
            "Epoch 009: Loss: 1.734, Accuracy: 69.632%\n",
            "Epoch 009: Loss: 1.740, Accuracy: 69.525%\n",
            "Epoch 009: Loss: 1.739, Accuracy: 69.444%\n",
            "Epoch 009: Loss: 1.741, Accuracy: 69.414%\n",
            "Epoch 009: Loss: 1.742, Accuracy: 69.336%\n",
            "Epoch 009: Loss: 1.739, Accuracy: 69.453%\n",
            "Epoch 009: Loss: 1.736, Accuracy: 69.495%\n",
            "Epoch 009: Loss: 1.739, Accuracy: 69.370%\n",
            "Epoch 009: Loss: 1.735, Accuracy: 69.437%\n",
            "Epoch 009: Loss: 1.734, Accuracy: 69.478%\n",
            "Epoch 009: Loss: 1.734, Accuracy: 69.520%\n",
            "Epoch 009: Loss: 1.738, Accuracy: 69.468%\n",
            "Epoch 009: Loss: 1.736, Accuracy: 69.508%\n",
            "Epoch 009: Loss: 1.735, Accuracy: 69.571%\n",
            "Epoch 009: Loss: 1.731, Accuracy: 69.701%\n",
            "Epoch 009: Loss: 1.730, Accuracy: 69.739%\n",
            "Epoch 009: Loss: 1.730, Accuracy: 69.732%\n",
            "Epoch 009: Loss: 1.730, Accuracy: 69.637%\n",
            "Epoch 009: Loss: 1.726, Accuracy: 69.784%\n",
            "Epoch 009: Loss: 1.723, Accuracy: 69.821%\n",
            "Epoch 009: Loss: 1.722, Accuracy: 69.857%\n",
            "Epoch 009: Loss: 1.723, Accuracy: 69.849%\n",
            "Epoch 009: Loss: 1.723, Accuracy: 69.863%\n",
            "Epoch 009: Loss: 1.724, Accuracy: 69.919%\n",
            "Epoch 009: Loss: 1.726, Accuracy: 69.869%\n",
            "Epoch 009: Loss: 1.728, Accuracy: 69.841%\n",
            "Epoch 009: Loss: 1.725, Accuracy: 69.917%\n",
            "Epoch 009: Loss: 1.724, Accuracy: 69.971%\n",
            "Epoch 009: Loss: 1.724, Accuracy: 69.963%\n",
            "Epoch 009: Loss: 1.724, Accuracy: 69.975%\n",
            "Epoch 009: Loss: 1.722, Accuracy: 70.049%\n",
            "Epoch 009: Loss: 1.721, Accuracy: 70.040%\n",
            "Epoch 009: Loss: 1.722, Accuracy: 70.032%\n",
            "Epoch 009: Loss: 1.720, Accuracy: 70.084%\n",
            "Epoch 009: Loss: 1.722, Accuracy: 69.996%\n",
            "Epoch 009: Loss: 1.723, Accuracy: 69.969%\n",
            "Epoch 009: Loss: 1.725, Accuracy: 69.902%\n",
            "Epoch 009: Loss: 1.726, Accuracy: 69.876%\n",
            "Epoch 009: Loss: 1.725, Accuracy: 69.869%\n",
            "Epoch 009: Loss: 1.722, Accuracy: 69.900%\n",
            "Epoch 009: Loss: 1.723, Accuracy: 69.893%\n",
            "Epoch 009: Loss: 1.723, Accuracy: 69.962%\n",
            "Epoch 009: Loss: 1.723, Accuracy: 69.955%\n",
            "Epoch 009: Loss: 1.725, Accuracy: 69.854%\n",
            "Epoch 009: Loss: 1.728, Accuracy: 69.754%\n",
            "Epoch 009: Loss: 1.729, Accuracy: 69.767%\n",
            "Epoch 009: Loss: 1.728, Accuracy: 69.798%\n",
            "Epoch 009: Loss: 1.726, Accuracy: 69.792%\n",
            "Epoch 009: Loss: 1.723, Accuracy: 69.876%\n",
            "Epoch 009: Loss: 1.720, Accuracy: 69.996%\n",
            "Epoch 009: Loss: 1.717, Accuracy: 70.079%\n",
            "Epoch 009: Loss: 1.713, Accuracy: 70.161%\n",
            "Epoch 009: Loss: 1.713, Accuracy: 70.188%\n",
            "Epoch 009: Loss: 1.712, Accuracy: 70.198%\n",
            "Epoch 009: Loss: 1.711, Accuracy: 70.190%\n",
            "Epoch 009: Loss: 1.713, Accuracy: 70.147%\n",
            "Epoch 009: Loss: 1.712, Accuracy: 70.139%\n",
            "Epoch 009: Loss: 1.713, Accuracy: 70.045%\n",
            "Epoch 009: Loss: 1.709, Accuracy: 70.141%\n",
            "Epoch 009: Loss: 1.708, Accuracy: 70.202%\n",
            "Epoch 009: Loss: 1.709, Accuracy: 70.143%\n",
            "Epoch 009: Loss: 1.710, Accuracy: 70.118%\n",
            "Epoch 009: Loss: 1.711, Accuracy: 70.077%\n",
            "Epoch 009: Loss: 1.712, Accuracy: 70.053%\n",
            "Epoch 009: Loss: 1.708, Accuracy: 70.146%\n",
            "Epoch 009: Loss: 1.709, Accuracy: 70.155%\n",
            "Epoch 009: Loss: 1.707, Accuracy: 70.148%\n",
            "Epoch 009: Loss: 1.707, Accuracy: 70.157%\n",
            "Epoch 009: Loss: 1.707, Accuracy: 70.117%\n",
            "Epoch 009: Loss: 1.708, Accuracy: 70.110%\n",
            "Epoch 009: Loss: 1.709, Accuracy: 70.135%\n",
            "Epoch 009: Loss: 1.712, Accuracy: 70.064%\n",
            "Epoch 009: Loss: 1.711, Accuracy: 70.073%\n",
            "Epoch 009: Loss: 1.711, Accuracy: 70.082%\n",
            "Epoch 009: Loss: 1.710, Accuracy: 70.123%\n",
            "Epoch 009: Loss: 1.709, Accuracy: 70.116%\n",
            "Epoch 009: Loss: 1.709, Accuracy: 70.156%\n",
            "Epoch 009: Loss: 1.707, Accuracy: 70.180%\n",
            "Epoch 009: Loss: 1.709, Accuracy: 70.096%\n",
            "Epoch 009: Loss: 1.708, Accuracy: 70.120%\n",
            "Epoch 009: Loss: 1.706, Accuracy: 70.144%\n",
            "Epoch 009: Loss: 1.706, Accuracy: 70.076%\n",
            "Epoch 009: Loss: 1.708, Accuracy: 70.070%\n",
            "Epoch 009: Loss: 1.705, Accuracy: 70.124%\n",
            "Epoch 009: Loss: 1.705, Accuracy: 70.147%\n",
            "Epoch 009: Loss: 1.704, Accuracy: 70.156%\n",
            "Epoch 009: Loss: 1.703, Accuracy: 70.179%\n",
            "Epoch 009: Loss: 1.704, Accuracy: 70.142%\n",
            "Epoch 009: Loss: 1.703, Accuracy: 70.150%\n",
            "Epoch 009: Loss: 1.704, Accuracy: 70.129%\n",
            "Epoch 009: Loss: 1.705, Accuracy: 70.050%\n",
            "Epoch 009: Loss: 1.706, Accuracy: 70.029%\n",
            "Epoch 009: Loss: 1.706, Accuracy: 70.038%\n",
            "Epoch 009: Loss: 1.705, Accuracy: 70.003%\n",
            "Epoch 009: Loss: 1.704, Accuracy: 70.011%\n",
            "Epoch 009: Loss: 1.705, Accuracy: 70.020%\n",
            "Epoch 009: Loss: 1.703, Accuracy: 70.071%\n",
            "Epoch 009: Loss: 1.702, Accuracy: 70.093%\n",
            "Epoch 009: Loss: 1.702, Accuracy: 70.087%\n",
            "Epoch 009: Loss: 1.701, Accuracy: 70.109%\n",
            "Epoch 009: Loss: 1.701, Accuracy: 70.103%\n",
            "Epoch 009: Loss: 1.699, Accuracy: 70.167%\n",
            "Epoch 009: Loss: 1.697, Accuracy: 70.174%\n",
            "Epoch 009: Loss: 1.696, Accuracy: 70.154%\n",
            "Epoch 009: Loss: 1.696, Accuracy: 70.189%\n",
            "Epoch 009: Loss: 1.696, Accuracy: 70.169%\n",
            "Epoch 009: Loss: 1.698, Accuracy: 70.163%\n",
            "Epoch 009: Loss: 1.698, Accuracy: 70.157%\n",
            "Epoch 009: Loss: 1.696, Accuracy: 70.218%\n",
            "Epoch 009: Loss: 1.695, Accuracy: 70.239%\n",
            "Epoch 009: Loss: 1.694, Accuracy: 70.232%\n",
            "Epoch 009: Loss: 1.696, Accuracy: 70.213%\n",
            "Epoch 009: Loss: 1.695, Accuracy: 70.246%\n",
            "Epoch 009: Loss: 1.693, Accuracy: 70.280%\n",
            "Epoch 009: Loss: 1.694, Accuracy: 70.260%\n",
            "Epoch 009: Loss: 1.695, Accuracy: 70.241%\n",
            "Epoch 009: Loss: 1.695, Accuracy: 70.247%\n",
            "Epoch 009: Loss: 1.695, Accuracy: 70.228%\n",
            "Epoch 009: Loss: 1.695, Accuracy: 70.222%\n",
            "Epoch 009: Loss: 1.696, Accuracy: 70.229%\n",
            "Epoch 009: Loss: 1.696, Accuracy: 70.223%\n",
            "Epoch 009: Loss: 1.697, Accuracy: 70.217%\n",
            "Epoch 009: Loss: 1.695, Accuracy: 70.236%\n",
            "Epoch 009: Loss: 1.693, Accuracy: 70.230%\n",
            "Epoch 009: Loss: 1.692, Accuracy: 70.275%\n",
            "Epoch 009: Loss: 1.693, Accuracy: 70.256%\n",
            "Epoch 009: Loss: 1.691, Accuracy: 70.288%\n",
            "Epoch 009: Loss: 1.690, Accuracy: 70.306%\n",
            "Epoch 009: Loss: 1.689, Accuracy: 70.350%\n",
            "Epoch 009: Loss: 1.690, Accuracy: 70.319%\n",
            "Epoch 009: Loss: 1.691, Accuracy: 70.263%\n",
            "Epoch 009: Loss: 1.691, Accuracy: 70.270%\n",
            "Epoch 009: Loss: 1.690, Accuracy: 70.239%\n",
            "Epoch 009: Loss: 1.691, Accuracy: 70.270%\n",
            "Epoch 009: Loss: 1.691, Accuracy: 70.276%\n",
            "Epoch 009: Loss: 1.691, Accuracy: 70.270%\n",
            "Epoch 009: Loss: 1.691, Accuracy: 70.264%\n",
            "Epoch 009: Loss: 1.691, Accuracy: 70.259%\n",
            "Epoch 009: Loss: 1.690, Accuracy: 70.277%\n",
            "Epoch 009: Loss: 1.692, Accuracy: 70.235%\n",
            "Epoch 009: Loss: 1.691, Accuracy: 70.241%\n",
            "Epoch 009: Loss: 1.692, Accuracy: 70.248%\n",
            "Epoch 009: Loss: 1.692, Accuracy: 70.254%\n",
            "Epoch 009: Loss: 1.692, Accuracy: 70.248%\n",
            "Epoch 009: Loss: 1.692, Accuracy: 70.266%\n",
            "Epoch 009: Loss: 1.693, Accuracy: 70.260%\n",
            "Epoch 009: Loss: 1.695, Accuracy: 70.162%\n",
            "Epoch 009: Loss: 1.694, Accuracy: 70.214%\n",
            "Epoch 009: Loss: 1.693, Accuracy: 70.221%\n",
            "Epoch 009: Loss: 1.693, Accuracy: 70.250%\n",
            "Epoch 009: Loss: 1.693, Accuracy: 70.278%\n",
            "Epoch 009: Loss: 1.692, Accuracy: 70.295%\n",
            "Epoch 009: Loss: 1.691, Accuracy: 70.346%\n",
            "Epoch 009: Loss: 1.692, Accuracy: 70.284%\n",
            "Epoch 009: Loss: 1.693, Accuracy: 70.290%\n",
            "Epoch 009: Loss: 1.692, Accuracy: 70.318%\n",
            "Epoch 009: Loss: 1.693, Accuracy: 70.290%\n",
            "Epoch 009: Loss: 1.693, Accuracy: 70.307%\n",
            "Duration :126.282\n"
          ]
        }
      ],
      "source": [
        "# Re-run the training loop\n",
        "\n",
        "\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, train_labels))\n",
        "train_dataset = train_dataset.batch(32)\n",
        "\n",
        "# keep results for plotting \n",
        "train_loss_results = []\n",
        "train_accuracy_results = []\n",
        "num_epochs = 10 \n",
        "weight_decay = 0.005\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  epoch_loss_avg = tf.keras.metrics.Mean()\n",
        "  epoch_accuracy = tf.keras.metrics.CategoricalAccuracy()\n",
        "\n",
        "  # training loop \n",
        "  for x, y in train_dataset:\n",
        "    # optimize the model \n",
        "    loss_value, grads = grad(model, x, y, weight_decay)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))   \n",
        "\n",
        "    # compute current loss\n",
        "    epoch_loss_avg(loss_value)\n",
        "    epoch_accuracy(to_categorical(y), model(x))\n",
        "\n",
        "    # end epoch \n",
        "    train_loss_results.append(epoch_loss_avg.result())\n",
        "    train_accuracy_results.append(epoch_accuracy.result())\n",
        "\n",
        "    print(\"Epoch {:03d}: Loss: {:.3f}, Accuracy: {:.3%}\".format(epoch, \n",
        "                                                                epoch_loss_avg.result(),\n",
        "                                                                epoch_accuracy.result()))\n",
        "    \n",
        "print(\"Duration :{:.3f}\".format(time.time() - start_time))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9iVRx-8wPwuN"
      },
      "source": [
        "#### Print the autograph code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "ez0eeab-PwuN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "108b8909-3914-4f96-a289-3b3dd75ff442"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "def tf__grad(model, inputs, targets, wd):\n",
            "    with ag__.FunctionScope('grad', 'fscope', ag__.ConversionOptions(recursive=True, user_requested=True, optional_features=(), internal_convert_user_code=True)) as fscope:\n",
            "        do_return = False\n",
            "        retval_ = ag__.UndefinedReturnValue()\n",
            "        with ag__.ld(tf).GradientTape() as tape:\n",
            "            loss_value = ag__.converted_call(ag__.ld(loss), (ag__.ld(model), ag__.ld(inputs), ag__.ld(targets), ag__.ld(wd)), None, fscope)\n",
            "        try:\n",
            "            do_return = True\n",
            "            retval_ = (ag__.ld(loss_value), ag__.converted_call(ag__.ld(tape).gradient, (ag__.ld(loss_value), ag__.ld(model).trainable_variables), None, fscope))\n",
            "        except:\n",
            "            do_return = False\n",
            "            raise\n",
            "        return fscope.ret(retval_, do_return)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Use tf.autograph.to_code to see the generated code\n",
        "\n",
        "print(tf.autograph.to_code(grad.python_function))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}